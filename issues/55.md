# Issue #55: Virtual Backgrounds

## User Story
As a **waddle participant**, I want to **use virtual backgrounds and video effects during calls** so that **I can maintain privacy and professionalism regardless of my physical environment**.

## Description
Implement advanced virtual background capabilities with AI-powered background removal, custom backgrounds, blur effects, and real-time video filters. This includes high-quality segmentation, performance optimization, and creative effects that work seamlessly across different devices and lighting conditions.

## Acceptance Criteria
- [ ] AI-powered background removal
- [ ] Background blur with adjustable intensity
- [ ] Custom image/video backgrounds
- [ ] Real-time video effects
- [ ] Green screen support
- [ ] Performance optimization
- [ ] Mobile device support
- [ ] Background library

## Technical Implementation

### 1. Virtual Background Engine
```typescript
// Virtual Background Core Engine
export interface VirtualBackgroundConfig {
  mode: 'blur' | 'image' | 'video' | 'none';
  blurIntensity?: number;
  backgroundSource?: string;
  segmentationQuality: 'low' | 'medium' | 'high' | 'auto';
  edgeSmoothing: boolean;
  lightingCorrection: boolean;
  gpuAcceleration: boolean;
}

export class VirtualBackgroundEngine {
  private segmentationModel: BodyPixModel;
  private compositor: VideoCompositor;
  private performanceMonitor: PerformanceMonitor;
  private backgroundProcessor: BackgroundProcessor;
  
  constructor(
    private config: VirtualBackgroundConfig,
    private canvas: HTMLCanvasElement
  ) {
    this.compositor = new VideoCompositor(canvas);
    this.performanceMonitor = new PerformanceMonitor();
    this.backgroundProcessor = new BackgroundProcessor();
    this.initialize();
  }
  
  private async initialize() {
    // Load segmentation model based on quality setting
    const modelConfig = this.getModelConfig();
    this.segmentationModel = await bodyPix.load(modelConfig);
    
    // Initialize GPU acceleration if available
    if (this.config.gpuAcceleration && this.isWebGLAvailable()) {
      await this.compositor.enableGPUAcceleration();
    }
    
    // Preload background if specified
    if (this.config.backgroundSource) {
      await this.backgroundProcessor.loadBackground(
        this.config.backgroundSource
      );
    }
  }
  
  async processFrame(
    inputFrame: VideoFrame | HTMLVideoElement
  ): Promise<VideoFrame> {
    const startTime = performance.now();
    
    // Perform segmentation
    const segmentation = await this.performSegmentation(inputFrame);
    
    // Apply background effect
    const processed = await this.applyBackgroundEffect(
      inputFrame,
      segmentation
    );
    
    // Monitor performance
    const processingTime = performance.now() - startTime;
    this.performanceMonitor.recordFrameTime(processingTime);
    
    // Adjust quality if needed
    if (this.config.segmentationQuality === 'auto') {
      await this.adjustQualityBasedOnPerformance();
    }
    
    return processed;
  }
  
  private async performSegmentation(
    frame: VideoFrame | HTMLVideoElement
  ): Promise<SemanticPersonSegmentation> {
    const segmentationConfig = {
      flipHorizontal: false,
      internalResolution: this.getInternalResolution(),
      segmentationThreshold: 0.7,
      scoreThreshold: 0.3,
      maxDetections: 1
    };
    
    const segmentation = await this.segmentationModel.segmentPerson(
      frame,
      segmentationConfig
    );
    
    // Apply edge refinement
    if (this.config.edgeSmoothing) {
      return this.refineSegmentationEdges(segmentation);
    }
    
    return segmentation;
  }
  
  private async applyBackgroundEffect(
    frame: VideoFrame | HTMLVideoElement,
    segmentation: SemanticPersonSegmentation
  ): Promise<VideoFrame> {
    switch (this.config.mode) {
      case 'blur':
        return this.applyBlurEffect(frame, segmentation);
        
      case 'image':
      case 'video':
        return this.applyReplacementBackground(frame, segmentation);
        
      case 'none':
      default:
        return frame as VideoFrame;
    }
  }
  
  private async applyBlurEffect(
    frame: VideoFrame | HTMLVideoElement,
    segmentation: SemanticPersonSegmentation
  ): Promise<VideoFrame> {
    const { width, height } = this.getFrameDimensions(frame);
    
    // Create mask from segmentation
    const mask = await this.createMaskFromSegmentation(
      segmentation,
      width,
      height
    );
    
    // Apply blur to background
    const blurredFrame = await this.compositor.blur(
      frame,
      this.config.blurIntensity || 15
    );
    
    // Composite person over blurred background
    return this.compositor.composite([
      { source: blurredFrame, mask: this.invertMask(mask) },
      { source: frame, mask: mask }
    ]);
  }
  
  private async applyReplacementBackground(
    frame: VideoFrame | HTMLVideoElement,
    segmentation: SemanticPersonSegmentation
  ): Promise<VideoFrame> {
    const { width, height } = this.getFrameDimensions(frame);
    
    // Get background frame
    const backgroundFrame = await this.backgroundProcessor.getFrame(
      width,
      height
    );
    
    // Create refined mask
    const mask = await this.createRefinedMask(
      segmentation,
      frame,
      width,
      height
    );
    
    // Apply lighting correction if enabled
    let processedPerson = frame;
    if (this.config.lightingCorrection) {
      processedPerson = await this.matchLighting(
        frame,
        backgroundFrame,
        mask
      );
    }
    
    // Composite layers
    return this.compositor.composite([
      { source: backgroundFrame, mask: this.invertMask(mask) },
      { source: processedPerson, mask: mask }
    ]);
  }
  
  private async createRefinedMask(
    segmentation: SemanticPersonSegmentation,
    frame: VideoFrame | HTMLVideoElement,
    width: number,
    height: number
  ): Promise<ImageData> {
    // Convert segmentation to mask
    const mask = await this.createMaskFromSegmentation(
      segmentation,
      width,
      height
    );
    
    // Apply edge refinement techniques
    const refined = await this.edgeRefinement.refine(mask, {
      // Feather edges
      featherRadius: 2,
      
      // Alpha matting for hair and fine details
      useAlphaMatting: true,
      
      // Temporal smoothing to reduce flicker
      temporalSmoothing: 0.3,
      
      // Color-based refinement
      colorRefinement: {
        enabled: true,
        threshold: 0.1
      }
    });
    
    return refined;
  }
}

// Background Processor
export class BackgroundProcessor {
  private imageCache = new Map<string, HTMLImageElement>();
  private videoCache = new Map<string, HTMLVideoElement>();
  private animatedBackgrounds = new Map<string, AnimatedBackground>();
  
  async loadBackground(source: string): Promise<void> {
    const type = this.detectBackgroundType(source);
    
    switch (type) {
      case 'image':
        await this.loadImage(source);
        break;
        
      case 'video':
        await this.loadVideo(source);
        break;
        
      case 'animated':
        await this.loadAnimatedBackground(source);
        break;
    }
  }
  
  async getFrame(
    width: number,
    height: number
  ): Promise<HTMLCanvasElement> {
    const canvas = document.createElement('canvas');
    canvas.width = width;
    canvas.height = height;
    const ctx = canvas.getContext('2d')!;
    
    if (this.currentSource.type === 'image') {
      const image = this.imageCache.get(this.currentSource.url)!;
      this.drawScaledImage(ctx, image, width, height);
    } else if (this.currentSource.type === 'video') {
      const video = this.videoCache.get(this.currentSource.url)!;
      ctx.drawImage(video, 0, 0, width, height);
    } else if (this.currentSource.type === 'animated') {
      const animated = this.animatedBackgrounds.get(this.currentSource.id)!;
      await animated.renderFrame(ctx, width, height);
    }
    
    return canvas;
  }
  
  private drawScaledImage(
    ctx: CanvasRenderingContext2D,
    image: HTMLImageElement,
    targetWidth: number,
    targetHeight: number
  ) {
    // Implement smart scaling (cover, contain, or smart crop)
    const imageAspect = image.width / image.height;
    const targetAspect = targetWidth / targetHeight;
    
    let sx = 0, sy = 0, sw = image.width, sh = image.height;
    
    if (imageAspect > targetAspect) {
      // Image is wider, crop sides
      sw = image.height * targetAspect;
      sx = (image.width - sw) / 2;
    } else {
      // Image is taller, crop top/bottom
      sh = image.width / targetAspect;
      sy = (image.height - sh) / 2;
    }
    
    ctx.drawImage(
      image,
      sx, sy, sw, sh,
      0, 0, targetWidth, targetHeight
    );
  }
}
```

### 2. Advanced Segmentation & Effects
```typescript
// Enhanced Segmentation System
export class EnhancedSegmentation {
  private segmentationHistory: SegmentationFrame[] = [];
  private hairSegmenter: HairSegmentationModel;
  private depthEstimator: DepthEstimationModel;
  
  constructor() {
    this.initializeModels();
  }
  
  async performAdvancedSegmentation(
    frame: VideoFrame,
    previousSegmentation?: SemanticPersonSegmentation
  ): Promise<EnhancedSegmentationResult> {
    // Run multiple segmentation models in parallel
    const [
      bodySegmentation,
      hairSegmentation,
      depthMap
    ] = await Promise.all([
      this.segmentBody(frame),
      this.segmentHair(frame),
      this.estimateDepth(frame)
    ]);
    
    // Combine segmentations
    const combined = this.combineSegmentations({
      body: bodySegmentation,
      hair: hairSegmentation,
      depth: depthMap,
      previous: previousSegmentation
    });
    
    // Apply temporal smoothing
    const smoothed = this.temporalSmoothing(combined);
    
    // Store in history
    this.segmentationHistory.push({
      segmentation: smoothed,
      timestamp: Date.now()
    });
    
    // Keep only recent frames
    if (this.segmentationHistory.length > 10) {
      this.segmentationHistory.shift();
    }
    
    return smoothed;
  }
  
  private combineSegmentations(
    inputs: SegmentationInputs
  ): EnhancedSegmentationResult {
    const { width, height } = inputs.body.allPoses[0];
    const combined = new Uint8ClampedArray(width * height * 4);
    
    for (let i = 0; i < width * height; i++) {
      const pixelIndex = i * 4;
      
      // Get confidence from each model
      const bodyConf = inputs.body.data[i];
      const hairConf = inputs.hair ? inputs.hair.data[i] : 0;
      const depth = inputs.depth ? inputs.depth.data[i] : 0;
      
      // Combine using weighted average
      let confidence = bodyConf * 0.7;
      
      // Boost confidence for hair regions
      if (hairConf > 0.5) {
        confidence = Math.max(confidence, hairConf * 0.9);
      }
      
      // Use depth to refine edges
      if (depth < 0.3) { // Close to camera
        confidence *= 1.1;
      }
      
      // Apply temporal consistency
      if (inputs.previous) {
        const prevConf = inputs.previous.data[i];
        confidence = confidence * 0.8 + prevConf * 0.2;
      }
      
      // Set alpha channel
      combined[pixelIndex + 3] = Math.min(255, confidence * 255);
    }
    
    return {
      data: combined,
      width,
      height,
      personBounds: this.calculatePersonBounds(combined, width, height)
    };
  }
  
  private temporalSmoothing(
    current: EnhancedSegmentationResult
  ): EnhancedSegmentationResult {
    if (this.segmentationHistory.length < 2) {
      return current;
    }
    
    const smoothed = new Uint8ClampedArray(current.data.length);
    const weights = [0.5, 0.3, 0.2]; // Current, previous, before previous
    
    for (let i = 0; i < current.data.length; i += 4) {
      let alpha = current.data[i + 3] * weights[0];
      
      for (let h = 0; h < Math.min(2, this.segmentationHistory.length); h++) {
        const historical = this.segmentationHistory[
          this.segmentationHistory.length - 1 - h
        ];
        alpha += historical.segmentation.data[i + 3] * weights[h + 1];
      }
      
      smoothed[i + 3] = alpha;
    }
    
    return {
      ...current,
      data: smoothed
    };
  }
}

// Creative Effects Processor
export class CreativeEffectsProcessor {
  private effects = new Map<string, VideoEffect>();
  
  constructor() {
    this.registerDefaultEffects();
  }
  
  private registerDefaultEffects() {
    // Artistic effects
    this.effects.set('cartoon', new CartoonEffect());
    this.effects.set('sketch', new SketchEffect());
    this.effects.set('watercolor', new WatercolorEffect());
    
    // Enhancement effects  
    this.effects.set('beauty', new BeautyEffect());
    this.effects.set('glamour', new GlamourEffect());
    
    // Fun effects
    this.effects.set('pixelate', new PixelateEffect());
    this.effects.set('glitch', new GlitchEffect());
    this.effects.set('retro', new RetroEffect());
    
    // Professional effects
    this.effects.set('softFocus', new SoftFocusEffect());
    this.effects.set('cinematic', new CinematicEffect());
  }
  
  async applyEffect(
    frame: VideoFrame,
    effectName: string,
    parameters: EffectParameters = {}
  ): Promise<VideoFrame> {
    const effect = this.effects.get(effectName);
    if (!effect) {
      throw new Error(`Effect ${effectName} not found`);
    }
    
    return effect.apply(frame, parameters);
  }
}

// Example: Beauty Effect Implementation
export class BeautyEffect implements VideoEffect {
  private faceDetector: FaceDetector;
  private skinSmoother: SkinSmoother;
  
  async apply(
    frame: VideoFrame,
    parameters: BeautyEffectParameters
  ): Promise<VideoFrame> {
    // Detect faces
    const faces = await this.faceDetector.detect(frame);
    
    if (faces.length === 0) {
      return frame; // No faces found
    }
    
    const canvas = new OffscreenCanvas(frame.width, frame.height);
    const ctx = canvas.getContext('2d')!;
    
    // Draw original frame
    ctx.drawImage(frame, 0, 0);
    
    // Process each face
    for (const face of faces) {
      // Apply skin smoothing
      if (parameters.skinSmoothing !== false) {
        await this.applySkinSmoothing(
          ctx,
          face,
          parameters.smoothingIntensity || 0.5
        );
      }
      
      // Eye enhancement
      if (parameters.eyeEnhancement) {
        await this.enhanceEyes(ctx, face);
      }
      
      // Teeth whitening
      if (parameters.teethWhitening && face.smile > 0.5) {
        await this.whitenTeeth(ctx, face);
      }
      
      // Face slimming (subtle)
      if (parameters.faceSlimming) {
        await this.applyFaceSlimming(
          ctx,
          face,
          parameters.slimmingIntensity || 0.1
        );
      }
    }
    
    return new VideoFrame(canvas, { timestamp: frame.timestamp });
  }
  
  private async applySkinSmoothing(
    ctx: CanvasRenderingContext2D,
    face: DetectedFace,
    intensity: number
  ) {
    // Get face region
    const faceImage = ctx.getImageData(
      face.bounds.x,
      face.bounds.y,
      face.bounds.width,
      face.bounds.height
    );
    
    // Apply bilateral filter for skin smoothing
    const smoothed = await this.skinSmoother.smooth(faceImage, {
      spatialSigma: 10 * intensity,
      rangeSigma: 0.1,
      iterations: 2
    });
    
    // Blend with original
    const blended = this.blend(faceImage, smoothed, intensity);
    
    ctx.putImageData(blended, face.bounds.x, face.bounds.y);
  }
}
```

### 3. Virtual Background UI
```tsx
export function VirtualBackgroundSettings() {
  const [mode, setMode] = useState<BackgroundMode>('none');
  const [selectedBackground, setSelectedBackground] = useState<string>();
  const [blurIntensity, setBlurIntensity] = useState(15);
  const [customBackgrounds, setCustomBackgrounds] = useState<Background[]>([]);
  const [isProcessing, setIsProcessing] = useState(false);
  const [preview, setPreview] = useState<string>();
  
  const backgroundEngine = useVirtualBackgroundEngine();
  
  useEffect(() => {
    loadCustomBackgrounds();
  }, []);
  
  const loadCustomBackgrounds = async () => {
    const backgrounds = await api.getCustomBackgrounds();
    setCustomBackgrounds(backgrounds);
  };
  
  const applyBackground = async (newMode: BackgroundMode, source?: string) => {
    setIsProcessing(true);
    try {
      await backgroundEngine.setMode(newMode, {
        backgroundSource: source,
        blurIntensity: newMode === 'blur' ? blurIntensity : undefined
      });
      
      setMode(newMode);
      setSelectedBackground(source);
      
      toast.success('Background applied');
    } catch (error) {
      toast.error('Failed to apply background');
    } finally {
      setIsProcessing(false);
    }
  };
  
  const uploadCustomBackground = async (file: File) => {
    try {
      const background = await api.uploadBackground(file);
      setCustomBackgrounds([...customBackgrounds, background]);
      toast.success('Background uploaded');
    } catch (error) {
      toast.error('Failed to upload background');
    }
  };
  
  return (
    <div className="virtual-background-settings">
      <div className="background-preview">
        <VideoPreview
          showVirtualBackground
          onFrameCapture={setPreview}
        />
      </div>
      
      <div className="background-modes">
        <BackgroundModeSelector
          value={mode}
          onChange={(newMode) => {
            if (newMode === 'none' || newMode === 'blur') {
              applyBackground(newMode);
            }
          }}
          disabled={isProcessing}
        />
        
        {mode === 'blur' && (
          <BlurIntensitySlider
            value={blurIntensity}
            onChange={setBlurIntensity}
            onChangeComplete={(value) => {
              setBlurIntensity(value);
              applyBackground('blur');
            }}
          />
        )}
      </div>
      
      {(mode === 'image' || mode === 'video') && (
        <BackgroundGallery
          backgrounds={[
            ...defaultBackgrounds,
            ...customBackgrounds
          ]}
          selected={selectedBackground}
          onSelect={(bg) => applyBackground(mode, bg.source)}
          onUpload={uploadCustomBackground}
        />
      )}
      
      <AdvancedSettings />
    </div>
  );
}

function BackgroundGallery({ 
  backgrounds, 
  selected, 
  onSelect, 
  onUpload 
}: {
  backgrounds: Background[];
  selected?: string;
  onSelect: (bg: Background) => void;
  onUpload: (file: File) => void;
}) {
  const [category, setCategory] = useState<string>('all');
  const [searchTerm, setSearchTerm] = useState('');
  
  const filteredBackgrounds = backgrounds.filter(bg => {
    const matchesCategory = category === 'all' || bg.category === category;
    const matchesSearch = bg.name.toLowerCase().includes(
      searchTerm.toLowerCase()
    );
    return matchesCategory && matchesSearch;
  });
  
  return (
    <div className="background-gallery">
      <div className="gallery-header">
        <CategoryFilter
          categories={['all', 'office', 'home', 'nature', 'abstract', 'custom']}
          selected={category}
          onChange={setCategory}
        />
        
        <SearchInput
          value={searchTerm}
          onChange={setSearchTerm}
          placeholder="Search backgrounds..."
        />
      </div>
      
      <div className="gallery-grid">
        <UploadTile onUpload={onUpload} />
        
        {filteredBackgrounds.map(bg => (
          <BackgroundTile
            key={bg.id}
            background={bg}
            isSelected={bg.source === selected}
            onClick={() => onSelect(bg)}
          />
        ))}
      </div>
    </div>
  );
}

function BackgroundTile({ 
  background, 
  isSelected, 
  onClick 
}: {
  background: Background;
  isSelected: boolean;
  onClick: () => void;
}) {
  const [isLoading, setIsLoading] = useState(true);
  
  return (
    <div 
      className={`background-tile ${isSelected ? 'selected' : ''}`}
      onClick={onClick}
    >
      {background.type === 'video' ? (
        <video
          src={background.thumbnail}
          muted
          loop
          autoPlay
          onLoadedData={() => setIsLoading(false)}
        />
      ) : (
        <img
          src={background.thumbnail}
          alt={background.name}
          onLoad={() => setIsLoading(false)}
        />
      )}
      
      {isLoading && <LoadingSpinner />}
      
      <div className="tile-overlay">
        <span className="background-name">{background.name}</span>
        {background.isPremium && <PremiumBadge />}
      </div>
      
      {isSelected && <CheckIcon className="selected-icon" />}
    </div>
  );
}
```

### 4. Performance Optimization
```typescript
// Performance-Optimized Background Processing
export class OptimizedBackgroundProcessor {
  private frameSkipCounter = 0;
  private qualityController: QualityController;
  private gpuProcessor?: GPUProcessor;
  private webWorkerPool: WorkerPool;
  
  constructor(
    private targetFPS: number = 30,
    private maxCPUUsage: number = 50
  ) {
    this.qualityController = new QualityController();
    this.webWorkerPool = new WorkerPool(navigator.hardwareConcurrency);
    this.initializeGPU();
  }
  
  private async initializeGPU() {
    if ('gpu' in navigator) {
      try {
        this.gpuProcessor = await GPUProcessor.create();
      } catch (error) {
        console.warn('GPU acceleration not available');
      }
    }
  }
  
  async processFrameOptimized(
    frame: VideoFrame
  ): Promise<VideoFrame | null> {
    // Check if we should skip this frame
    if (this.shouldSkipFrame()) {
      this.frameSkipCounter++;
      return null;
    }
    
    // Determine processing method based on available resources
    const processingMethod = this.selectProcessingMethod();
    
    switch (processingMethod) {
      case 'gpu':
        return this.processOnGPU(frame);
        
      case 'worker':
        return this.processInWorker(frame);
        
      case 'main':
        return this.processOnMainThread(frame);
        
      case 'skip':
        return null;
    }
  }
  
  private shouldSkipFrame(): boolean {
    const stats = this.qualityController.getStats();
    
    // Skip frames if performance is poor
    if (stats.averageFPS < this.targetFPS * 0.7) {
      // Skip every other frame
      return this.frameSkipCounter % 2 === 0;
    }
    
    if (stats.cpuUsage > this.maxCPUUsage) {
      // Skip 2 out of 3 frames
      return this.frameSkipCounter % 3 !== 0;
    }
    
    return false;
  }
  
  private selectProcessingMethod(): ProcessingMethod {
    const resources = this.getAvailableResources();
    
    // Prefer GPU if available and not overloaded
    if (this.gpuProcessor && resources.gpu.available) {
      return 'gpu';
    }
    
    // Use worker threads if CPU has headroom
    if (resources.cpu.usage < this.maxCPUUsage * 0.8) {
      return 'worker';
    }
    
    // Fall back to main thread for critical frames
    if (this.frameSkipCounter > 2) {
      return 'main';
    }
    
    // Skip frame if resources are exhausted
    return 'skip';
  }
  
  private async processOnGPU(frame: VideoFrame): Promise<VideoFrame> {
    return this.gpuProcessor!.process(frame, {
      shader: 'background-removal',
      uniforms: {
        threshold: 0.5,
        smoothing: 0.1
      }
    });
  }
  
  private async processInWorker(frame: VideoFrame): Promise<VideoFrame> {
    // Get available worker from pool
    const worker = await this.webWorkerPool.getWorker();
    
    try {
      // Transfer frame to worker
      const result = await worker.process({
        frame,
        config: this.getCurrentConfig()
      }, [frame]); // Transfer ownership
      
      return result;
    } finally {
      // Return worker to pool
      this.webWorkerPool.releaseWorker(worker);
    }
  }
  
  async adjustQuality(metrics: PerformanceMetrics) {
    const quality = this.qualityController.determineQuality(metrics);
    
    // Adjust segmentation resolution
    if (quality.segmentationScale !== this.currentQuality.segmentationScale) {
      await this.updateSegmentationModel(quality.segmentationScale);
    }
    
    // Adjust edge refinement
    this.edgeRefinement.setQuality(quality.edgeQuality);
    
    // Update frame skip pattern
    this.frameSkipPattern = quality.frameSkipPattern;
  }
}

// GPU-Accelerated Processing
export class GPUProcessor {
  private device: GPUDevice;
  private pipeline: GPURenderPipeline;
  private segmentationTexture: GPUTexture;
  
  static async create(): Promise<GPUProcessor> {
    const adapter = await navigator.gpu.requestAdapter();
    if (!adapter) throw new Error('GPU not available');
    
    const device = await adapter.requestDevice();
    return new GPUProcessor(device);
  }
  
  private constructor(device: GPUDevice) {
    this.device = device;
    this.initializePipeline();
  }
  
  private initializePipeline() {
    const shaderModule = this.device.createShaderModule({
      code: `
        struct Uniforms {
          threshold: f32,
          smoothing: f32,
          time: f32,
        }
        
        @group(0) @binding(0) var inputTexture: texture_2d<f32>;
        @group(0) @binding(1) var segmentationTexture: texture_2d<f32>;
        @group(0) @binding(2) var<uniform> uniforms: Uniforms;
        @group(0) @binding(3) var outputTexture: texture_storage_2d<rgba8unorm, write>;
        
        @compute @workgroup_size(8, 8)
        fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {
          let coords = vec2<i32>(global_id.xy);
          let input_color = textureLoad(inputTexture, coords, 0);
          let segmentation = textureLoad(segmentationTexture, coords, 0).r;
          
          // Apply threshold with smoothing
          let alpha = smoothstep(
            uniforms.threshold - uniforms.smoothing,
            uniforms.threshold + uniforms.smoothing,
            segmentation
          );
          
          // Apply edge refinement
          let refined_alpha = refineEdges(coords, alpha);
          
          // Output with alpha channel
          textureStore(
            outputTexture,
            coords,
            vec4<f32>(input_color.rgb, refined_alpha)
          );
        }
        
        fn refineEdges(coords: vec2<i32>, alpha: f32) -> f32 {
          // Implement edge refinement algorithm
          // Sample neighboring pixels and apply smoothing
          var sum = 0.0;
          var weight_sum = 0.0;
          
          for (var dy = -2; dy <= 2; dy++) {
            for (var dx = -2; dx <= 2; dx++) {
              let sample_coords = coords + vec2<i32>(dx, dy);
              let sample = textureLoad(segmentationTexture, sample_coords, 0).r;
              let weight = exp(-f32(dx*dx + dy*dy) / 4.0);
              sum += sample * weight;
              weight_sum += weight;
            }
          }
          
          return mix(alpha, sum / weight_sum, 0.3);
        }
      `
    });
    
    this.pipeline = this.device.createComputePipeline({
      layout: 'auto',
      compute: {
        module: shaderModule,
        entryPoint: 'main'
      }
    });
  }
  
  async process(
    frame: VideoFrame,
    options: GPUProcessOptions
  ): Promise<VideoFrame> {
    // Create textures from video frame
    const inputTexture = await this.createTextureFromFrame(frame);
    
    // Run segmentation
    const segmentation = await this.runSegmentation(inputTexture);
    
    // Apply background removal
    const outputTexture = this.device.createTexture({
      size: [frame.width, frame.height],
      format: 'rgba8unorm',
      usage: GPUTextureUsage.STORAGE_BINDING | GPUTextureUsage.COPY_SRC
    });
    
    // Execute compute shader
    const commandEncoder = this.device.createCommandEncoder();
    const computePass = commandEncoder.beginComputePass();
    
    computePass.setPipeline(this.pipeline);
    computePass.setBindGroup(0, this.createBindGroup(
      inputTexture,
      segmentation,
      outputTexture,
      options
    ));
    
    computePass.dispatchWorkgroups(
      Math.ceil(frame.width / 8),
      Math.ceil(frame.height / 8)
    );
    
    computePass.end();
    this.device.queue.submit([commandEncoder.finish()]);
    
    // Convert texture back to video frame
    return this.textureToVideoFrame(outputTexture, frame.timestamp);
  }
}
```

### 5. Green Screen Support
```typescript
// Professional Green Screen System
export class GreenScreenProcessor {
  private colorKeySettings: ColorKeySettings = {
    targetColor: { r: 0, g: 255, b: 0 },
    tolerance: 0.3,
    smoothness: 0.1,
    spill: 0.2
  };
  
  async processGreenScreen(
    frame: VideoFrame,
    settings?: Partial<ColorKeySettings>
  ): Promise<ProcessedFrame> {
    const mergedSettings = { ...this.colorKeySettings, ...settings };
    
    // Extract color channels
    const { data, width, height } = await this.getFrameData(frame);
    
    // Create alpha mask
    const mask = new Uint8ClampedArray(width * height);
    
    for (let i = 0; i < data.length; i += 4) {
      const r = data[i] / 255;
      const g = data[i + 1] / 255;
      const b = data[i + 2] / 255;
      
      // Calculate distance from key color
      const distance = this.colorDistance(
        { r, g, b },
        mergedSettings.targetColor
      );
      
      // Apply keying
      let alpha = 1.0;
      if (distance < mergedSettings.tolerance) {
        alpha = 0.0;
      } else if (distance < mergedSettings.tolerance + mergedSettings.smoothness) {
        // Smooth transition
        alpha = (distance - mergedSettings.tolerance) / mergedSettings.smoothness;
      }
      
      mask[i / 4] = alpha * 255;
      
      // Color spill suppression
      if (alpha > 0 && alpha < 1) {
        this.suppressColorSpill(data, i, mergedSettings);
      }
    }
    
    // Refine mask
    const refinedMask = await this.refineMask(mask, width, height);
    
    return {
      frame,
      mask: refinedMask,
      settings: mergedSettings
    };
  }
  
  private colorDistance(
    color1: RGB,
    color2: RGB
  ): number {
    // Use CIE76 color distance for better accuracy
    const lab1 = this.rgbToLab(color1);
    const lab2 = this.rgbToLab(color2);
    
    const deltaL = lab1.l - lab2.l;
    const deltaA = lab1.a - lab2.a;
    const deltaB = lab1.b - lab2.b;
    
    return Math.sqrt(deltaL * deltaL + deltaA * deltaA + deltaB * deltaB) / 100;
  }
  
  private suppressColorSpill(
    data: Uint8ClampedArray,
    index: number,
    settings: ColorKeySettings
  ) {
    const r = data[index];
    const g = data[index + 1];
    const b = data[index + 2];
    
    // Detect green spill
    const spillAmount = Math.max(0, g - Math.max(r, b));
    
    if (spillAmount > 0) {
      // Reduce green channel
      data[index + 1] = g - spillAmount * settings.spill;
      
      // Compensate with other channels
      data[index] = Math.min(255, r + spillAmount * 0.5 * settings.spill);
      data[index + 2] = Math.min(255, b + spillAmount * 0.5 * settings.spill);
    }
  }
  
  private async refineMask(
    mask: Uint8ClampedArray,
    width: number,
    height: number
  ): Promise<ImageData> {
    // Apply morphological operations
    let refined = mask;
    
    // Erosion to remove noise
    refined = this.morphologicalOperation(refined, width, height, 'erode', 1);
    
    // Dilation to restore edges
    refined = this.morphologicalOperation(refined, width, height, 'dilate', 2);
    
    // Gaussian blur for smooth edges
    refined = await this.gaussianBlur(refined, width, height, 1.5);
    
    // Convert to ImageData
    const imageData = new ImageData(width, height);
    for (let i = 0; i < refined.length; i++) {
      const pixelIndex = i * 4;
      imageData.data[pixelIndex + 3] = refined[i];
    }
    
    return imageData;
  }
}

// Green Screen UI
export function GreenScreenSetup() {
  const [isCalibrating, setIsCalibrating] = useState(false);
  const [keyColor, setKeyColor] = useState({ r: 0, g: 255, b: 0 });
  const [settings, setSettings] = useState<ColorKeySettings>({
    targetColor: keyColor,
    tolerance: 0.3,
    smoothness: 0.1,
    spill: 0.2
  });
  
  const calibrate = async () => {
    setIsCalibrating(true);
    try {
      // Capture frame
      const frame = await captureVideoFrame();
      
      // Auto-detect key color
      const detected = await detectKeyColor(frame);
      setKeyColor(detected);
      setSettings(prev => ({ ...prev, targetColor: detected }));
      
      toast.success('Green screen calibrated');
    } finally {
      setIsCalibrating(false);
    }
  };
  
  return (
    <div className="green-screen-setup">
      <div className="setup-preview">
        <VideoPreview showGreenScreenMask settings={settings} />
      </div>
      
      <div className="setup-controls">
        <button 
          onClick={calibrate}
          disabled={isCalibrating}
          className="calibrate-button"
        >
          {isCalibrating ? 'Calibrating...' : 'Auto Calibrate'}
        </button>
        
        <div className="color-picker">
          <label>Key Color</label>
          <ColorPicker
            value={keyColor}
            onChange={(color) => {
              setKeyColor(color);
              setSettings(prev => ({ ...prev, targetColor: color }));
            }}
          />
        </div>
        
        <div className="settings-sliders">
          <SliderControl
            label="Tolerance"
            value={settings.tolerance}
            min={0}
            max={1}
            step={0.01}
            onChange={(v) => setSettings(prev => ({ ...prev, tolerance: v }))}
          />
          
          <SliderControl
            label="Edge Smoothness"
            value={settings.smoothness}
            min={0}
            max={0.5}
            step={0.01}
            onChange={(v) => setSettings(prev => ({ ...prev, smoothness: v }))}
          />
          
          <SliderControl
            label="Spill Suppression"
            value={settings.spill}
            min={0}
            max={1}
            step={0.01}
            onChange={(v) => setSettings(prev => ({ ...prev, spill: v }))}
          />
        </div>
        
        <LightingTips />
      </div>
    </div>
  );
}
```

### 6. Background Effects Library
```typescript
// Animated Background System
export class AnimatedBackgroundSystem {
  private animations = new Map<string, BackgroundAnimation>();
  
  constructor() {
    this.registerBuiltInAnimations();
  }
  
  private registerBuiltInAnimations() {
    // Particle effects
    this.animations.set('particles', new ParticleAnimation({
      particleCount: 100,
      particleSize: { min: 2, max: 6 },
      particleSpeed: { min: 0.5, max: 2 },
      particleColors: ['#ffffff', '#e0e0e0', '#c0c0c0'],
      direction: 'up'
    }));
    
    // Gradient animations
    this.animations.set('gradient-shift', new GradientAnimation({
      colors: [
        { stop: 0, color: '#667eea' },
        { stop: 0.5, color: '#764ba2' },
        { stop: 1, color: '#f093fb' }
      ],
      animationDuration: 10000,
      direction: 'diagonal'
    }));
    
    // Geometric patterns
    this.animations.set('hexagon-pulse', new GeometricAnimation({
      shape: 'hexagon',
      gridSize: 30,
      pulseSpeed: 2,
      colorScheme: 'monochrome'
    }));
    
    // Nature effects
    this.animations.set('aurora', new AuroraAnimation({
      layers: 3,
      speed: 0.5,
      colors: ['#00ff00', '#00ffff', '#ff00ff'],
      intensity: 0.7
    }));
    
    // Tech effects
    this.animations.set('matrix', new MatrixRainAnimation({
      fontSize: 14,
      dropSpeed: { min: 5, max: 15 },
      color: '#0f0',
      density: 0.1
    }));
  }
  
  async renderAnimatedBackground(
    animationId: string,
    canvas: HTMLCanvasElement,
    time: number
  ): Promise<void> {
    const animation = this.animations.get(animationId);
    if (!animation) {
      throw new Error(`Animation ${animationId} not found`);
    }
    
    await animation.render(canvas, time);
  }
}

// Example: Particle Animation
export class ParticleAnimation implements BackgroundAnimation {
  private particles: Particle[] = [];
  private lastTime = 0;
  
  constructor(private config: ParticleConfig) {
    this.initializeParticles();
  }
  
  private initializeParticles() {
    for (let i = 0; i < this.config.particleCount; i++) {
      this.particles.push({
        x: Math.random(),
        y: Math.random(),
        size: this.randomBetween(
          this.config.particleSize.min,
          this.config.particleSize.max
        ),
        speed: this.randomBetween(
          this.config.particleSpeed.min,
          this.config.particleSpeed.max
        ),
        color: this.randomChoice(this.config.particleColors),
        opacity: Math.random() * 0.5 + 0.5
      });
    }
  }
  
  async render(canvas: HTMLCanvasElement, time: number): Promise<void> {
    const ctx = canvas.getContext('2d')!;
    const deltaTime = time - this.lastTime;
    this.lastTime = time;
    
    // Clear canvas with fade effect
    ctx.fillStyle = 'rgba(0, 0, 0, 0.05)';
    ctx.fillRect(0, 0, canvas.width, canvas.height);
    
    // Update and draw particles
    for (const particle of this.particles) {
      // Update position
      if (this.config.direction === 'up') {
        particle.y -= particle.speed * deltaTime / 1000;
        if (particle.y < -0.1) {
          particle.y = 1.1;
          particle.x = Math.random();
        }
      }
      
      // Draw particle
      ctx.fillStyle = particle.color;
      ctx.globalAlpha = particle.opacity;
      ctx.beginPath();
      ctx.arc(
        particle.x * canvas.width,
        particle.y * canvas.height,
        particle.size,
        0,
        Math.PI * 2
      );
      ctx.fill();
    }
    
    ctx.globalAlpha = 1;
  }
}

// Background Effects Gallery
export function BackgroundEffectsGallery() {
  const [selectedEffect, setSelectedEffect] = useState<string>();
  const [customParams, setCustomParams] = useState<any>({});
  const [previewActive, setPreviewActive] = useState(false);
  
  const effects = [
    {
      id: 'particles',
      name: 'Floating Particles',
      category: 'animated',
      preview: '/effects/particles.mp4'
    },
    {
      id: 'gradient-shift',
      name: 'Color Gradient',
      category: 'animated',
      preview: '/effects/gradient.mp4'
    },
    {
      id: 'bokeh',
      name: 'Bokeh Lights',
      category: 'static',
      preview: '/effects/bokeh.jpg'
    },
    {
      id: 'aurora',
      name: 'Aurora Borealis',
      category: 'animated',
      preview: '/effects/aurora.mp4'
    }
  ];
  
  return (
    <div className="background-effects-gallery">
      <div className="effects-categories">
        <button className="category-btn active">All</button>
        <button className="category-btn">Animated</button>
        <button className="category-btn">Static</button>
        <button className="category-btn">Interactive</button>
      </div>
      
      <div className="effects-grid">
        {effects.map(effect => (
          <EffectCard
            key={effect.id}
            effect={effect}
            isSelected={selectedEffect === effect.id}
            onSelect={() => setSelectedEffect(effect.id)}
            onPreview={() => {
              setSelectedEffect(effect.id);
              setPreviewActive(true);
            }}
          />
        ))}
      </div>
      
      {selectedEffect && (
        <EffectCustomizer
          effectId={selectedEffect}
          params={customParams}
          onChange={setCustomParams}
          onApply={() => applyEffect(selectedEffect, customParams)}
        />
      )}
      
      {previewActive && (
        <EffectPreviewModal
          effectId={selectedEffect!}
          params={customParams}
          onClose={() => setPreviewActive(false)}
        />
      )}
    </div>
  );
}
```

## Dependencies
- TensorFlow.js with BodyPix model
- WebGL/WebGPU for GPU acceleration
- Web Workers for parallel processing
- MediaPipe for face detection
- Canvas API for compositing

## Estimated Effort
**5 days**
- 1 day: Core segmentation engine
- 1 day: Background effects and compositing
- 1 day: UI components and gallery
- 1 day: Performance optimization
- 1 day: Green screen and advanced features

## Notes
- Implement background caching for performance
- Add support for virtual props and stickers
- Consider bandwidth optimization for video backgrounds
- Implement background recommendation based on context
- Add accessibility features for motion sensitivity
- Support for portrait mode depth data
- Create background creation tools