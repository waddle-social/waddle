# Issue #52: Voice Commands

## User Story
As a **waddle user**, I want to **control waddle features using natural language voice commands** so that **I can interact hands-free and more efficiently during conversations**.

## Description
Implement a comprehensive voice command system that allows users to control various waddle features through natural language processing. This includes channel navigation, user actions, settings adjustments, and workflow automation through voice input, making the platform more accessible and efficient.

## Acceptance Criteria
- [ ] Wake word detection ("Hey Waddle")
- [ ] Natural language command processing
- [ ] Multi-language voice commands
- [ ] Context-aware command interpretation
- [ ] Command confirmation and feedback
- [ ] Voice command history
- [ ] Custom command creation
- [ ] Accessibility compliance

## Technical Implementation

### 1. Voice Command Engine
```typescript
// Voice Command System
export interface VoiceCommandConfig {
  wakeWord: string;
  language: string;
  sensitivity: number;
  confirmationRequired: boolean;
  customCommands: CustomCommand[];
  shortcuts: VoiceShortcut[];
}

export class VoiceCommandEngine {
  private isListening = false;
  private wakeWordDetector: WakeWordDetector;
  private commandProcessor: CommandProcessor;
  private audioStream: MediaStream | null = null;
  
  constructor(
    private config: VoiceCommandConfig,
    private nlpService: NLPService,
    private actionExecutor: ActionExecutor
  ) {
    this.wakeWordDetector = new WakeWordDetector(config.wakeWord);
    this.commandProcessor = new CommandProcessor(nlpService);
  }
  
  async initialize(): Promise<void> {
    // Request microphone permission
    this.audioStream = await navigator.mediaDevices.getUserMedia({ 
      audio: {
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true
      } 
    });
    
    // Initialize wake word detection
    await this.wakeWordDetector.initialize(this.audioStream);
    
    // Start listening for wake word
    this.wakeWordDetector.on('detected', () => {
      this.handleWakeWordDetected();
    });
  }
  
  private async handleWakeWordDetected() {
    console.log('Wake word detected!');
    
    // Visual/audio feedback
    await this.provideFeedback('listening');
    
    // Start command recording
    const command = await this.recordCommand();
    
    if (command) {
      await this.processCommand(command);
    }
  }
  
  private async recordCommand(): Promise<AudioBuffer | null> {
    const recorder = new CommandRecorder(this.audioStream!);
    
    // Record with timeout and silence detection
    const recording = await recorder.record({
      maxDuration: 10000, // 10 seconds max
      silenceThreshold: 500, // 500ms of silence ends recording
      noiseGate: -40 // dB threshold
    });
    
    return recording;
  }
  
  private async processCommand(audioBuffer: AudioBuffer) {
    try {
      // Convert to text using speech recognition
      const text = await this.speechToText(audioBuffer);
      
      // Process with NLP
      const intent = await this.commandProcessor.process(text, {
        context: this.getCurrentContext(),
        userId: this.getCurrentUserId(),
        language: this.config.language
      });
      
      // Confirm if required
      if (this.config.confirmationRequired && intent.requiresConfirmation) {
        const confirmed = await this.confirmCommand(intent);
        if (!confirmed) {
          await this.provideFeedback('cancelled');
          return;
        }
      }
      
      // Execute command
      const result = await this.actionExecutor.execute(intent);
      
      // Provide result feedback
      await this.provideResultFeedback(result);
      
      // Log command
      await this.logCommand(text, intent, result);
      
    } catch (error) {
      await this.handleCommandError(error);
    }
  }
  
  private async speechToText(audioBuffer: AudioBuffer): Promise<string> {
    // Use Web Speech API or cloud service
    const recognition = new (window as any).webkitSpeechRecognition();
    recognition.lang = this.config.language;
    recognition.continuous = false;
    recognition.interimResults = false;
    
    return new Promise((resolve, reject) => {
      recognition.onresult = (event: any) => {
        const transcript = event.results[0][0].transcript;
        resolve(transcript);
      };
      
      recognition.onerror = reject;
      
      // Convert AudioBuffer to audio blob
      const blob = this.audioBufferToBlob(audioBuffer);
      recognition.start();
      
      // Play audio to recognition
      const audio = new Audio(URL.createObjectURL(blob));
      audio.play();
    });
  }
}

// Command Processor with NLP
export class CommandProcessor {
  private intentClassifier: IntentClassifier;
  private entityExtractor: EntityExtractor;
  
  constructor(private nlpService: NLPService) {
    this.intentClassifier = new IntentClassifier();
    this.entityExtractor = new EntityExtractor();
  }
  
  async process(
    text: string,
    context: CommandContext
  ): Promise<CommandIntent> {
    // Preprocess text
    const normalized = this.normalizeText(text);
    
    // Extract intent and entities
    const [intent, entities] = await Promise.all([
      this.classifyIntent(normalized, context),
      this.extractEntities(normalized, context)
    ]);
    
    // Resolve references
    const resolved = await this.resolveReferences(intent, entities, context);
    
    // Build command structure
    return {
      id: generateId(),
      text,
      normalized,
      intent: resolved.intent,
      entities: resolved.entities,
      confidence: resolved.confidence,
      requiresConfirmation: this.requiresConfirmation(resolved.intent),
      timestamp: Date.now()
    };
  }
  
  private async classifyIntent(
    text: string,
    context: CommandContext
  ): Promise<IntentClassification> {
    // Use NLP service for intent classification
    const response = await this.nlpService.classify({
      text,
      model: 'command-intent-v2',
      context: {
        currentChannel: context.currentChannel,
        userRole: context.userRole,
        previousCommands: context.commandHistory.slice(-3)
      }
    });
    
    return {
      primary: response.intents[0],
      alternatives: response.intents.slice(1),
      confidence: response.confidence
    };
  }
  
  private async extractEntities(
    text: string,
    context: CommandContext
  ): Promise<ExtractedEntity[]> {
    const entities = await this.nlpService.extractEntities({
      text,
      types: ['user', 'channel', 'time', 'action', 'setting'],
      context
    });
    
    // Enhance with local knowledge
    return this.enhanceEntities(entities, context);
  }
  
  private requiresConfirmation(intent: string): boolean {
    const dangerousIntents = [
      'delete', 'remove', 'kick', 'ban', 'mute',
      'change_settings', 'invite_all', 'record'
    ];
    
    return dangerousIntents.includes(intent);
  }
}
```

### 2. Command Actions & Execution
```typescript
// Action Executor
export class ActionExecutor {
  private actionHandlers = new Map<string, ActionHandler>();
  
  constructor(
    private waddleAPI: WaddleAPI,
    private uiController: UIController
  ) {
    this.registerDefaultHandlers();
  }
  
  private registerDefaultHandlers() {
    // Navigation commands
    this.register('navigate_channel', async (params) => {
      const channel = await this.resolveChannel(params.channel);
      await this.uiController.navigateToChannel(channel.id);
      return { success: true, message: `Switched to ${channel.name}` };
    });
    
    // User actions
    this.register('mute_user', async (params) => {
      const user = await this.resolveUser(params.user);
      await this.waddleAPI.muteUser(user.id, params.duration);
      return { success: true, message: `Muted ${user.name}` };
    });
    
    // Message actions
    this.register('send_message', async (params) => {
      await this.waddleAPI.sendMessage({
        channelId: params.channelId || this.getCurrentChannelId(),
        content: params.message,
        type: 'text'
      });
      return { success: true, message: 'Message sent' };
    });
    
    // Settings commands
    this.register('change_setting', async (params) => {
      const setting = this.resolveSetting(params.setting);
      await this.waddleAPI.updateSetting(setting.key, params.value);
      return { success: true, message: `${setting.name} updated` };
    });
    
    // Search commands
    this.register('search', async (params) => {
      const results = await this.waddleAPI.search({
        query: params.query,
        type: params.searchType || 'all',
        filters: params.filters
      });
      
      await this.uiController.showSearchResults(results);
      return { 
        success: true, 
        message: `Found ${results.length} results` 
      };
    });
    
    // Call controls
    this.register('call_action', async (params) => {
      switch (params.action) {
        case 'mute':
          await this.waddleAPI.toggleMute();
          break;
        case 'video':
          await this.waddleAPI.toggleVideo();
          break;
        case 'screenshare':
          await this.waddleAPI.toggleScreenShare();
          break;
        case 'end':
          await this.waddleAPI.endCall();
          break;
      }
      return { success: true, message: `${params.action} executed` };
    });
    
    // Workflow commands
    this.register('start_workflow', async (params) => {
      const workflow = await this.resolveWorkflow(params.workflow);
      const instance = await this.waddleAPI.startWorkflow(workflow.id, params);
      return { 
        success: true, 
        message: `Started ${workflow.name}`,
        data: instance
      };
    });
  }
  
  async execute(intent: CommandIntent): Promise<ActionResult> {
    const handler = this.actionHandlers.get(intent.intent);
    
    if (!handler) {
      return {
        success: false,
        error: `Unknown command: ${intent.intent}`,
        suggestions: this.getSuggestions(intent)
      };
    }
    
    try {
      // Validate parameters
      const validation = await this.validateParams(intent);
      if (!validation.valid) {
        return {
          success: false,
          error: validation.error,
          missingParams: validation.missing
        };
      }
      
      // Execute action
      const result = await handler(intent.entities);
      
      // Post-execution hooks
      await this.postExecute(intent, result);
      
      return result;
      
    } catch (error) {
      return {
        success: false,
        error: error.message,
        retry: true
      };
    }
  }
  
  private async validateParams(
    intent: CommandIntent
  ): Promise<ValidationResult> {
    const schema = this.getParamSchema(intent.intent);
    const params = intent.entities;
    
    const missing: string[] = [];
    
    for (const [key, config] of Object.entries(schema)) {
      if (config.required && !params[key]) {
        missing.push(key);
      }
    }
    
    if (missing.length > 0) {
      return {
        valid: false,
        missing,
        error: `Missing required parameters: ${missing.join(', ')}`
      };
    }
    
    return { valid: true };
  }
}

// Custom Command Builder
export class CustomCommandBuilder {
  private commands = new Map<string, CustomCommand>();
  
  async createCommand(
    userId: string,
    config: CustomCommandConfig
  ): Promise<CustomCommand> {
    const command: CustomCommand = {
      id: generateId(),
      userId,
      trigger: config.trigger,
      description: config.description,
      actions: config.actions,
      parameters: config.parameters || [],
      context: config.context || 'global',
      enabled: true,
      createdAt: Date.now()
    };
    
    // Validate command
    await this.validateCommand(command);
    
    // Store command
    await this.storeCommand(command);
    
    // Register with command processor
    await this.registerCommand(command);
    
    return command;
  }
  
  private async validateCommand(command: CustomCommand) {
    // Check trigger uniqueness
    if (this.commands.has(command.trigger)) {
      throw new Error('Command trigger already exists');
    }
    
    // Validate actions
    for (const action of command.actions) {
      if (!this.isValidAction(action)) {
        throw new Error(`Invalid action: ${action.type}`);
      }
    }
    
    // Validate parameters
    for (const param of command.parameters) {
      if (!this.isValidParamType(param.type)) {
        throw new Error(`Invalid parameter type: ${param.type}`);
      }
    }
  }
  
  async executeCustomCommand(
    commandId: string,
    params: Record<string, any>
  ): Promise<ActionResult> {
    const command = this.commands.get(commandId);
    if (!command) {
      throw new Error('Command not found');
    }
    
    // Execute actions in sequence
    const results: any[] = [];
    
    for (const action of command.actions) {
      const result = await this.executeAction(action, params);
      results.push(result);
      
      // Check if should continue
      if (action.stopOnError && !result.success) {
        return {
          success: false,
          error: `Action failed: ${action.type}`,
          results
        };
      }
    }
    
    return {
      success: true,
      message: `Executed ${command.trigger}`,
      results
    };
  }
}
```

### 3. Voice UI Components
```tsx
export function VoiceCommandInterface() {
  const [isListening, setIsListening] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [status, setStatus] = useState<'idle' | 'listening' | 'processing'>('idle');
  const [commandHistory, setCommandHistory] = useState<CommandHistoryItem[]>([]);
  
  const voiceEngine = useVoiceCommandEngine();
  
  useEffect(() => {
    // Initialize voice engine
    voiceEngine.initialize();
    
    // Subscribe to events
    voiceEngine.on('listening', () => {
      setIsListening(true);
      setStatus('listening');
    });
    
    voiceEngine.on('processing', (text: string) => {
      setTranscript(text);
      setStatus('processing');
    });
    
    voiceEngine.on('complete', (result: CommandResult) => {
      setCommandHistory(prev => [{
        id: generateId(),
        text: transcript,
        result,
        timestamp: Date.now()
      }, ...prev].slice(0, 10));
      
      setIsListening(false);
      setStatus('idle');
      setTranscript('');
    });
    
    return () => voiceEngine.cleanup();
  }, []);
  
  const toggleListening = () => {
    if (isListening) {
      voiceEngine.stop();
    } else {
      voiceEngine.start();
    }
  };
  
  return (
    <div className="voice-command-interface">
      <VoiceButton
        isListening={isListening}
        status={status}
        onClick={toggleListening}
      />
      
      {transcript && (
        <TranscriptDisplay
          text={transcript}
          status={status}
        />
      )}
      
      <CommandHistory
        history={commandHistory}
        onRetry={(item) => voiceEngine.retry(item.text)}
      />
      
      <VoiceSettings />
    </div>
  );
}

function VoiceButton({ 
  isListening, 
  status, 
  onClick 
}: { 
  isListening: boolean;
  status: string;
  onClick: () => void;
}) {
  return (
    <button
      className={`voice-button ${status}`}
      onClick={onClick}
      aria-label={isListening ? 'Stop listening' : 'Start voice command'}
    >
      <div className="voice-button-content">
        {status === 'listening' && <ListeningAnimation />}
        {status === 'processing' && <ProcessingSpinner />}
        {status === 'idle' && <MicrophoneIcon />}
      </div>
      
      <span className="voice-button-label">
        {status === 'listening' && 'Listening...'}
        {status === 'processing' && 'Processing...'}
        {status === 'idle' && 'Voice Command'}
      </span>
    </button>
  );
}

function TranscriptDisplay({ 
  text, 
  status 
}: { 
  text: string;
  status: string;
}) {
  return (
    <div className="transcript-display">
      <div className="transcript-text">{text}</div>
      {status === 'processing' && (
        <div className="transcript-status">
          Understanding your command...
        </div>
      )}
    </div>
  );
}

function CommandHistory({ 
  history, 
  onRetry 
}: { 
  history: CommandHistoryItem[];
  onRetry: (item: CommandHistoryItem) => void;
}) {
  if (history.length === 0) return null;
  
  return (
    <div className="command-history">
      <h4>Recent Commands</h4>
      <div className="history-list">
        {history.map(item => (
          <div key={item.id} className="history-item">
            <div className="history-text">{item.text}</div>
            <div className="history-result">
              {item.result.success ? (
                <CheckIcon className="success" />
              ) : (
                <XIcon className="error" />
              )}
              <span>{item.result.message}</span>
            </div>
            {!item.result.success && (
              <button
                className="retry-button"
                onClick={() => onRetry(item)}
              >
                Retry
              </button>
            )}
          </div>
        ))}
      </div>
    </div>
  );
}
```

### 4. Natural Language Processing
```typescript
// NLP Service Integration
export class NLPService {
  private modelCache = new Map<string, any>();
  
  constructor(
    private apiKey: string,
    private config: NLPConfig
  ) {}
  
  async processCommand(
    text: string,
    context: ProcessingContext
  ): Promise<NLPResult> {
    // Tokenize and preprocess
    const tokens = await this.tokenize(text);
    
    // Intent classification
    const intent = await this.classifyIntent(tokens, context);
    
    // Entity extraction
    const entities = await this.extractEntities(tokens, context);
    
    // Coreference resolution
    const resolved = await this.resolveReferences(entities, context);
    
    // Sentiment analysis (for tone)
    const sentiment = await this.analyzeSentiment(text);
    
    return {
      originalText: text,
      tokens,
      intent,
      entities: resolved,
      sentiment,
      confidence: this.calculateConfidence(intent, entities)
    };
  }
  
  private async classifyIntent(
    tokens: Token[],
    context: ProcessingContext
  ): Promise<IntentResult> {
    // Use transformer model for intent classification
    const model = await this.loadModel('intent-classifier');
    
    const embeddings = await this.getEmbeddings(tokens);
    const contextVector = this.encodeContext(context);
    
    const prediction = await model.predict({
      embeddings,
      context: contextVector
    });
    
    return {
      primary: prediction.intents[0],
      alternatives: prediction.intents.slice(1, 4),
      scores: prediction.scores
    };
  }
  
  private async extractEntities(
    tokens: Token[],
    context: ProcessingContext
  ): Promise<Entity[]> {
    const model = await this.loadModel('entity-extractor');
    
    const results = await model.extract({
      tokens,
      context: context.recentEntities
    });
    
    // Post-process entities
    return results.map(entity => ({
      ...entity,
      normalized: this.normalizeEntity(entity),
      resolved: this.resolveEntity(entity, context)
    }));
  }
  
  private async resolveReferences(
    entities: Entity[],
    context: ProcessingContext
  ): Promise<Entity[]> {
    const resolved = [...entities];
    
    for (let i = 0; i < resolved.length; i++) {
      const entity = resolved[i];
      
      // Resolve pronouns
      if (entity.type === 'pronoun') {
        const referent = this.findReferent(entity, context);
        if (referent) {
          resolved[i] = {
            ...entity,
            resolved: referent,
            originalText: entity.text
          };
        }
      }
      
      // Resolve relative references
      if (entity.type === 'relative') {
        const absolute = this.resolveRelative(entity, context);
        if (absolute) {
          resolved[i] = {
            ...entity,
            resolved: absolute
          };
        }
      }
    }
    
    return resolved;
  }
}

// Command Grammar & Patterns
export class CommandGrammar {
  private patterns: CommandPattern[] = [
    // Navigation
    {
      pattern: /^(go to|switch to|open) (?<target>.+)$/i,
      intent: 'navigate',
      extract: (match) => ({ target: match.groups.target })
    },
    
    // User actions
    {
      pattern: /^(mute|unmute) (?<user>.+?)( for (?<duration>\d+) (?<unit>seconds?|minutes?|hours?))?$/i,
      intent: 'mute_user',
      extract: (match) => ({
        user: match.groups.user,
        duration: match.groups.duration ? 
          this.parseDuration(match.groups.duration, match.groups.unit) : null
      })
    },
    
    // Message sending
    {
      pattern: /^(send|say|tell) (?<recipient>.+?) (?<message>.+)$/i,
      intent: 'send_message',
      extract: (match) => ({
        recipient: match.groups.recipient,
        message: match.groups.message
      })
    },
    
    // Search
    {
      pattern: /^(search|find|look for) (?<query>.+?)( in (?<scope>.+))?$/i,
      intent: 'search',
      extract: (match) => ({
        query: match.groups.query,
        scope: match.groups.scope || 'all'
      })
    },
    
    // Settings
    {
      pattern: /^(set|change|update) (?<setting>.+?) to (?<value>.+)$/i,
      intent: 'change_setting',
      extract: (match) => ({
        setting: match.groups.setting,
        value: match.groups.value
      })
    },
    
    // Call controls
    {
      pattern: /^(mute|unmute) (?:my )?(?<device>mic|microphone|audio)$/i,
      intent: 'toggle_mute',
      extract: (match) => ({ device: 'microphone' })
    },
    
    // Workflows
    {
      pattern: /^(start|begin|initiate) (?<workflow>.+?)( with (?<params>.+))?$/i,
      intent: 'start_workflow',
      extract: (match) => ({
        workflow: match.groups.workflow,
        params: match.groups.params ? 
          this.parseParams(match.groups.params) : {}
      })
    }
  ];
  
  match(text: string): CommandMatch | null {
    const normalized = text.trim().toLowerCase();
    
    for (const pattern of this.patterns) {
      const match = normalized.match(pattern.pattern);
      if (match) {
        return {
          intent: pattern.intent,
          params: pattern.extract(match),
          confidence: 0.9
        };
      }
    }
    
    return null;
  }
  
  private parseDuration(value: string, unit: string): number {
    const num = parseInt(value);
    const multipliers: Record<string, number> = {
      'second': 1000,
      'seconds': 1000,
      'minute': 60000,
      'minutes': 60000,
      'hour': 3600000,
      'hours': 3600000
    };
    
    return num * (multipliers[unit] || 1000);
  }
}
```

### 5. Voice Feedback System
```typescript
// Voice Feedback Manager
export class VoiceFeedbackManager {
  private tts: TextToSpeech;
  private soundEffects: SoundEffectPlayer;
  
  constructor(
    private config: VoiceFeedbackConfig
  ) {
    this.tts = new TextToSpeech(config.voice);
    this.soundEffects = new SoundEffectPlayer();
  }
  
  async provideFeedback(
    type: FeedbackType,
    data?: any
  ): Promise<void> {
    switch (type) {
      case 'listening':
        await this.soundEffects.play('chime_up');
        break;
        
      case 'processing':
        // Visual feedback only
        break;
        
      case 'success':
        await this.soundEffects.play('success');
        if (data?.message && this.config.spokenFeedback) {
          await this.speak(data.message);
        }
        break;
        
      case 'error':
        await this.soundEffects.play('error');
        if (data?.message) {
          await this.speak(`Sorry, ${data.message}`);
        }
        break;
        
      case 'confirmation':
        await this.speak(data.prompt);
        break;
    }
  }
  
  async speak(text: string): Promise<void> {
    if (!this.config.spokenFeedback) return;
    
    const utterance = new SpeechSynthesisUtterance(text);
    utterance.voice = await this.getVoice();
    utterance.rate = this.config.speechRate || 1.0;
    utterance.pitch = this.config.pitch || 1.0;
    utterance.volume = this.config.volume || 0.8;
    
    return new Promise((resolve) => {
      utterance.onend = () => resolve();
      speechSynthesis.speak(utterance);
    });
  }
  
  private async getVoice(): Promise<SpeechSynthesisVoice | null> {
    const voices = speechSynthesis.getVoices();
    
    // Find preferred voice
    const preferred = voices.find(v => 
      v.name === this.config.voice || 
      v.lang.startsWith(this.config.language)
    );
    
    return preferred || voices[0];
  }
}

// Confirmation Dialog
export function VoiceConfirmationDialog({ 
  command, 
  onConfirm, 
  onCancel 
}: {
  command: CommandIntent;
  onConfirm: () => void;
  onCancel: () => void;
}) {
  const [countdown, setCountdown] = useState(5);
  
  useEffect(() => {
    const timer = setInterval(() => {
      setCountdown(c => {
        if (c <= 1) {
          clearInterval(timer);
          onCancel();
          return 0;
        }
        return c - 1;
      });
    }, 1000);
    
    return () => clearInterval(timer);
  }, []);
  
  return (
    <div className="voice-confirmation-dialog">
      <div className="confirmation-content">
        <h3>Confirm Command</h3>
        <p className="command-text">{command.text}</p>
        <div className="command-interpretation">
          <span className="intent">{command.intent}</span>
          {Object.entries(command.entities).map(([key, value]) => (
            <span key={key} className="entity">
              {key}: {value}
            </span>
          ))}
        </div>
      </div>
      
      <div className="confirmation-actions">
        <button 
          className="confirm-button"
          onClick={onConfirm}
          autoFocus
        >
          Confirm
        </button>
        <button 
          className="cancel-button"
          onClick={onCancel}
        >
          Cancel ({countdown})
        </button>
      </div>
      
      <div className="voice-hint">
        Say "yes" to confirm or "no" to cancel
      </div>
    </div>
  );
}
```

### 6. Custom Commands & Automation
```typescript
// Custom Command Creator
export function CustomCommandCreator() {
  const [command, setCommand] = useState<Partial<CustomCommand>>({
    trigger: '',
    description: '',
    actions: []
  });
  
  const [isRecording, setIsRecording] = useState(false);
  const [testResult, setTestResult] = useState<any>();
  
  const addAction = (action: CommandAction) => {
    setCommand(prev => ({
      ...prev,
      actions: [...(prev.actions || []), action]
    }));
  };
  
  const recordTrigger = async () => {
    setIsRecording(true);
    try {
      const audio = await recordAudio({ maxDuration: 5000 });
      const text = await speechToText(audio);
      setCommand(prev => ({ ...prev, trigger: text }));
    } finally {
      setIsRecording(false);
    }
  };
  
  const testCommand = async () => {
    const result = await api.testCustomCommand(command);
    setTestResult(result);
  };
  
  const saveCommand = async () => {
    await api.createCustomCommand(command as CustomCommand);
    toast.success('Custom command created!');
  };
  
  return (
    <div className="custom-command-creator">
      <h3>Create Custom Voice Command</h3>
      
      <div className="command-trigger">
        <label>Trigger Phrase</label>
        <div className="trigger-input">
          <input
            type="text"
            value={command.trigger}
            onChange={(e) => setCommand(prev => ({
              ...prev,
              trigger: e.target.value
            }))}
            placeholder="e.g., 'start daily standup'"
          />
          <button 
            onClick={recordTrigger}
            disabled={isRecording}
          >
            {isRecording ? <RecordingIcon /> : <MicIcon />}
          </button>
        </div>
      </div>
      
      <div className="command-description">
        <label>Description</label>
        <textarea
          value={command.description}
          onChange={(e) => setCommand(prev => ({
            ...prev,
            description: e.target.value
          }))}
          placeholder="What does this command do?"
        />
      </div>
      
      <div className="command-actions">
        <h4>Actions</h4>
        <ActionList 
          actions={command.actions || []}
          onEdit={(index, action) => {
            const newActions = [...(command.actions || [])];
            newActions[index] = action;
            setCommand(prev => ({ ...prev, actions: newActions }));
          }}
          onDelete={(index) => {
            const newActions = command.actions?.filter((_, i) => i !== index);
            setCommand(prev => ({ ...prev, actions: newActions }));
          }}
        />
        <ActionBuilder onAdd={addAction} />
      </div>
      
      <div className="command-parameters">
        <h4>Parameters</h4>
        <ParameterBuilder
          parameters={command.parameters || []}
          onChange={(params) => setCommand(prev => ({
            ...prev,
            parameters: params
          }))}
        />
      </div>
      
      <div className="command-actions-footer">
        <button onClick={testCommand}>Test Command</button>
        <button onClick={saveCommand} disabled={!isValid(command)}>
          Save Command
        </button>
      </div>
      
      {testResult && (
        <TestResultDisplay result={testResult} />
      )}
    </div>
  );
}

// Voice Command Analytics
export class VoiceCommandAnalytics {
  async trackCommand(event: CommandEvent) {
    await this.analytics.track('voice_command', {
      commandId: event.commandId,
      intent: event.intent,
      success: event.success,
      duration: event.duration,
      confidence: event.confidence,
      errorType: event.error?.type
    });
  }
  
  async getCommandInsights(
    userId: string,
    timeRange: TimeRange
  ): Promise<CommandInsights> {
    const events = await this.getCommandEvents(userId, timeRange);
    
    return {
      totalCommands: events.length,
      successRate: this.calculateSuccessRate(events),
      mostUsedCommands: this.getMostUsedCommands(events),
      averageConfidence: this.calculateAverageConfidence(events),
      errorPatterns: this.analyzeErrors(events),
      usageTimeline: this.buildUsageTimeline(events),
      suggestions: await this.generateSuggestions(events)
    };
  }
  
  private async generateSuggestions(
    events: CommandEvent[]
  ): Promise<CommandSuggestion[]> {
    const suggestions: CommandSuggestion[] = [];
    
    // Suggest shortcuts for frequently used commands
    const frequent = this.getMostUsedCommands(events).slice(0, 3);
    for (const cmd of frequent) {
      if (cmd.count > 10 && !cmd.hasShortcut) {
        suggestions.push({
          type: 'create_shortcut',
          command: cmd.intent,
          reason: `You use "${cmd.intent}" frequently`
        });
      }
    }
    
    // Suggest fixing common errors
    const errors = this.analyzeErrors(events);
    for (const error of errors) {
      if (error.count > 3) {
        suggestions.push({
          type: 'improve_recognition',
          pattern: error.pattern,
          suggestion: error.suggestion
        });
      }
    }
    
    return suggestions;
  }
}
```

## Dependencies
- Web Speech API for speech recognition
- NLP service (e.g., Google Cloud Natural Language)
- WebRTC for audio capture
- Wake word detection library
- Text-to-speech engine

## Estimated Effort
**5 days**
- 1 day: Voice command engine setup
- 1 day: NLP integration and command processing
- 1 day: Action execution framework
- 1 day: UI components and feedback
- 1 day: Custom commands and analytics

## Notes
- Implement offline command processing for basic commands
- Add support for command chaining
- Consider privacy implications of always-listening
- Implement command shortcuts and macros
- Add voice training for better recognition
- Support multiple wake words
- Implement command disambiguation