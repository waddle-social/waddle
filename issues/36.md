# Issue #36: Transcription

## User Story
As a **waddle member**, I want to **see real-time transcriptions of voice conversations** so that **I can follow along with text, search past conversations, and maintain accessibility for all users**.

## Description
Implement real-time transcription for voice channels using speech-to-text technology. This includes live captions during voice conversations, searchable transcription history, speaker identification, and multi-language support with translation capabilities.

## Acceptance Criteria
- [ ] Real-time voice transcription
- [ ] Speaker identification
- [ ] Multi-language support
- [ ] Live captions display
- [ ] Transcription history storage
- [ ] Search transcriptions
- [ ] Export transcriptions
- [ ] Accuracy improvements over time
- [ ] Privacy controls
- [ ] Translation between languages

## Technical Implementation

### 1. Transcription Service Architecture
```typescript
// Transcription Types
export interface TranscriptionSession {
  id: string;
  waddleId: string;
  channelId: string;
  startTime: Date;
  endTime?: Date;
  language: string;
  participants: TranscriptionParticipant[];
  segments: TranscriptionSegment[];
  status: 'active' | 'processing' | 'completed' | 'error';
  metadata: {
    totalDuration: number;
    wordCount: number;
    speakerCount: number;
    languages: string[];
  };
}

export interface TranscriptionSegment {
  id: string;
  sessionId: string;
  speakerId: string;
  userId?: string;
  text: string;
  startTime: number;
  endTime: number;
  confidence: number;
  language: string;
  words: WordTiming[];
  translation?: {
    text: string;
    language: string;
  };
}

export interface WordTiming {
  word: string;
  start: number;
  end: number;
  confidence: number;
}

export interface TranscriptionParticipant {
  speakerId: string;
  userId?: string;
  username?: string;
  voiceSample?: ArrayBuffer;
  totalSpeakingTime: number;
  segments: number;
}

// Real-time Transcription Service
export class RealtimeTranscriptionService {
  private sessions = new Map<string, TranscriptionSession>();
  private speechRecognizers = new Map<string, SpeechRecognizer>();
  private speakerIdentifier: SpeakerIdentificationService;
  private translator: TranslationService;
  
  constructor(
    private transcriptionProvider: TranscriptionProvider,
    private db: D1Database,
    private pubsub: PubSubService
  ) {
    this.speakerIdentifier = new SpeakerIdentificationService();
    this.translator = new TranslationService();
  }
  
  async startTranscription(
    channelId: string,
    options: TranscriptionOptions
  ): Promise<TranscriptionSession> {
    // Check if session already exists
    const existingSession = this.getActiveSession(channelId);
    if (existingSession) {
      return existingSession;
    }
    
    // Create new session
    const session: TranscriptionSession = {
      id: generateId(),
      waddleId: options.waddleId,
      channelId,
      startTime: new Date(),
      language: options.language || 'auto',
      participants: [],
      segments: [],
      status: 'active',
      metadata: {
        totalDuration: 0,
        wordCount: 0,
        speakerCount: 0,
        languages: []
      }
    };
    
    // Initialize speech recognizer
    const recognizer = await this.createSpeechRecognizer(session, options);
    
    // Store session
    this.sessions.set(channelId, session);
    this.speechRecognizers.set(channelId, recognizer);
    
    // Save to database
    await this.saveSession(session);
    
    // Start processing audio
    await this.startAudioProcessing(channelId);
    
    return session;
  }
  
  async stopTranscription(channelId: string): Promise<void> {
    const session = this.sessions.get(channelId);
    const recognizer = this.speechRecognizers.get(channelId);
    
    if (!session || !recognizer) {
      throw new Error('No active transcription session');
    }
    
    // Stop recognizer
    await recognizer.stop();
    
    // Update session
    session.endTime = new Date();
    session.status = 'processing';
    
    // Post-process transcription
    await this.postProcessTranscription(session);
    
    // Clean up
    this.sessions.delete(channelId);
    this.speechRecognizers.delete(channelId);
    
    // Update database
    await this.updateSession(session);
  }
  
  private async createSpeechRecognizer(
    session: TranscriptionSession,
    options: TranscriptionOptions
  ): Promise<SpeechRecognizer> {
    const recognizer = new SpeechRecognizer({
      language: options.language || 'auto',
      continuous: true,
      interimResults: true,
      maxAlternatives: 3,
      profanityFilter: options.profanityFilter ?? true,
      punctuation: true,
      speakerDiarization: true,
      model: options.model || 'latest_long'
    });
    
    // Handle recognition results
    recognizer.on('result', async (result) => {
      await this.handleRecognitionResult(session, result);
    });
    
    // Handle errors
    recognizer.on('error', (error) => {
      console.error('Recognition error:', error);
      this.handleRecognitionError(session, error);
    });
    
    return recognizer;
  }
  
  private async handleRecognitionResult(
    session: TranscriptionSession,
    result: RecognitionResult
  ): Promise<void> {
    // Identify speaker
    const speakerId = await this.speakerIdentifier.identifySpeaker(
      result.audio,
      session.participants
    );
    
    // Create segment
    const segment: TranscriptionSegment = {
      id: generateId(),
      sessionId: session.id,
      speakerId,
      userId: await this.matchSpeakerToUser(speakerId, session),
      text: result.text,
      startTime: result.startTime,
      endTime: result.endTime,
      confidence: result.confidence,
      language: result.language || session.language,
      words: result.words || []
    };
    
    // Add translation if needed
    if (session.metadata.languages.length > 1) {
      segment.translation = await this.translateSegment(segment, session);
    }
    
    // Update session
    session.segments.push(segment);
    this.updateSessionMetadata(session, segment);
    
    // Broadcast update
    await this.broadcastTranscriptionUpdate(session, segment);
    
    // Save segment
    await this.saveSegment(segment);
  }
  
  private async broadcastTranscriptionUpdate(
    session: TranscriptionSession,
    segment: TranscriptionSegment
  ): Promise<void> {
    // Broadcast to channel participants
    await this.pubsub.publish(`transcription:${session.channelId}`, {
      type: 'segment',
      sessionId: session.id,
      segment,
      timestamp: Date.now()
    });
    
    // Update live caption subscribers
    await this.pubsub.publish(`captions:${session.channelId}`, {
      speakerId: segment.speakerId,
      text: segment.text,
      timestamp: segment.startTime
    });
  }
  
  private async postProcessTranscription(
    session: TranscriptionSession
  ): Promise<void> {
    // Improve accuracy with context
    const improvedSegments = await this.improveAccuracy(session.segments);
    
    // Merge short segments
    const mergedSegments = this.mergeShortSegments(improvedSegments);
    
    // Add punctuation and formatting
    const formattedSegments = await this.formatTranscription(mergedSegments);
    
    // Update session
    session.segments = formattedSegments;
    session.status = 'completed';
    
    // Generate summary
    const summary = await this.generateSummary(session);
    
    // Save final transcription
    await this.saveFinalTranscription(session, summary);
  }
}
```

### 2. Speech Recognition Implementation
```typescript
// Speech Recognition Provider
export class SpeechRecognizer extends EventEmitter {
  private audioProcessor: AudioWorkletNode;
  private recognitionStream?: WebSocket;
  private audioBuffer: Float32Array[] = [];
  private isRecognizing = false;
  
  constructor(
    private config: RecognizerConfig
  ) {
    super();
    this.setupAudioProcessing();
  }
  
  private async setupAudioProcessing(): Promise<void> {
    const audioContext = new AudioContext({
      sampleRate: 16000 // Optimal for speech recognition
    });
    
    // Load audio worklet
    await audioContext.audioWorklet.addModule('/speech-processor.js');
    
    // Create processor
    this.audioProcessor = new AudioWorkletNode(
      audioContext,
      'speech-processor',
      {
        processorOptions: {
          bufferSize: 2048,
          hopSize: 512
        }
      }
    );
    
    // Handle audio chunks
    this.audioProcessor.port.onmessage = (event) => {
      if (event.data.type === 'audio' && this.isRecognizing) {
        this.processAudioChunk(event.data.buffer);
      }
    };
  }
  
  async start(audioStream: MediaStream): Promise<void> {
    // Connect to recognition service
    this.recognitionStream = new WebSocket(
      `wss://speech.waddle.chat/recognize?` + 
      `language=${this.config.language}&` +
      `model=${this.config.model}&` +
      `diarization=${this.config.speakerDiarization}`
    );
    
    this.recognitionStream.onopen = () => {
      this.isRecognizing = true;
      this.sendConfigMessage();
    };
    
    this.recognitionStream.onmessage = (event) => {
      this.handleRecognitionMessage(JSON.parse(event.data));
    };
    
    this.recognitionStream.onerror = (error) => {
      this.emit('error', error);
    };
    
    // Connect audio stream
    const audioContext = this.audioProcessor.context as AudioContext;
    const source = audioContext.createMediaStreamSource(audioStream);
    source.connect(this.audioProcessor);
  }
  
  async stop(): Promise<void> {
    this.isRecognizing = false;
    
    // Send final audio
    if (this.audioBuffer.length > 0) {
      await this.sendAudioBuffer();
    }
    
    // Close connection
    if (this.recognitionStream) {
      this.recognitionStream.close();
      this.recognitionStream = undefined;
    }
    
    // Disconnect audio
    this.audioProcessor.disconnect();
  }
  
  private processAudioChunk(chunk: Float32Array): void {
    this.audioBuffer.push(chunk);
    
    // Send chunks every 100ms
    if (this.audioBuffer.length >= 5) {
      this.sendAudioBuffer();
    }
  }
  
  private async sendAudioBuffer(): Promise<void> {
    if (!this.recognitionStream || this.audioBuffer.length === 0) {
      return;
    }
    
    // Combine chunks
    const totalLength = this.audioBuffer.reduce((sum, chunk) => 
      sum + chunk.length, 0
    );
    const combined = new Float32Array(totalLength);
    
    let offset = 0;
    for (const chunk of this.audioBuffer) {
      combined.set(chunk, offset);
      offset += chunk.length;
    }
    
    // Convert to 16-bit PCM
    const pcm = this.float32ToPCM16(combined);
    
    // Send to recognition service
    this.recognitionStream.send(pcm.buffer);
    
    // Clear buffer
    this.audioBuffer = [];
  }
  
  private float32ToPCM16(float32: Float32Array): Int16Array {
    const pcm16 = new Int16Array(float32.length);
    
    for (let i = 0; i < float32.length; i++) {
      const sample = Math.max(-1, Math.min(1, float32[i]));
      pcm16[i] = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
    }
    
    return pcm16;
  }
  
  private handleRecognitionMessage(message: RecognitionMessage): void {
    switch (message.type) {
      case 'partial':
        this.emit('partial', message.result);
        break;
        
      case 'final':
        this.emit('result', message.result);
        break;
        
      case 'speaker_change':
        this.emit('speaker_change', message.speaker);
        break;
        
      case 'error':
        this.emit('error', new Error(message.error));
        break;
    }
  }
  
  private sendConfigMessage(): void {
    if (!this.recognitionStream) return;
    
    this.recognitionStream.send(JSON.stringify({
      type: 'config',
      config: {
        language: this.config.language,
        interimResults: this.config.interimResults,
        punctuation: this.config.punctuation,
        profanityFilter: this.config.profanityFilter,
        speakerDiarization: this.config.speakerDiarization,
        maxAlternatives: this.config.maxAlternatives
      }
    }));
  }
}

// Speech Processor Worklet
class SpeechProcessor extends AudioWorkletProcessor {
  constructor(options) {
    super();
    this.bufferSize = options.processorOptions.bufferSize || 2048;
    this.hopSize = options.processorOptions.hopSize || 512;
    this.buffer = new Float32Array(this.bufferSize);
    this.bufferIndex = 0;
  }
  
  process(inputs, outputs, parameters) {
    const input = inputs[0];
    
    if (input.length > 0) {
      const samples = input[0];
      
      for (let i = 0; i < samples.length; i++) {
        this.buffer[this.bufferIndex++] = samples[i];
        
        if (this.bufferIndex >= this.hopSize) {
          // Send buffer to main thread
          this.port.postMessage({
            type: 'audio',
            buffer: this.buffer.slice(0, this.bufferIndex)
          });
          
          // Shift buffer
          this.buffer.copyWithin(0, this.hopSize);
          this.bufferIndex -= this.hopSize;
        }
      }
    }
    
    return true;
  }
}
```

### 3. Speaker Identification
```typescript
// Speaker Identification Service
export class SpeakerIdentificationService {
  private speakerModels = new Map<string, SpeakerModel>();
  private embeddingExtractor: EmbeddingExtractor;
  
  constructor() {
    this.embeddingExtractor = new EmbeddingExtractor({
      model: 'pyannote/embedding',
      dimension: 512
    });
  }
  
  async identifySpeaker(
    audioSegment: ArrayBuffer,
    knownSpeakers: TranscriptionParticipant[]
  ): Promise<string> {
    // Extract speaker embedding
    const embedding = await this.embeddingExtractor.extract(audioSegment);
    
    // Compare with known speakers
    let bestMatch: { speakerId: string; similarity: number } | null = null;
    let highestSimilarity = -1;
    
    for (const speaker of knownSpeakers) {
      if (!speaker.voiceSample) continue;
      
      const speakerEmbedding = await this.getSpeakerEmbedding(speaker.speakerId);
      const similarity = this.cosineSimilarity(embedding, speakerEmbedding);
      
      if (similarity > highestSimilarity && similarity > 0.7) {
        highestSimilarity = similarity;
        bestMatch = { speakerId: speaker.speakerId, similarity };
      }
    }
    
    // If no good match, create new speaker
    if (!bestMatch) {
      const newSpeakerId = `speaker_${generateId()}`;
      await this.registerSpeaker(newSpeakerId, embedding, audioSegment);
      return newSpeakerId;
    }
    
    // Update speaker model with new sample
    await this.updateSpeakerModel(bestMatch.speakerId, embedding);
    
    return bestMatch.speakerId;
  }
  
  private async getSpeakerEmbedding(speakerId: string): Promise<Float32Array> {
    const model = this.speakerModels.get(speakerId);
    if (!model) {
      throw new Error('Speaker model not found');
    }
    
    return model.embedding;
  }
  
  private async registerSpeaker(
    speakerId: string,
    embedding: Float32Array,
    voiceSample: ArrayBuffer
  ): Promise<void> {
    const model: SpeakerModel = {
      speakerId,
      embedding,
      samples: [voiceSample],
      createdAt: Date.now(),
      updatedAt: Date.now()
    };
    
    this.speakerModels.set(speakerId, model);
    
    // Save to persistent storage
    await this.saveSpeakerModel(model);
  }
  
  private async updateSpeakerModel(
    speakerId: string,
    newEmbedding: Float32Array
  ): Promise<void> {
    const model = this.speakerModels.get(speakerId);
    if (!model) return;
    
    // Update embedding with weighted average
    const weight = 0.1; // Weight for new sample
    for (let i = 0; i < model.embedding.length; i++) {
      model.embedding[i] = (1 - weight) * model.embedding[i] + 
                          weight * newEmbedding[i];
    }
    
    model.updatedAt = Date.now();
    
    // Save updated model
    await this.saveSpeakerModel(model);
  }
  
  private cosineSimilarity(a: Float32Array, b: Float32Array): number {
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;
    
    for (let i = 0; i < a.length; i++) {
      dotProduct += a[i] * b[i];
      normA += a[i] * a[i];
      normB += b[i] * b[i];
    }
    
    return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
  }
  
  async linkSpeakerToUser(
    speakerId: string,
    userId: string
  ): Promise<void> {
    // Link speaker ID to user ID in database
    await this.db
      .prepare(`
        INSERT INTO speaker_user_mapping (speaker_id, user_id, created_at)
        VALUES (?, ?, ?)
        ON CONFLICT(speaker_id) DO UPDATE SET
          user_id = excluded.user_id
      `)
      .bind(speakerId, userId, new Date().toISOString())
      .run();
  }
}
```

### 4. Live Captions UI
```tsx
// Live Captions Component
export function LiveCaptions({ 
  channelId,
  enabled = true 
}: LiveCaptionsProps) {
  const [captions, setCaptions] = useState<Caption[]>([]);
  const [speakers, setSpeakers] = useState<Map<string, SpeakerInfo>>();
  const captionRef = useRef<HTMLDivElement>(null);
  
  useEffect(() => {
    if (!enabled) return;
    
    const subscription = subscribeToCaptions(channelId, (caption) => {
      setCaptions(prev => {
        const updated = [...prev, caption];
        // Keep only last 10 captions
        return updated.slice(-10);
      });
      
      // Auto-scroll to bottom
      if (captionRef.current) {
        captionRef.current.scrollTop = captionRef.current.scrollHeight;
      }
    });
    
    return () => subscription.unsubscribe();
  }, [channelId, enabled]);
  
  if (!enabled) return null;
  
  return (
    <div className="live-captions">
      <div className="captions-header">
        <h4>Live Captions</h4>
        <CaptionSettings />
      </div>
      
      <div className="captions-container" ref={captionRef}>
        {captions.map((caption, index) => (
          <CaptionLine
            key={index}
            caption={caption}
            speaker={speakers?.get(caption.speakerId)}
          />
        ))}
      </div>
    </div>
  );
}

// Caption Line Component
export function CaptionLine({ 
  caption, 
  speaker 
}: { 
  caption: Caption; 
  speaker?: SpeakerInfo;
}) {
  const [showTranslation, setShowTranslation] = useState(false);
  const userLanguage = useUserLanguage();
  
  const needsTranslation = caption.language !== userLanguage;
  
  return (
    <div className="caption-line">
      <div className="caption-speaker">
        {speaker ? (
          <>
            <Avatar user={speaker.user} size="small" />
            <span className="speaker-name">{speaker.name}</span>
          </>
        ) : (
          <span className="unknown-speaker">Speaker {caption.speakerId}</span>
        )}
      </div>
      
      <div className="caption-text">
        <p className={`original-text ${showTranslation ? 'dimmed' : ''}`}>
          {caption.text}
        </p>
        
        {needsTranslation && caption.translation && (
          <>
            {showTranslation && (
              <p className="translated-text">
                {caption.translation.text}
              </p>
            )}
            
            <button
              className="translation-toggle"
              onClick={() => setShowTranslation(!showTranslation)}
            >
              {showTranslation ? 'Show Original' : 'Translate'}
            </button>
          </>
        )}
      </div>
      
      <div className="caption-confidence">
        <ConfidenceIndicator value={caption.confidence} />
      </div>
    </div>
  );
}

// Transcription History Component
export function TranscriptionHistory({ 
  waddleId,
  channelId 
}: TranscriptionHistoryProps) {
  const [sessions, setSessions] = useState<TranscriptionSession[]>([]);
  const [selectedSession, setSelectedSession] = useState<TranscriptionSession>();
  const [searchQuery, setSearchQuery] = useState('');
  const [loading, setLoading] = useState(true);
  
  useEffect(() => {
    loadSessions();
  }, [waddleId, channelId]);
  
  const loadSessions = async () => {
    try {
      const data = await api.getTranscriptionSessions({
        waddleId,
        channelId,
        limit: 50
      });
      setSessions(data);
    } finally {
      setLoading(false);
    }
  };
  
  const searchTranscriptions = async () => {
    if (!searchQuery.trim()) return;
    
    const results = await api.searchTranscriptions({
      waddleId,
      query: searchQuery
    });
    
    // Show search results
    setSearchResults(results);
  };
  
  const exportTranscription = async (
    session: TranscriptionSession,
    format: 'txt' | 'srt' | 'vtt' | 'json'
  ) => {
    const data = await api.exportTranscription(session.id, format);
    downloadFile(data, `transcription-${session.id}.${format}`);
  };
  
  return (
    <div className="transcription-history">
      <div className="history-header">
        <h3>Transcription History</h3>
        
        <div className="search-bar">
          <Input
            value={searchQuery}
            onChange={(e) => setSearchQuery(e.target.value)}
            onKeyPress={(e) => e.key === 'Enter' && searchTranscriptions()}
            placeholder="Search transcriptions..."
            icon={<SearchIcon />}
          />
        </div>
      </div>
      
      {loading ? (
        <LoadingSpinner />
      ) : sessions.length === 0 ? (
        <EmptyState message="No transcription history" />
      ) : (
        <div className="sessions-list">
          {sessions.map(session => (
            <TranscriptionSessionCard
              key={session.id}
              session={session}
              onClick={() => setSelectedSession(session)}
            />
          ))}
        </div>
      )}
      
      {selectedSession && (
        <TranscriptionViewer
          session={selectedSession}
          onClose={() => setSelectedSession(undefined)}
          onExport={(format) => exportTranscription(selectedSession, format)}
        />
      )}
    </div>
  );
}

// Transcription Viewer Component
export function TranscriptionViewer({ 
  session,
  onClose,
  onExport
}: TranscriptionViewerProps) {
  const [viewMode, setViewMode] = useState<'timeline' | 'speaker' | 'document'>('timeline');
  const [selectedSpeaker, setSelectedSpeaker] = useState<string>();
  const [playbackTime, setPlaybackTime] = useState(0);
  
  const segments = selectedSpeaker
    ? session.segments.filter(s => s.speakerId === selectedSpeaker)
    : session.segments;
  
  return (
    <Modal onClose={onClose} size="large">
      <div className="transcription-viewer">
        <div className="viewer-header">
          <h2>Transcription</h2>
          <div className="session-info">
            <span>{formatDate(session.startTime)}</span>
            <span>{formatDuration(session.metadata.totalDuration)}</span>
            <span>{session.metadata.wordCount} words</span>
          </div>
        </div>
        
        <div className="viewer-toolbar">
          <ViewModeSelector
            value={viewMode}
            onChange={setViewMode}
          />
          
          <SpeakerFilter
            speakers={session.participants}
            selected={selectedSpeaker}
            onChange={setSelectedSpeaker}
          />
          
          <ExportMenu onExport={onExport} />
        </div>
        
        <div className="viewer-content">
          {viewMode === 'timeline' && (
            <TimelineView
              segments={segments}
              playbackTime={playbackTime}
              onSeek={setPlaybackTime}
            />
          )}
          
          {viewMode === 'speaker' && (
            <SpeakerView
              segments={segments}
              participants={session.participants}
            />
          )}
          
          {viewMode === 'document' && (
            <DocumentView
              segments={segments}
              showTimestamps={true}
            />
          )}
        </div>
        
        <TranscriptionPlayer
          session={session}
          currentTime={playbackTime}
          onTimeUpdate={setPlaybackTime}
        />
      </div>
    </Modal>
  );
}
```

### 5. Translation Service
```typescript
// Translation Service
export class TranslationService {
  private cache = new Map<string, TranslationResult>();
  
  constructor(
    private translationAPI: TranslationAPI,
    private supportedLanguages: string[]
  ) {}
  
  async translateSegment(
    segment: TranscriptionSegment,
    targetLanguage: string
  ): Promise<TranslationResult> {
    // Check cache
    const cacheKey = `${segment.id}:${targetLanguage}`;
    const cached = this.cache.get(cacheKey);
    if (cached) return cached;
    
    // Skip if same language
    if (segment.language === targetLanguage) {
      return { text: segment.text, language: targetLanguage };
    }
    
    // Translate
    const result = await this.translationAPI.translate({
      text: segment.text,
      sourceLanguage: segment.language,
      targetLanguage,
      format: 'text'
    });
    
    // Cache result
    this.cache.set(cacheKey, result);
    
    return result;
  }
  
  async detectLanguage(text: string): Promise<LanguageDetection> {
    const result = await this.translationAPI.detectLanguage(text);
    
    return {
      language: result.language,
      confidence: result.confidence,
      isReliable: result.confidence > 0.8
    };
  }
  
  async translateBatch(
    segments: TranscriptionSegment[],
    targetLanguage: string
  ): Promise<Map<string, TranslationResult>> {
    const results = new Map<string, TranslationResult>();
    
    // Group by source language
    const languageGroups = new Map<string, TranscriptionSegment[]>();
    
    for (const segment of segments) {
      const lang = segment.language;
      if (!languageGroups.has(lang)) {
        languageGroups.set(lang, []);
      }
      languageGroups.get(lang)!.push(segment);
    }
    
    // Translate each language group
    const promises = Array.from(languageGroups.entries()).map(
      async ([sourceLanguage, segments]) => {
        if (sourceLanguage === targetLanguage) {
          // No translation needed
          for (const segment of segments) {
            results.set(segment.id, {
              text: segment.text,
              language: targetLanguage
            });
          }
          return;
        }
        
        // Batch translate
        const texts = segments.map(s => s.text);
        const translations = await this.translationAPI.translateBatch({
          texts,
          sourceLanguage,
          targetLanguage
        });
        
        // Map results
        segments.forEach((segment, index) => {
          results.set(segment.id, {
            text: translations[index],
            language: targetLanguage
          });
        });
      }
    );
    
    await Promise.all(promises);
    
    return results;
  }
}

// Multi-language Support
export class MultiLanguageTranscriptionService {
  private activeLanguages = new Set<string>();
  private languageDetector: LanguageDetector;
  
  constructor(
    private transcriptionService: RealtimeTranscriptionService,
    private translationService: TranslationService
  ) {
    this.languageDetector = new LanguageDetector();
  }
  
  async enableMultiLanguageMode(
    sessionId: string,
    languages: string[]
  ): Promise<void> {
    // Enable multiple recognizers for different languages
    for (const language of languages) {
      this.activeLanguages.add(language);
      await this.transcriptionService.addLanguageRecognizer(sessionId, language);
    }
  }
  
  async processMultiLanguageAudio(
    audio: ArrayBuffer,
    sessionId: string
  ): Promise<MultiLanguageResult> {
    // Detect primary language
    const detection = await this.languageDetector.detect(audio);
    
    // Process with primary language recognizer
    const primaryResult = await this.transcriptionService.recognize(
      audio,
      detection.language
    );
    
    // If confidence is low, try other languages
    if (primaryResult.confidence < 0.7) {
      const alternativeResults = await this.tryAlternativeLanguages(
        audio,
        detection.language
      );
      
      // Select best result
      const bestResult = this.selectBestResult(
        primaryResult,
        alternativeResults
      );
      
      return {
        ...bestResult,
        detectedLanguage: detection.language,
        alternativeLanguages: alternativeResults.map(r => r.language)
      };
    }
    
    return {
      ...primaryResult,
      detectedLanguage: detection.language,
      alternativeLanguages: []
    };
  }
}
```

### 6. Accuracy Improvement
```typescript
// Accuracy Improvement Service
export class TranscriptionAccuracyService {
  private contextModels = new Map<string, ContextModel>();
  private vocabularyDB: VocabularyDatabase;
  
  constructor(
    private nlpService: NLPService,
    private db: D1Database
  ) {
    this.vocabularyDB = new VocabularyDatabase(db);
  }
  
  async improveAccuracy(
    segments: TranscriptionSegment[],
    context: TranscriptionContext
  ): Promise<TranscriptionSegment[]> {
    // Load context model
    const contextModel = await this.getOrCreateContextModel(context);
    
    const improvedSegments: TranscriptionSegment[] = [];
    
    for (const segment of segments) {
      let improvedText = segment.text;
      
      // Apply context-based corrections
      improvedText = await this.applyContextCorrections(
        improvedText,
        contextModel
      );
      
      // Fix common speech recognition errors
      improvedText = this.fixCommonErrors(improvedText);
      
      // Apply custom vocabulary
      improvedText = await this.applyCustomVocabulary(
        improvedText,
        context.waddleId
      );
      
      // Grammar and punctuation correction
      improvedText = await this.correctGrammar(improvedText);
      
      improvedSegments.push({
        ...segment,
        text: improvedText,
        confidence: this.recalculateConfidence(segment, improvedText)
      });
    }
    
    // Post-process for coherence
    return this.ensureCoherence(improvedSegments);
  }
  
  private async applyContextCorrections(
    text: string,
    model: ContextModel
  ): Promise<string> {
    // Tokenize text
    const tokens = this.nlpService.tokenize(text);
    const correctedTokens: string[] = [];
    
    for (let i = 0; i < tokens.length; i++) {
      const token = tokens[i];
      const context = tokens.slice(Math.max(0, i - 3), i);
      
      // Check if token needs correction
      if (model.shouldCorrect(token, context)) {
        const correction = model.getCorrection(token, context);
        correctedTokens.push(correction);
      } else {
        correctedTokens.push(token);
      }
    }
    
    return correctedTokens.join(' ');
  }
  
  private fixCommonErrors(text: string): string {
    const corrections = [
      // Homophones
      { pattern: /there/gi, replacement: (match, context) => {
        if (context.includes('location')) return 'there';
        if (context.includes('possession')) return 'their';
        return "they're";
      }},
      // Technical terms
      { pattern: /java script/gi, replacement: 'JavaScript' },
      { pattern: /python/gi, replacement: 'Python' },
      // Common mishearings
      { pattern: /could of/gi, replacement: 'could have' },
      { pattern: /would of/gi, replacement: 'would have' }
    ];
    
    let corrected = text;
    for (const { pattern, replacement } of corrections) {
      corrected = corrected.replace(pattern, replacement);
    }
    
    return corrected;
  }
  
  private async applyCustomVocabulary(
    text: string,
    waddleId: string
  ): Promise<string> {
    // Get custom vocabulary for waddle
    const vocabulary = await this.vocabularyDB.getVocabulary(waddleId);
    
    // Apply replacements
    let corrected = text;
    for (const term of vocabulary.terms) {
      const pattern = new RegExp(term.pattern, 'gi');
      corrected = corrected.replace(pattern, term.replacement);
    }
    
    return corrected;
  }
  
  async learnFromCorrections(
    original: string,
    corrected: string,
    context: TranscriptionContext
  ): Promise<void> {
    // Extract correction patterns
    const corrections = this.extractCorrections(original, corrected);
    
    // Update context model
    const model = await this.getOrCreateContextModel(context);
    
    for (const correction of corrections) {
      await model.addCorrection(
        correction.original,
        correction.corrected,
        correction.context
      );
    }
    
    // Save model
    await this.saveContextModel(context, model);
  }
}

// Custom Vocabulary Management
export class VocabularyManager {
  async addTerm(
    waddleId: string,
    term: string,
    variations?: string[]
  ): Promise<void> {
    const pattern = this.createPattern(term, variations);
    
    await this.db
      .prepare(`
        INSERT INTO custom_vocabulary (waddle_id, term, pattern, created_at)
        VALUES (?, ?, ?, ?)
      `)
      .bind(waddleId, term, pattern, new Date().toISOString())
      .run();
  }
  
  async importGlossary(
    waddleId: string,
    glossary: GlossaryEntry[]
  ): Promise<void> {
    const batch = this.db.batch();
    
    for (const entry of glossary) {
      batch.prepare(`
        INSERT INTO custom_vocabulary (waddle_id, term, pattern, definition)
        VALUES (?, ?, ?, ?)
      `).bind(
        waddleId,
        entry.term,
        this.createPattern(entry.term, entry.variations),
        entry.definition
      );
    }
    
    await batch.run();
  }
  
  private createPattern(term: string, variations?: string[]): string {
    const patterns = [term];
    
    if (variations) {
      patterns.push(...variations);
    }
    
    // Add common variations
    patterns.push(
      term.toLowerCase(),
      term.toUpperCase(),
      this.camelToSpaced(term),
      this.spacedToCamel(term)
    );
    
    // Create regex pattern
    const escaped = patterns.map(p => 
      p.replace(/[.*+?^${}()|[\]\\]/g, '\\$&')
    );
    
    return `\\b(${escaped.join('|')})\\b`;
  }
}
```

## Dependencies
- WebSocket for real-time streaming
- Speech recognition API (Google Cloud Speech-to-Text or similar)
- Speaker diarization model
- Translation API
- NLP libraries for accuracy improvement
- Web Audio API

## Estimated Effort
**5 days**
- 1 day: Core transcription service setup
- 1 day: Real-time streaming and processing
- 1 day: Speaker identification system
- 1 day: UI components and live captions
- 0.5 day: Translation integration
- 0.5 day: Accuracy improvements and testing

## Notes
- Consider implementing offline transcription capability
- Add support for custom wake words
- Implement speaker training for better identification
- Consider adding sentiment analysis
- Plan for GDPR compliance with voice data
- Add support for meeting minutes generation