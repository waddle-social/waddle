# Issue #34: Voice Recording

## User Story
As a **waddle member**, I want to **record and send voice messages** so that **I can communicate more personally when typing isn't convenient**.

## Description
Implement voice recording functionality that allows users to record, preview, and send voice messages within chat. This includes real-time audio visualization, automatic compression, noise reduction, and storage in Cloudflare R2 with transcription capabilities.

## Acceptance Criteria
- [ ] Push-to-talk voice recording
- [ ] Real-time audio waveform visualization
- [ ] Voice message preview before sending
- [ ] Automatic audio compression
- [ ] Background noise reduction
- [ ] Voice message duration limits
- [ ] Playback speed control
- [ ] Download voice messages
- [ ] Voice message transcription
- [ ] Mobile-friendly recording interface

## Technical Implementation

### 1. Voice Recording Service
```typescript
// Voice Recording Types
export interface VoiceRecording {
  id: string;
  userId: string;
  waddleId: string;
  channelId: string;
  messageId?: string;
  duration: number; // seconds
  size: number; // bytes
  format: 'webm' | 'mp3' | 'ogg';
  sampleRate: number;
  bitrate: number;
  waveform: number[]; // Normalized amplitude values
  transcription?: {
    text: string;
    confidence: number;
    language: string;
  };
  r2Key: string;
  url: string;
  createdAt: Date;
}

export interface RecordingOptions {
  maxDuration: number; // seconds
  format: 'webm' | 'mp3';
  sampleRate: number;
  bitrate: number;
  echoCancellation: boolean;
  noiseSuppression: boolean;
  autoGainControl: boolean;
}

// Voice Recorder Class
export class VoiceRecorder {
  private mediaRecorder?: MediaRecorder;
  private audioContext?: AudioContext;
  private analyser?: AnalyserNode;
  private source?: MediaStreamAudioSourceNode;
  private chunks: Blob[] = [];
  private startTime?: number;
  private visualizationInterval?: number;
  private stream?: MediaStream;
  
  constructor(
    private options: RecordingOptions = {
      maxDuration: 300, // 5 minutes
      format: 'webm',
      sampleRate: 48000,
      bitrate: 128000,
      echoCancellation: true,
      noiseSuppression: true,
      autoGainControl: true
    }
  ) {}
  
  async startRecording(
    onVisualization?: (amplitude: number) => void
  ): Promise<void> {
    try {
      // Request microphone access
      this.stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: this.options.echoCancellation,
          noiseSuppression: this.options.noiseSuppression,
          autoGainControl: this.options.autoGainControl,
          sampleRate: this.options.sampleRate
        }
      });
      
      // Setup audio context for visualization
      this.audioContext = new AudioContext();
      this.analyser = this.audioContext.createAnalyser();
      this.analyser.fftSize = 256;
      this.source = this.audioContext.createMediaStreamSource(this.stream);
      this.source.connect(this.analyser);
      
      // Setup media recorder
      const mimeType = this.getMimeType();
      this.mediaRecorder = new MediaRecorder(this.stream, {
        mimeType,
        audioBitsPerSecond: this.options.bitrate
      });
      
      this.chunks = [];
      this.startTime = Date.now();
      
      this.mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          this.chunks.push(event.data);
        }
      };
      
      this.mediaRecorder.start(100); // Collect data every 100ms
      
      // Start visualization
      if (onVisualization) {
        this.startVisualization(onVisualization);
      }
      
      // Auto-stop at max duration
      setTimeout(() => {
        if (this.isRecording()) {
          this.stopRecording();
        }
      }, this.options.maxDuration * 1000);
      
    } catch (error) {
      throw new Error(`Failed to start recording: ${error.message}`);
    }
  }
  
  async stopRecording(): Promise<Blob> {
    if (!this.mediaRecorder || this.mediaRecorder.state !== 'recording') {
      throw new Error('No active recording');
    }
    
    return new Promise((resolve) => {
      this.mediaRecorder!.onstop = () => {
        const blob = new Blob(this.chunks, { 
          type: this.getMimeType() 
        });
        
        this.cleanup();
        resolve(blob);
      };
      
      this.mediaRecorder.stop();
      
      // Stop all tracks
      this.stream?.getTracks().forEach(track => track.stop());
    });
  }
  
  pauseRecording(): void {
    if (this.mediaRecorder?.state === 'recording') {
      this.mediaRecorder.pause();
    }
  }
  
  resumeRecording(): void {
    if (this.mediaRecorder?.state === 'paused') {
      this.mediaRecorder.resume();
    }
  }
  
  isRecording(): boolean {
    return this.mediaRecorder?.state === 'recording' || false;
  }
  
  getDuration(): number {
    if (!this.startTime) return 0;
    return (Date.now() - this.startTime) / 1000;
  }
  
  private startVisualization(callback: (amplitude: number) => void): void {
    if (!this.analyser) return;
    
    const dataArray = new Uint8Array(this.analyser.frequencyBinCount);
    
    const updateVisualization = () => {
      if (!this.analyser || !this.isRecording()) {
        return;
      }
      
      this.analyser.getByteFrequencyData(dataArray);
      
      // Calculate average amplitude
      const sum = dataArray.reduce((a, b) => a + b, 0);
      const average = sum / dataArray.length;
      const normalized = average / 255; // Normalize to 0-1
      
      callback(normalized);
      
      this.visualizationInterval = requestAnimationFrame(updateVisualization);
    };
    
    updateVisualization();
  }
  
  private cleanup(): void {
    if (this.visualizationInterval) {
      cancelAnimationFrame(this.visualizationInterval);
    }
    
    this.source?.disconnect();
    this.audioContext?.close();
    
    this.mediaRecorder = undefined;
    this.audioContext = undefined;
    this.analyser = undefined;
    this.source = undefined;
    this.chunks = [];
    this.startTime = undefined;
  }
  
  private getMimeType(): string {
    if (this.options.format === 'webm') {
      // Check for codec support
      const types = [
        'audio/webm;codecs=opus',
        'audio/webm;codecs=vp9',
        'audio/webm'
      ];
      
      for (const type of types) {
        if (MediaRecorder.isTypeSupported(type)) {
          return type;
        }
      }
    }
    
    return 'audio/webm';
  }
}
```

### 2. Audio Processing
```typescript
// Audio Processor
export class AudioProcessor {
  private audioContext: OfflineAudioContext;
  
  async processRecording(
    blob: Blob,
    options: ProcessingOptions
  ): Promise<ProcessedAudio> {
    // Convert blob to audio buffer
    const arrayBuffer = await blob.arrayBuffer();
    const audioBuffer = await this.decodeAudioData(arrayBuffer);
    
    // Apply processing
    let processed = audioBuffer;
    
    if (options.trimSilence) {
      processed = await this.trimSilence(processed);
    }
    
    if (options.normalizeVolume) {
      processed = await this.normalizeVolume(processed);
    }
    
    if (options.noiseReduction) {
      processed = await this.reduceNoise(processed);
    }
    
    if (options.compress) {
      processed = await this.compressAudio(processed, options.targetBitrate);
    }
    
    // Generate waveform
    const waveform = this.generateWaveform(processed, 100);
    
    // Convert back to blob
    const outputBlob = await this.encodeAudioBuffer(
      processed,
      options.outputFormat || 'mp3'
    );
    
    return {
      blob: outputBlob,
      duration: processed.duration,
      waveform,
      sampleRate: processed.sampleRate
    };
  }
  
  private async decodeAudioData(arrayBuffer: ArrayBuffer): Promise<AudioBuffer> {
    const audioContext = new AudioContext();
    return await audioContext.decodeAudioData(arrayBuffer);
  }
  
  private async trimSilence(
    buffer: AudioBuffer,
    threshold = 0.01
  ): Promise<AudioBuffer> {
    const channelData = buffer.getChannelData(0);
    let start = 0;
    let end = channelData.length - 1;
    
    // Find start of audio
    for (let i = 0; i < channelData.length; i++) {
      if (Math.abs(channelData[i]) > threshold) {
        start = Math.max(0, i - buffer.sampleRate * 0.1); // 100ms padding
        break;
      }
    }
    
    // Find end of audio
    for (let i = channelData.length - 1; i >= 0; i--) {
      if (Math.abs(channelData[i]) > threshold) {
        end = Math.min(channelData.length - 1, i + buffer.sampleRate * 0.1);
        break;
      }
    }
    
    // Create trimmed buffer
    const trimmedLength = end - start;
    const trimmedBuffer = new AudioBuffer({
      numberOfChannels: buffer.numberOfChannels,
      length: trimmedLength,
      sampleRate: buffer.sampleRate
    });
    
    for (let channel = 0; channel < buffer.numberOfChannels; channel++) {
      const channelData = buffer.getChannelData(channel);
      const trimmedData = channelData.slice(start, end);
      trimmedBuffer.copyToChannel(trimmedData, channel);
    }
    
    return trimmedBuffer;
  }
  
  private async normalizeVolume(
    buffer: AudioBuffer,
    targetLevel = 0.95
  ): Promise<AudioBuffer> {
    // Find peak amplitude
    let maxAmplitude = 0;
    
    for (let channel = 0; channel < buffer.numberOfChannels; channel++) {
      const channelData = buffer.getChannelData(channel);
      for (let i = 0; i < channelData.length; i++) {
        maxAmplitude = Math.max(maxAmplitude, Math.abs(channelData[i]));
      }
    }
    
    // Calculate gain
    const gain = targetLevel / maxAmplitude;
    
    // Apply gain
    const normalizedBuffer = new AudioBuffer({
      numberOfChannels: buffer.numberOfChannels,
      length: buffer.length,
      sampleRate: buffer.sampleRate
    });
    
    for (let channel = 0; channel < buffer.numberOfChannels; channel++) {
      const channelData = buffer.getChannelData(channel);
      const normalizedData = new Float32Array(channelData.length);
      
      for (let i = 0; i < channelData.length; i++) {
        normalizedData[i] = channelData[i] * gain;
      }
      
      normalizedBuffer.copyToChannel(normalizedData, channel);
    }
    
    return normalizedBuffer;
  }
  
  private async reduceNoise(buffer: AudioBuffer): Promise<AudioBuffer> {
    const sampleRate = buffer.sampleRate;
    const offlineContext = new OfflineAudioContext(
      buffer.numberOfChannels,
      buffer.length,
      sampleRate
    );
    
    // Create source
    const source = offlineContext.createBufferSource();
    source.buffer = buffer;
    
    // High-pass filter to remove low-frequency noise
    const highPass = offlineContext.createBiquadFilter();
    highPass.type = 'highpass';
    highPass.frequency.value = 80; // Remove below 80Hz
    
    // Dynamic compressor
    const compressor = offlineContext.createDynamicsCompressor();
    compressor.threshold.value = -24;
    compressor.knee.value = 30;
    compressor.ratio.value = 12;
    compressor.attack.value = 0.003;
    compressor.release.value = 0.25;
    
    // Connect nodes
    source.connect(highPass);
    highPass.connect(compressor);
    compressor.connect(offlineContext.destination);
    
    // Start processing
    source.start();
    
    return await offlineContext.startRendering();
  }
  
  private generateWaveform(
    buffer: AudioBuffer,
    points: number
  ): number[] {
    const channelData = buffer.getChannelData(0);
    const samplesPerPoint = Math.floor(channelData.length / points);
    const waveform: number[] = [];
    
    for (let i = 0; i < points; i++) {
      const start = i * samplesPerPoint;
      const end = start + samplesPerPoint;
      
      let sum = 0;
      for (let j = start; j < end && j < channelData.length; j++) {
        sum += Math.abs(channelData[j]);
      }
      
      const average = sum / samplesPerPoint;
      waveform.push(average);
    }
    
    // Normalize to 0-1
    const max = Math.max(...waveform);
    return waveform.map(v => v / max);
  }
  
  private async encodeAudioBuffer(
    buffer: AudioBuffer,
    format: 'mp3' | 'ogg' | 'webm'
  ): Promise<Blob> {
    switch (format) {
      case 'mp3':
        return await this.encodeToMp3(buffer);
      case 'ogg':
        return await this.encodeToOgg(buffer);
      default:
        return await this.encodeToWebM(buffer);
    }
  }
  
  private async encodeToMp3(buffer: AudioBuffer): Promise<Blob> {
    const encoder = new lamejs.Mp3Encoder(
      buffer.numberOfChannels,
      buffer.sampleRate,
      128 // kbps
    );
    
    const samples = buffer.getChannelData(0);
    const sampleBlockSize = 1152;
    const mp3Data: Int8Array[] = [];
    
    for (let i = 0; i < samples.length; i += sampleBlockSize) {
      const sampleChunk = samples.subarray(i, i + sampleBlockSize);
      const mp3buf = encoder.encodeBuffer(sampleChunk);
      if (mp3buf.length > 0) {
        mp3Data.push(mp3buf);
      }
    }
    
    const mp3buf = encoder.flush();
    if (mp3buf.length > 0) {
      mp3Data.push(mp3buf);
    }
    
    return new Blob(mp3Data, { type: 'audio/mp3' });
  }
}
```

### 3. Voice Message Storage
```typescript
// Voice Storage Service
export class VoiceStorageService {
  constructor(
    private r2: R2Bucket,
    private db: D1Database,
    private transcriptionService: TranscriptionService
  ) {}
  
  async storeVoiceMessage(
    recording: Blob,
    metadata: VoiceMessageMetadata
  ): Promise<VoiceRecording> {
    const recordingId = generateId();
    
    // Process audio
    const processed = await this.audioProcessor.processRecording(recording, {
      trimSilence: true,
      normalizeVolume: true,
      noiseReduction: true,
      compress: true,
      targetBitrate: 64000, // 64kbps for voice
      outputFormat: 'mp3'
    });
    
    // Generate R2 key
    const r2Key = this.generateR2Key(metadata, recordingId);
    
    // Upload to R2
    await this.r2.put(r2Key, processed.blob, {
      httpMetadata: {
        contentType: 'audio/mp3',
        cacheControl: 'public, max-age=31536000'
      },
      customMetadata: {
        userId: metadata.userId,
        waddleId: metadata.waddleId,
        channelId: metadata.channelId,
        duration: processed.duration.toString(),
        originalFormat: metadata.format
      }
    });
    
    // Generate signed URL
    const url = this.generateSignedUrl(r2Key);
    
    // Create database record
    const voiceRecording: VoiceRecording = {
      id: recordingId,
      userId: metadata.userId,
      waddleId: metadata.waddleId,
      channelId: metadata.channelId,
      messageId: metadata.messageId,
      duration: processed.duration,
      size: processed.blob.size,
      format: 'mp3',
      sampleRate: processed.sampleRate,
      bitrate: 64000,
      waveform: processed.waveform,
      r2Key,
      url,
      createdAt: new Date()
    };
    
    await this.saveVoiceRecord(voiceRecording);
    
    // Queue transcription
    await this.queueTranscription(voiceRecording);
    
    return voiceRecording;
  }
  
  private async queueTranscription(recording: VoiceRecording): Promise<void> {
    // Add to transcription queue
    await this.transcriptionQueue.add({
      recordingId: recording.id,
      r2Key: recording.r2Key,
      duration: recording.duration,
      language: 'auto' // Auto-detect language
    });
  }
  
  async processTranscriptionJob(job: TranscriptionJob): Promise<void> {
    try {
      // Download audio from R2
      const audio = await this.r2.get(job.r2Key);
      if (!audio) {
        throw new Error('Audio file not found');
      }
      
      // Transcribe
      const result = await this.transcriptionService.transcribe(
        await audio.arrayBuffer(),
        {
          language: job.language,
          model: 'whisper-1'
        }
      );
      
      // Update database
      await this.db
        .prepare(`
          UPDATE voice_recordings 
          SET transcription = ?, transcribed_at = ?
          WHERE id = ?
        `)
        .bind(
          JSON.stringify(result),
          new Date().toISOString(),
          job.recordingId
        )
        .run();
        
      // Send real-time update
      await this.sendTranscriptionUpdate(job.recordingId, result);
      
    } catch (error) {
      console.error('Transcription failed:', error);
      await this.handleTranscriptionError(job, error);
    }
  }
  
  private generateR2Key(
    metadata: VoiceMessageMetadata,
    recordingId: string
  ): string {
    const date = new Date();
    const year = date.getFullYear();
    const month = String(date.getMonth() + 1).padStart(2, '0');
    
    return `voice/${metadata.waddleId}/${year}/${month}/${recordingId}.mp3`;
  }
  
  private generateSignedUrl(key: string): string {
    // Generate a signed URL that expires in 7 days
    const url = new URL(`https://media.waddle.chat/${key}`);
    const expires = Math.floor(Date.now() / 1000) + (7 * 24 * 60 * 60);
    
    const signature = this.generateUrlSignature(key, expires);
    url.searchParams.set('expires', expires.toString());
    url.searchParams.set('signature', signature);
    
    return url.toString();
  }
}
```

### 4. Voice Message UI Components
```tsx
// Voice Recorder Component
export function VoiceRecorder({ 
  onSend,
  maxDuration = 300 
}: VoiceRecorderProps) {
  const [isRecording, setIsRecording] = useState(false);
  const [isPaused, setIsPaused] = useState(false);
  const [duration, setDuration] = useState(0);
  const [amplitude, setAmplitude] = useState(0);
  const [blob, setBlob] = useState<Blob>();
  const [isProcessing, setIsProcessing] = useState(false);
  
  const recorder = useRef<VoiceRecorder>();
  const durationInterval = useRef<number>();
  
  useEffect(() => {
    recorder.current = new VoiceRecorder({
      maxDuration,
      format: 'webm',
      echoCancellation: true,
      noiseSuppression: true,
      autoGainControl: true
    });
    
    return () => {
      if (recorder.current?.isRecording()) {
        recorder.current.stopRecording();
      }
    };
  }, [maxDuration]);
  
  const startRecording = async () => {
    try {
      await recorder.current!.startRecording((amp) => {
        setAmplitude(amp);
      });
      
      setIsRecording(true);
      setDuration(0);
      
      // Update duration
      durationInterval.current = setInterval(() => {
        setDuration(recorder.current!.getDuration());
      }, 100);
      
    } catch (error) {
      toast.error('Failed to access microphone');
      console.error(error);
    }
  };
  
  const stopRecording = async () => {
    if (!recorder.current?.isRecording()) return;
    
    clearInterval(durationInterval.current);
    
    try {
      const recordedBlob = await recorder.current.stopRecording();
      setBlob(recordedBlob);
      setIsRecording(false);
      setAmplitude(0);
    } catch (error) {
      toast.error('Failed to stop recording');
      console.error(error);
    }
  };
  
  const pauseRecording = () => {
    recorder.current?.pauseRecording();
    setIsPaused(true);
    clearInterval(durationInterval.current);
  };
  
  const resumeRecording = () => {
    recorder.current?.resumeRecording();
    setIsPaused(false);
    
    durationInterval.current = setInterval(() => {
      setDuration(recorder.current!.getDuration());
    }, 100);
  };
  
  const cancelRecording = () => {
    if (recorder.current?.isRecording()) {
      recorder.current.stopRecording();
    }
    
    clearInterval(durationInterval.current);
    setIsRecording(false);
    setIsPaused(false);
    setDuration(0);
    setAmplitude(0);
    setBlob(undefined);
  };
  
  const sendRecording = async () => {
    if (!blob) return;
    
    setIsProcessing(true);
    try {
      await onSend(blob, duration);
      cancelRecording();
    } catch (error) {
      toast.error('Failed to send voice message');
    } finally {
      setIsProcessing(false);
    }
  };
  
  if (blob && !isRecording) {
    return (
      <VoicePreview
        blob={blob}
        duration={duration}
        onSend={sendRecording}
        onCancel={cancelRecording}
        isProcessing={isProcessing}
      />
    );
  }
  
  return (
    <div className="voice-recorder">
      <div className="recorder-controls">
        {!isRecording ? (
          <Button
            className="record-button"
            onClick={startRecording}
            icon={<MicIcon />}
            variant="primary"
          >
            Hold to record
          </Button>
        ) : (
          <>
            <Button
              className="cancel-button"
              onClick={cancelRecording}
              icon={<CancelIcon />}
              variant="text"
            />
            
            <div className="recording-info">
              <div className="recording-indicator">
                <span className="recording-dot" />
                Recording
              </div>
              <div className="duration">{formatDuration(duration)}</div>
            </div>
            
            <VoiceVisualizer amplitude={amplitude} />
            
            <div className="recording-actions">
              {isPaused ? (
                <Button
                  onClick={resumeRecording}
                  icon={<PlayIcon />}
                  variant="text"
                />
              ) : (
                <Button
                  onClick={pauseRecording}
                  icon={<PauseIcon />}
                  variant="text"
                />
              )}
              
              <Button
                className="stop-button"
                onClick={stopRecording}
                icon={<StopIcon />}
                variant="primary"
              />
            </div>
          </>
        )}
      </div>
      
      {isRecording && duration > maxDuration - 30 && (
        <div className="duration-warning">
          {maxDuration - duration}s remaining
        </div>
      )}
    </div>
  );
}

// Voice Message Player Component
export function VoiceMessagePlayer({ 
  recording 
}: { 
  recording: VoiceRecording 
}) {
  const [isPlaying, setIsPlaying] = useState(false);
  const [currentTime, setCurrentTime] = useState(0);
  const [playbackRate, setPlaybackRate] = useState(1);
  const [showTranscript, setShowTranscript] = useState(false);
  
  const audioRef = useRef<HTMLAudioElement>(null);
  const progressInterval = useRef<number>();
  
  useEffect(() => {
    const audio = audioRef.current;
    if (!audio) return;
    
    audio.addEventListener('ended', () => {
      setIsPlaying(false);
      setCurrentTime(0);
    });
    
    return () => {
      clearInterval(progressInterval.current);
    };
  }, []);
  
  const togglePlay = () => {
    const audio = audioRef.current;
    if (!audio) return;
    
    if (isPlaying) {
      audio.pause();
      clearInterval(progressInterval.current);
    } else {
      audio.play();
      progressInterval.current = setInterval(() => {
        setCurrentTime(audio.currentTime);
      }, 100);
    }
    
    setIsPlaying(!isPlaying);
  };
  
  const seek = (time: number) => {
    const audio = audioRef.current;
    if (!audio) return;
    
    audio.currentTime = time;
    setCurrentTime(time);
  };
  
  const changePlaybackRate = () => {
    const rates = [1, 1.25, 1.5, 2];
    const currentIndex = rates.indexOf(playbackRate);
    const nextRate = rates[(currentIndex + 1) % rates.length];
    
    setPlaybackRate(nextRate);
    if (audioRef.current) {
      audioRef.current.playbackRate = nextRate;
    }
  };
  
  const progress = (currentTime / recording.duration) * 100;
  
  return (
    <div className="voice-message-player">
      <audio
        ref={audioRef}
        src={recording.url}
        preload="metadata"
      />
      
      <div className="player-controls">
        <Button
          className="play-button"
          onClick={togglePlay}
          icon={isPlaying ? <PauseIcon /> : <PlayIcon />}
          variant="text"
          size="small"
        />
        
        <div className="player-info">
          <div className="waveform-container">
            <Waveform
              data={recording.waveform}
              progress={progress}
              onClick={(percent) => seek(recording.duration * percent)}
            />
          </div>
          
          <div className="time-info">
            <span className="current-time">{formatTime(currentTime)}</span>
            <span className="duration">{formatTime(recording.duration)}</span>
          </div>
        </div>
        
        <Button
          className="speed-button"
          onClick={changePlaybackRate}
          variant="text"
          size="small"
        >
          {playbackRate}x
        </Button>
        
        {recording.transcription && (
          <Button
            className="transcript-button"
            onClick={() => setShowTranscript(!showTranscript)}
            icon={<TranscriptIcon />}
            variant="text"
            size="small"
          />
        )}
      </div>
      
      {showTranscript && recording.transcription && (
        <div className="voice-transcript">
          <p>{recording.transcription.text}</p>
          <span className="confidence">
            {Math.round(recording.transcription.confidence * 100)}% confident
          </span>
        </div>
      )}
    </div>
  );
}

// Waveform Visualizer Component
export function Waveform({ 
  data, 
  progress = 0,
  onClick
}: WaveformProps) {
  const canvasRef = useRef<HTMLCanvasElement>(null);
  
  useEffect(() => {
    const canvas = canvasRef.current;
    if (!canvas) return;
    
    const ctx = canvas.getContext('2d');
    if (!ctx) return;
    
    // Clear canvas
    ctx.clearRect(0, 0, canvas.width, canvas.height);
    
    const barWidth = canvas.width / data.length;
    const barGap = 1;
    
    data.forEach((amplitude, index) => {
      const x = index * barWidth;
      const height = amplitude * canvas.height * 0.8;
      const y = (canvas.height - height) / 2;
      
      // Determine color based on progress
      const isPlayed = (index / data.length) * 100 <= progress;
      ctx.fillStyle = isPlayed ? '#3b82f6' : '#e5e7eb';
      
      ctx.fillRect(
        x + barGap / 2,
        y,
        barWidth - barGap,
        height
      );
    });
  }, [data, progress]);
  
  const handleClick = (e: React.MouseEvent<HTMLCanvasElement>) => {
    if (!onClick) return;
    
    const canvas = canvasRef.current;
    if (!canvas) return;
    
    const rect = canvas.getBoundingClientRect();
    const x = e.clientX - rect.left;
    const percent = x / canvas.width;
    
    onClick(percent);
  };
  
  return (
    <canvas
      ref={canvasRef}
      className="waveform"
      width={300}
      height={48}
      onClick={handleClick}
    />
  );
}

// Voice Visualizer Component
export function VoiceVisualizer({ amplitude }: { amplitude: number }) {
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const animationRef = useRef<number>();
  
  useEffect(() => {
    const canvas = canvasRef.current;
    if (!canvas) return;
    
    const ctx = canvas.getContext('2d');
    if (!ctx) return;
    
    const draw = () => {
      // Clear canvas with fade effect
      ctx.fillStyle = 'rgba(255, 255, 255, 0.1)';
      ctx.fillRect(0, 0, canvas.width, canvas.height);
      
      // Draw amplitude bars
      const barCount = 40;
      const barWidth = canvas.width / barCount;
      const barGap = 2;
      
      for (let i = 0; i < barCount; i++) {
        const barHeight = (Math.sin(i * 0.3 + Date.now() * 0.002) * 0.5 + 0.5) * 
                         amplitude * canvas.height * 0.8;
        
        const x = i * barWidth;
        const y = (canvas.height - barHeight) / 2;
        
        const hue = (i / barCount) * 60 + 200; // Blue to purple gradient
        ctx.fillStyle = `hsl(${hue}, 70%, 50%)`;
        
        ctx.fillRect(
          x + barGap / 2,
          y,
          barWidth - barGap,
          barHeight
        );
      }
      
      animationRef.current = requestAnimationFrame(draw);
    };
    
    draw();
    
    return () => {
      if (animationRef.current) {
        cancelAnimationFrame(animationRef.current);
      }
    };
  }, [amplitude]);
  
  return (
    <canvas
      ref={canvasRef}
      className="voice-visualizer"
      width={200}
      height={60}
    />
  );
}
```

### 5. Transcription Service
```typescript
// Transcription Service
export class TranscriptionService {
  constructor(
    private openaiApiKey: string,
    private cache: KVNamespace
  ) {}
  
  async transcribe(
    audioBuffer: ArrayBuffer,
    options: TranscriptionOptions
  ): Promise<TranscriptionResult> {
    // Check cache first
    const cacheKey = this.getCacheKey(audioBuffer);
    const cached = await this.cache.get(cacheKey);
    if (cached) {
      return JSON.parse(cached);
    }
    
    try {
      // Convert to format OpenAI expects
      const audioBlob = new Blob([audioBuffer], { type: 'audio/mp3' });
      const formData = new FormData();
      formData.append('file', audioBlob, 'audio.mp3');
      formData.append('model', options.model || 'whisper-1');
      
      if (options.language && options.language !== 'auto') {
        formData.append('language', options.language);
      }
      
      if (options.prompt) {
        formData.append('prompt', options.prompt);
      }
      
      // Call OpenAI Whisper API
      const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.openaiApiKey}`
        },
        body: formData
      });
      
      if (!response.ok) {
        throw new Error(`Transcription failed: ${response.statusText}`);
      }
      
      const data = await response.json();
      
      const result: TranscriptionResult = {
        text: data.text,
        language: data.language || options.language || 'en',
        confidence: this.estimateConfidence(data),
        segments: data.segments,
        duration: data.duration
      };
      
      // Cache result
      await this.cache.put(
        cacheKey,
        JSON.stringify(result),
        { expirationTtl: 86400 } // 24 hours
      );
      
      return result;
      
    } catch (error) {
      console.error('Transcription error:', error);
      throw error;
    }
  }
  
  private getCacheKey(audioBuffer: ArrayBuffer): string {
    // Generate hash of audio data
    const hash = createHash('sha256');
    hash.update(new Uint8Array(audioBuffer));
    return `transcription:${hash.digest('hex')}`;
  }
  
  private estimateConfidence(data: any): number {
    // Whisper doesn't provide confidence scores, so we estimate
    // based on various factors
    let confidence = 0.85; // Base confidence
    
    // Adjust based on language detection confidence
    if (data.language_confidence) {
      confidence *= data.language_confidence;
    }
    
    // Adjust based on duration (very short clips are less reliable)
    if (data.duration < 1) {
      confidence *= 0.8;
    }
    
    // Adjust based on detected issues
    if (data.text.includes('[inaudible]') || data.text.includes('[?]')) {
      confidence *= 0.7;
    }
    
    return Math.max(0.5, Math.min(1, confidence));
  }
  
  async translateTranscription(
    text: string,
    fromLanguage: string,
    toLanguage: string
  ): Promise<string> {
    // Use translation API if languages differ
    if (fromLanguage === toLanguage) {
      return text;
    }
    
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.openaiApiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'gpt-3.5-turbo',
        messages: [
          {
            role: 'system',
            content: `Translate the following text from ${fromLanguage} to ${toLanguage}. Only provide the translation, no explanations.`
          },
          {
            role: 'user',
            content: text
          }
        ],
        temperature: 0.3,
        max_tokens: 500
      })
    });
    
    const data = await response.json();
    return data.choices[0].message.content;
  }
}
```

### 6. Mobile Integration
```typescript
// Mobile Voice Recording Handler
export class MobileVoiceHandler {
  private wakeLock?: WakeLockSentinel;
  
  async enableMobileRecording(): Promise<void> {
    // Request wake lock to prevent screen from sleeping
    if ('wakeLock' in navigator) {
      try {
        this.wakeLock = await navigator.wakeLock.request('screen');
      } catch (error) {
        console.warn('Wake lock failed:', error);
      }
    }
    
    // Handle iOS audio session
    if (this.isIOS()) {
      await this.setupIOSAudioSession();
    }
    
    // Handle Android audio focus
    if (this.isAndroid()) {
      await this.requestAndroidAudioFocus();
    }
  }
  
  async disableMobileRecording(): Promise<void> {
    // Release wake lock
    if (this.wakeLock) {
      await this.wakeLock.release();
      this.wakeLock = undefined;
    }
  }
  
  private async setupIOSAudioSession(): Promise<void> {
    // Play silent audio to activate audio session
    const audio = new Audio('data:audio/wav;base64,UklGRnoGAABXQVZFZm10IBAAAAABAAEAQB8AAEAfAAABAAgAZGF0YQoGAACBhYqFbF1fdJivrJBhNjVgodDbq2EcBj+a2/LDciUFLIHO8tiJNwgZaLvt559NEAxQp+PwtmMcBjiR1/LMeSwFJHfH8N2QQAoUXrTp66hVFApGn+DyvmwhBjiS2Oy9diMFl2+z2mVLBTRo0Os=');
    audio.volume = 0.001;
    await audio.play();
    audio.pause();
  }
  
  private async requestAndroidAudioFocus(): Promise<void> {
    // This would typically be handled by the native Android app
    // For PWA, we ensure proper audio context state
    const audioContext = new AudioContext();
    if (audioContext.state === 'suspended') {
      await audioContext.resume();
    }
  }
  
  private isIOS(): boolean {
    return /iPad|iPhone|iPod/.test(navigator.userAgent);
  }
  
  private isAndroid(): boolean {
    return /Android/.test(navigator.userAgent);
  }
}
```

## Dependencies
- Web Audio API for recording
- MediaRecorder API
- Cloudflare R2 for storage
- OpenAI Whisper API for transcription
- Audio processing libraries (lamejs for MP3)
- Wake Lock API for mobile

## Estimated Effort
**4 days**
- 1 day: Voice recording implementation
- 1 day: Audio processing and compression
- 1 day: UI components and playback
- 0.5 day: Transcription integration
- 0.5 day: Mobile optimization and testing

## Notes
- Consider implementing voice activity detection (VAD)
- Add support for voice effects (pitch, speed)
- Implement automatic language detection
- Consider adding voice-to-text commands
- Plan for offline recording capability
- Add support for voice replies to messages