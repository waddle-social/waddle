# Issue #47: RealTimeKit iOS

## User Story
As an **iOS developer**, I want to **integrate RealTimeKit voice capabilities into the iOS app** so that **users can participate in voice chat with low latency and high quality audio**.

## Description
Implement RealTimeKit integration for iOS, providing native Swift bindings and iOS-specific optimizations for voice chat functionality. This includes audio session management, WebRTC integration, and platform-specific features like CallKit support.

## Acceptance Criteria
- [ ] Swift package for RealTimeKit integration
- [ ] Native audio session management
- [ ] WebRTC Swift bindings
- [ ] CallKit integration for system UI
- [ ] Background audio support
- [ ] Bluetooth device handling
- [ ] Network quality adaptation
- [ ] Voice activity detection

## Technical Implementation

### 1. RealTimeKit Swift Package
```swift
// Package.swift
// swift-tools-version: 5.9
import PackageDescription

let package = Package(
    name: "WaddleRealTimeKit",
    platforms: [
        .iOS(.v16)
    ],
    products: [
        .library(
            name: "WaddleRealTimeKit",
            targets: ["WaddleRealTimeKit"]
        ),
    ],
    dependencies: [
        .package(url: "https://github.com/webrtc-sdk/ios", from: "114.0.0"),
        .package(url: "https://github.com/daltoniam/Starscream", from: "4.0.0")
    ],
    targets: [
        .target(
            name: "WaddleRealTimeKit",
            dependencies: [
                .product(name: "WebRTC", package: "ios"),
                .product(name: "Starscream", package: "Starscream")
            ]
        ),
        .testTarget(
            name: "WaddleRealTimeKitTests",
            dependencies: ["WaddleRealTimeKit"]
        ),
    ]
)

// RealTimeKit Core
import Foundation
import WebRTC
import AVFoundation
import CallKit

public protocol RealTimeKitDelegate: AnyObject {
    func realTimeKit(_ rtk: RealTimeKit, didChangeConnectionState state: ConnectionState)
    func realTimeKit(_ rtk: RealTimeKit, didReceiveError error: RTKError)
    func realTimeKit(_ rtk: RealTimeKit, didJoinChannel channel: VoiceChannel)
    func realTimeKit(_ rtk: RealTimeKit, didUpdateParticipants participants: [Participant])
    func realTimeKit(_ rtk: RealTimeKit, didReceiveAudioLevel level: Float, for participantId: String)
}

public class RealTimeKit {
    public weak var delegate: RealTimeKitDelegate?
    
    private let signaling: SignalingClient
    private let webRTCClient: WebRTCClient
    private let audioSession: AudioSessionManager
    private let callController: CallController
    
    private var currentChannel: VoiceChannel?
    private var participants: [String: Participant] = [:]
    
    public init(configuration: RTKConfiguration) {
        self.signaling = SignalingClient(url: configuration.signalingURL)
        self.webRTCClient = WebRTCClient(configuration: configuration.webRTCConfig)
        self.audioSession = AudioSessionManager()
        self.callController = CallController()
        
        setupConnections()
    }
    
    private func setupConnections() {
        signaling.delegate = self
        webRTCClient.delegate = self
        
        // Configure audio session
        audioSession.configure(for: .voiceChat)
    }
    
    // MARK: - Public Methods
    
    public func connect() async throws {
        try await signaling.connect()
        delegate?.realTimeKit(self, didChangeConnectionState: .connecting)
    }
    
    public func joinChannel(_ channelId: String) async throws {
        // Request to join channel
        let joinRequest = JoinChannelRequest(channelId: channelId)
        try await signaling.send(joinRequest)
        
        // Setup WebRTC peer connection
        try await webRTCClient.createPeerConnection()
        
        // Configure audio
        audioSession.activate()
        
        // Start CallKit session
        let channelName = "Waddle Voice Chat"
        callController.startCall(channelId: channelId, channelName: channelName)
    }
    
    public func leaveChannel() async throws {
        guard let channel = currentChannel else { return }
        
        // Leave signaling channel
        let leaveRequest = LeaveChannelRequest(channelId: channel.id)
        try await signaling.send(leaveRequest)
        
        // Clean up WebRTC
        webRTCClient.disconnect()
        
        // Deactivate audio
        audioSession.deactivate()
        
        // End CallKit session
        callController.endCall(channelId: channel.id)
        
        currentChannel = nil
        participants.removeAll()
    }
    
    public func setMuted(_ muted: Bool) {
        webRTCClient.setAudioEnabled(!muted)
        
        if let channel = currentChannel {
            callController.setMuted(muted, for: channel.id)
        }
    }
    
    public func setSpeaker(_ enabled: Bool) {
        audioSession.setSpeaker(enabled)
    }
}
```

### 2. Audio Session Management
```swift
// Audio session manager for iOS
import AVFoundation

class AudioSessionManager {
    private let audioSession = AVAudioSession.sharedInstance()
    private var audioSessionObservers: [NSObjectProtocol] = []
    
    enum AudioMode {
        case voiceChat
        case video
        case music
    }
    
    func configure(for mode: AudioMode) {
        do {
            switch mode {
            case .voiceChat:
                try audioSession.setCategory(.playAndRecord, mode: .voiceChat, options: [
                    .allowBluetooth,
                    .allowBluetoothA2DP,
                    .defaultToSpeaker,
                    .mixWithOthers
                ])
                
            case .video:
                try audioSession.setCategory(.playAndRecord, mode: .videoChat, options: [
                    .allowBluetooth,
                    .defaultToSpeaker,
                    .mixWithOthers
                ])
                
            case .music:
                try audioSession.setCategory(.playback, mode: .default, options: [
                    .mixWithOthers
                ])
            }
            
            // Configure audio session properties
            try audioSession.setPreferredSampleRate(48000)
            try audioSession.setPreferredIOBufferDuration(0.005) // 5ms for low latency
            
        } catch {
            Logger.error("Failed to configure audio session: \(error)")
        }
        
        setupObservers()
    }
    
    func activate() {
        do {
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
            Logger.info("Audio session activated")
        } catch {
            Logger.error("Failed to activate audio session: \(error)")
        }
    }
    
    func deactivate() {
        do {
            try audioSession.setActive(false, options: .notifyOthersOnDeactivation)
            Logger.info("Audio session deactivated")
        } catch {
            Logger.error("Failed to deactivate audio session: \(error)")
        }
    }
    
    func setSpeaker(_ enabled: Bool) {
        do {
            try audioSession.overrideOutputAudioPort(enabled ? .speaker : .none)
        } catch {
            Logger.error("Failed to set speaker: \(error)")
        }
    }
    
    private func setupObservers() {
        // Route change observer
        let routeChangeObserver = NotificationCenter.default.addObserver(
            forName: AVAudioSession.routeChangeNotification,
            object: audioSession,
            queue: .main
        ) { [weak self] notification in
            self?.handleRouteChange(notification)
        }
        audioSessionObservers.append(routeChangeObserver)
        
        // Interruption observer
        let interruptionObserver = NotificationCenter.default.addObserver(
            forName: AVAudioSession.interruptionNotification,
            object: audioSession,
            queue: .main
        ) { [weak self] notification in
            self?.handleInterruption(notification)
        }
        audioSessionObservers.append(interruptionObserver)
    }
    
    private func handleRouteChange(_ notification: Notification) {
        guard let userInfo = notification.userInfo,
              let reasonValue = userInfo[AVAudioSession.routeChangeReasonKey] as? UInt,
              let reason = AVAudioSession.RouteChangeReason(rawValue: reasonValue) else {
            return
        }
        
        switch reason {
        case .newDeviceAvailable:
            Logger.info("New audio device available")
            updateAudioRoute()
            
        case .oldDeviceUnavailable:
            Logger.info("Audio device became unavailable")
            updateAudioRoute()
            
        case .categoryChange:
            Logger.info("Audio category changed")
            
        default:
            break
        }
    }
    
    private func handleInterruption(_ notification: Notification) {
        guard let userInfo = notification.userInfo,
              let typeValue = userInfo[AVAudioSession.interruptionTypeKey] as? UInt,
              let type = AVAudioSession.InterruptionType(rawValue: typeValue) else {
            return
        }
        
        switch type {
        case .began:
            Logger.info("Audio interruption began")
            // Pause audio
            NotificationCenter.default.post(name: .audioInterruptionBegan, object: nil)
            
        case .ended:
            if let optionsValue = userInfo[AVAudioSession.interruptionOptionKey] as? UInt {
                let options = AVAudioSession.InterruptionOptions(rawValue: optionsValue)
                if options.contains(.shouldResume) {
                    Logger.info("Audio interruption ended - should resume")
                    activate()
                    NotificationCenter.default.post(name: .audioInterruptionEnded, object: nil)
                }
            }
            
        @unknown default:
            break
        }
    }
    
    private func updateAudioRoute() {
        let currentRoute = audioSession.currentRoute
        
        for output in currentRoute.outputs {
            Logger.info("Audio output: \(output.portName) - \(output.portType.rawValue)")
            
            switch output.portType {
            case .bluetoothA2DP, .bluetoothHFP, .bluetoothLE:
                handleBluetoothConnection(output)
            case .builtInSpeaker:
                handleSpeakerOutput()
            case .builtInReceiver:
                handleReceiverOutput()
            default:
                break
            }
        }
    }
    
    private func handleBluetoothConnection(_ output: AVAudioSessionPortDescription) {
        Logger.info("Bluetooth device connected: \(output.portName)")
        
        // Optimize for Bluetooth latency
        do {
            try audioSession.setPreferredIOBufferDuration(0.01) // 10ms for Bluetooth
        } catch {
            Logger.error("Failed to set Bluetooth buffer duration: \(error)")
        }
    }
}
```

### 3. WebRTC Integration
```swift
// WebRTC client for iOS
import WebRTC

class WebRTCClient: NSObject {
    weak var delegate: WebRTCClientDelegate?
    
    private let factory: RTCPeerConnectionFactory
    private var peerConnection: RTCPeerConnection?
    private var localAudioTrack: RTCAudioTrack?
    private var audioSource: RTCAudioSource?
    
    private let configuration: RTCConfiguration
    
    override init() {
        // Initialize WebRTC
        RTCInitializeSSL()
        
        // Create peer connection factory
        let encoderFactory = RTCDefaultVideoEncoderFactory()
        let decoderFactory = RTCDefaultVideoDecoderFactory()
        
        factory = RTCPeerConnectionFactory(
            encoderFactory: encoderFactory,
            decoderFactory: decoderFactory
        )
        
        // Configure STUN/TURN servers
        configuration = RTCConfiguration()
        configuration.iceServers = [
            RTCIceServer(urlStrings: ["stun:stun.waddle.chat:3478"]),
            RTCIceServer(
                urlStrings: ["turn:turn.waddle.chat:3478"],
                username: "waddle",
                credential: "turnpassword"
            )
        ]
        configuration.sdpSemantics = .unifiedPlan
        configuration.continualGatheringPolicy = .gatherContinually
        
        super.init()
    }
    
    func createPeerConnection() async throws {
        let constraints = RTCMediaConstraints(
            mandatoryConstraints: nil,
            optionalConstraints: ["DtlsSrtpKeyAgreement": kRTCMediaConstraintsValueTrue]
        )
        
        guard let pc = factory.peerConnection(
            with: configuration,
            constraints: constraints,
            delegate: self
        ) else {
            throw RTKError.peerConnectionFailed
        }
        
        peerConnection = pc
        
        // Add audio track
        addAudioTrack()
    }
    
    private func addAudioTrack() {
        let audioConstraints = RTCMediaConstraints(
            mandatoryConstraints: [
                "googEchoCancellation": kRTCMediaConstraintsValueTrue,
                "googAutoGainControl": kRTCMediaConstraintsValueTrue,
                "googNoiseSuppression": kRTCMediaConstraintsValueTrue,
                "googHighpassFilter": kRTCMediaConstraintsValueTrue
            ],
            optionalConstraints: nil
        )
        
        audioSource = factory.audioSource(with: audioConstraints)
        localAudioTrack = factory.audioTrack(with: audioSource!, trackId: "audio0")
        
        if let track = localAudioTrack {
            peerConnection?.add(track, streamIds: ["stream0"])
        }
    }
    
    func createOffer() async throws -> RTCSessionDescription {
        let constraints = RTCMediaConstraints(
            mandatoryConstraints: [
                "OfferToReceiveAudio": kRTCMediaConstraintsValueTrue,
                "OfferToReceiveVideo": kRTCMediaConstraintsValueFalse
            ],
            optionalConstraints: nil
        )
        
        return try await withCheckedThrowingContinuation { continuation in
            peerConnection?.offer(for: constraints) { sdp, error in
                if let error = error {
                    continuation.resume(throwing: error)
                } else if let sdp = sdp {
                    continuation.resume(returning: sdp)
                } else {
                    continuation.resume(throwing: RTKError.createOfferFailed)
                }
            }
        }
    }
    
    func setRemoteDescription(_ sdp: RTCSessionDescription) async throws {
        try await withCheckedThrowingContinuation { continuation in
            peerConnection?.setRemoteDescription(sdp) { error in
                if let error = error {
                    continuation.resume(throwing: error)
                } else {
                    continuation.resume()
                }
            }
        }
    }
    
    func addIceCandidate(_ candidate: RTCIceCandidate) {
        peerConnection?.add(candidate)
    }
    
    func setAudioEnabled(_ enabled: Bool) {
        localAudioTrack?.isEnabled = enabled
    }
    
    func getStats() async -> RTCStatisticsReport? {
        await withCheckedContinuation { continuation in
            peerConnection?.statistics { report in
                continuation.resume(returning: report)
            }
        }
    }
}

// MARK: - RTCPeerConnectionDelegate
extension WebRTCClient: RTCPeerConnectionDelegate {
    func peerConnection(_ peerConnection: RTCPeerConnection, didChange stateChanged: RTCSignalingState) {
        Logger.debug("Signaling state changed: \(stateChanged.rawValue)")
    }
    
    func peerConnection(_ peerConnection: RTCPeerConnection, didAdd stream: RTCMediaStream) {
        Logger.debug("Media stream added: \(stream.streamId)")
        
        if let audioTrack = stream.audioTracks.first {
            delegate?.webRTCClient(self, didReceiveRemoteAudioTrack: audioTrack)
        }
    }
    
    func peerConnection(_ peerConnection: RTCPeerConnection, didRemove stream: RTCMediaStream) {
        Logger.debug("Media stream removed: \(stream.streamId)")
    }
    
    func peerConnectionShouldNegotiate(_ peerConnection: RTCPeerConnection) {
        Logger.debug("Negotiation needed")
        delegate?.webRTCClientShouldNegotiate(self)
    }
    
    func peerConnection(_ peerConnection: RTCPeerConnection, didChange newState: RTCIceConnectionState) {
        Logger.debug("ICE connection state changed: \(newState.rawValue)")
        
        let connectionState: ConnectionState
        switch newState {
        case .new, .checking:
            connectionState = .connecting
        case .connected, .completed:
            connectionState = .connected
        case .failed:
            connectionState = .failed
        case .disconnected:
            connectionState = .disconnected
        case .closed:
            connectionState = .closed
        default:
            connectionState = .unknown
        }
        
        delegate?.webRTCClient(self, didChangeConnectionState: connectionState)
    }
    
    func peerConnection(_ peerConnection: RTCPeerConnection, didGenerate candidate: RTCIceCandidate) {
        delegate?.webRTCClient(self, didGenerateIceCandidate: candidate)
    }
}
```

### 4. CallKit Integration
```swift
// CallKit integration for system UI
import CallKit

class CallController: NSObject {
    private let callController = CXCallController()
    private let provider: CXProvider
    private var activeCalls: [String: UUID] = [:]
    
    override init() {
        let configuration = CXProviderConfiguration(localizedName: "Waddle")
        configuration.supportsVideo = false
        configuration.maximumCallsPerCallGroup = 1
        configuration.supportedHandleTypes = [.generic]
        configuration.iconTemplateImageData = UIImage(named: "CallKitIcon")?.pngData()
        
        provider = CXProvider(configuration: configuration)
        
        super.init()
        
        provider.setDelegate(self, queue: nil)
    }
    
    func startCall(channelId: String, channelName: String) {
        let callUUID = UUID()
        activeCalls[channelId] = callUUID
        
        let handle = CXHandle(type: .generic, value: channelId)
        let startCallAction = CXStartCallAction(call: callUUID, handle: handle)
        startCallAction.isVideo = false
        startCallAction.contactIdentifier = channelName
        
        let transaction = CXTransaction(action: startCallAction)
        
        callController.request(transaction) { error in
            if let error = error {
                Logger.error("Failed to start call: \(error)")
            } else {
                Logger.info("Call started successfully")
            }
        }
    }
    
    func endCall(channelId: String) {
        guard let callUUID = activeCalls[channelId] else { return }
        
        let endCallAction = CXEndCallAction(call: callUUID)
        let transaction = CXTransaction(action: endCallAction)
        
        callController.request(transaction) { error in
            if let error = error {
                Logger.error("Failed to end call: \(error)")
            }
        }
        
        activeCalls.removeValue(forKey: channelId)
    }
    
    func setMuted(_ muted: Bool, for channelId: String) {
        guard let callUUID = activeCalls[channelId] else { return }
        
        let muteAction = CXSetMutedCallAction(call: callUUID, muted: muted)
        let transaction = CXTransaction(action: muteAction)
        
        callController.request(transaction) { error in
            if let error = error {
                Logger.error("Failed to set mute state: \(error)")
            }
        }
    }
    
    func reportIncomingCall(channelId: String, channelName: String) {
        let callUUID = UUID()
        activeCalls[channelId] = callUUID
        
        let update = CXCallUpdate()
        update.remoteHandle = CXHandle(type: .generic, value: channelId)
        update.localizedCallerName = channelName
        update.hasVideo = false
        update.supportsGrouping = false
        update.supportsUngrouping = false
        update.supportsHolding = false
        
        provider.reportNewIncomingCall(with: callUUID, update: update) { error in
            if let error = error {
                Logger.error("Failed to report incoming call: \(error)")
            }
        }
    }
}

// MARK: - CXProviderDelegate
extension CallController: CXProviderDelegate {
    func providerDidReset(_ provider: CXProvider) {
        // Handle provider reset
        activeCalls.removeAll()
    }
    
    func provider(_ provider: CXProvider, perform action: CXStartCallAction) {
        // Configure audio session for call
        action.fulfill()
    }
    
    func provider(_ provider: CXProvider, perform action: CXAnswerCallAction) {
        // Answer incoming call
        action.fulfill()
    }
    
    func provider(_ provider: CXProvider, perform action: CXEndCallAction) {
        // End call
        action.fulfill()
    }
    
    func provider(_ provider: CXProvider, perform action: CXSetMutedCallAction) {
        // Set mute state
        action.fulfill()
    }
    
    func provider(_ provider: CXProvider, didActivate audioSession: AVAudioSession) {
        // Audio session activated by CallKit
        Logger.info("CallKit activated audio session")
    }
    
    func provider(_ provider: CXProvider, didDeactivate audioSession: AVAudioSession) {
        // Audio session deactivated by CallKit
        Logger.info("CallKit deactivated audio session")
    }
}
```

### 5. Voice Activity Detection
```swift
// Voice activity detection for iOS
import Accelerate

class VoiceActivityDetector {
    private let sampleRate: Double = 48000
    private let frameSize: Int = 480 // 10ms at 48kHz
    private let energyThreshold: Float = -40.0 // dB
    private let speechThreshold: Float = 0.3
    
    private var energyHistory: [Float] = []
    private let historySize = 30 // 300ms of history
    
    func processSamples(_ samples: [Float]) -> Bool {
        // Calculate RMS energy
        let energy = calculateRMS(samples)
        let energyDB = 20 * log10(energy)
        
        // Update history
        energyHistory.append(energyDB)
        if energyHistory.count > historySize {
            energyHistory.removeFirst()
        }
        
        // Calculate voice activity
        return detectVoiceActivity()
    }
    
    private func calculateRMS(_ samples: [Float]) -> Float {
        var rms: Float = 0
        vDSP_rmsqv(samples, 1, &rms, vDSP_Length(samples.count))
        return rms
    }
    
    private func detectVoiceActivity() -> Bool {
        guard energyHistory.count >= 10 else { return false }
        
        // Calculate statistics
        let recentEnergy = Array(energyHistory.suffix(10))
        let averageEnergy = recentEnergy.reduce(0, +) / Float(recentEnergy.count)
        
        // Simple VAD logic
        let isSpeech = averageEnergy > energyThreshold
        
        // Additional checks for speech patterns
        let variance = calculateVariance(recentEnergy)
        let hasSpeechPattern = variance > 5.0 // Speech has more variation
        
        return isSpeech && hasSpeechPattern
    }
    
    private func calculateVariance(_ values: [Float]) -> Float {
        let mean = values.reduce(0, +) / Float(values.count)
        let squaredDifferences = values.map { pow($0 - mean, 2) }
        return squaredDifferences.reduce(0, +) / Float(values.count)
    }
}

// Audio level monitoring
class AudioLevelMonitor {
    private let updateInterval: TimeInterval = 0.1
    private var timer: Timer?
    private var audioLevelCallback: ((Float) -> Void)?
    
    func startMonitoring(callback: @escaping (Float) -> Void) {
        audioLevelCallback = callback
        
        timer = Timer.scheduledTimer(withTimeInterval: updateInterval, repeats: true) { _ in
            self.measureAudioLevel()
        }
    }
    
    func stopMonitoring() {
        timer?.invalidate()
        timer = nil
        audioLevelCallback = nil
    }
    
    private func measureAudioLevel() {
        // Get audio level from WebRTC
        // This would integrate with the WebRTC audio track
        let level = getCurrentAudioLevel()
        audioLevelCallback?(level)
    }
    
    private func getCurrentAudioLevel() -> Float {
        // Placeholder - would get actual level from WebRTC
        return Float.random(in: 0...1)
    }
}
```

### 6. Network Quality Adaptation
```swift
// Network quality monitor and adaptation
import Network

class NetworkQualityMonitor {
    private let monitor = NWPathMonitor()
    private let queue = DispatchQueue(label: "com.waddle.networkmonitor")
    
    var onNetworkChange: ((NetworkStatus) -> Void)?
    
    struct NetworkStatus {
        let isConnected: Bool
        let isExpensive: Bool
        let connectionType: ConnectionType
        
        enum ConnectionType {
            case wifi
            case cellular
            case wired
            case unknown
        }
    }
    
    func startMonitoring() {
        monitor.pathUpdateHandler = { [weak self] path in
            self?.handlePathUpdate(path)
        }
        
        monitor.start(queue: queue)
    }
    
    func stopMonitoring() {
        monitor.cancel()
    }
    
    private func handlePathUpdate(_ path: NWPath) {
        let status = NetworkStatus(
            isConnected: path.status == .satisfied,
            isExpensive: path.isExpensive,
            connectionType: getConnectionType(path)
        )
        
        DispatchQueue.main.async {
            self.onNetworkChange?(status)
        }
    }
    
    private func getConnectionType(_ path: NWPath) -> NetworkStatus.ConnectionType {
        if path.usesInterfaceType(.wifi) {
            return .wifi
        } else if path.usesInterfaceType(.cellular) {
            return .cellular
        } else if path.usesInterfaceType(.wiredEthernet) {
            return .wired
        } else {
            return .unknown
        }
    }
}

// Adaptive quality controller
class AdaptiveQualityController {
    private let webRTCClient: WebRTCClient
    private let networkMonitor: NetworkQualityMonitor
    
    private var currentBitrate: Int = 32000
    private var statsTimer: Timer?
    
    init(webRTCClient: WebRTCClient, networkMonitor: NetworkQualityMonitor) {
        self.webRTCClient = webRTCClient
        self.networkMonitor = networkMonitor
        
        setupMonitoring()
    }
    
    private func setupMonitoring() {
        // Monitor network changes
        networkMonitor.onNetworkChange = { [weak self] status in
            self?.handleNetworkChange(status)
        }
        
        // Monitor WebRTC stats
        statsTimer = Timer.scheduledTimer(withTimeInterval: 2.0, repeats: true) { _ in
            Task {
                await self.checkConnectionQuality()
            }
        }
    }
    
    private func handleNetworkChange(_ status: NetworkQualityMonitor.NetworkStatus) {
        switch status.connectionType {
        case .wifi:
            updateBitrate(32000)
        case .cellular:
            updateBitrate(status.isExpensive ? 16000 : 24000)
        case .wired:
            updateBitrate(48000)
        case .unknown:
            updateBitrate(16000)
        }
    }
    
    private func checkConnectionQuality() async {
        guard let stats = await webRTCClient.getStats() else { return }
        
        // Analyze stats and adjust quality
        for (_, stat) in stats.statistics {
            if stat.type == "candidate-pair" {
                if let rtt = stat.values["currentRoundTripTime"] as? Double {
                    adjustForLatency(rtt)
                }
            }
            
            if stat.type == "inbound-rtp" {
                if let packetsLost = stat.values["packetsLost"] as? Int,
                   let packetsReceived = stat.values["packetsReceived"] as? Int {
                    let lossRate = Double(packetsLost) / Double(packetsReceived + packetsLost)
                    adjustForPacketLoss(lossRate)
                }
            }
        }
    }
    
    private func adjustForLatency(_ rtt: Double) {
        if rtt > 300 {
            // High latency - reduce bitrate
            updateBitrate(min(currentBitrate, 16000))
        } else if rtt < 100 {
            // Low latency - can increase bitrate
            updateBitrate(min(currentBitrate + 4000, 48000))
        }
    }
    
    private func adjustForPacketLoss(_ lossRate: Double) {
        if lossRate > 0.05 {
            // High packet loss - reduce bitrate
            updateBitrate(Int(Double(currentBitrate) * 0.8))
        } else if lossRate < 0.01 {
            // Low packet loss - can increase bitrate
            updateBitrate(min(currentBitrate + 4000, 48000))
        }
    }
    
    private func updateBitrate(_ bitrate: Int) {
        currentBitrate = bitrate
        // Apply bitrate change to WebRTC
        // This would update the audio encoder settings
        Logger.info("Updated bitrate to \(bitrate)bps")
    }
}
```

## Dependencies
- WebRTC iOS SDK
- CallKit framework
- AVFoundation framework
- Network framework
- Accelerate framework

## Estimated Effort
**7 days**
- 2 days: Swift package setup and core integration
- 1 day: Audio session management
- 1 day: WebRTC bindings and configuration
- 1 day: CallKit integration
- 1 day: Voice activity detection
- 1 day: Network adaptation and testing

## Notes
- Ensure proper background mode configuration
- Test with various Bluetooth devices
- Implement proper error recovery
- Consider battery optimization
- Plan for App Store audio permissions