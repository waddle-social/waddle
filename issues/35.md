# Issue #35: AI Voice Agents

## User Story
As a **waddle owner**, I want to **add AI-powered voice agents to channels** so that **members can have natural voice conversations with AI assistants for help, entertainment, or companionship**.

## Description
Implement AI voice agents using ElevenLabs for text-to-speech, combining with GPT-4 for conversational AI. These agents can join voice channels, listen to conversations, respond naturally, and provide various services like moderation, translation, or interactive experiences.

## Acceptance Criteria
- [ ] ElevenLabs integration for natural TTS
- [ ] Real-time voice synthesis
- [ ] Multiple voice personalities
- [ ] Context-aware conversations
- [ ] Voice activity detection
- [ ] Interrupt handling
- [ ] Multi-language support
- [ ] Custom agent personalities
- [ ] Usage analytics and limits
- [ ] Agent management interface

## Technical Implementation

### 1. AI Voice Agent Architecture
```typescript
// Voice Agent Types
export interface VoiceAgent {
  id: string;
  waddleId: string;
  name: string;
  personality: AgentPersonality;
  voice: ElevenLabsVoice;
  capabilities: AgentCapability[];
  language: string;
  welcomeMessage?: string;
  systemPrompt: string;
  maxTokensPerResponse: number;
  interruptionThreshold: number; // 0-1, how easily interrupted
  responseDelay: number; // ms before responding
  status: 'active' | 'inactive' | 'processing';
  usage: {
    charactersGenerated: number;
    tokensUsed: number;
    conversationCount: number;
  };
  createdAt: Date;
  updatedAt: Date;
}

export interface AgentPersonality {
  name: string;
  description: string;
  traits: string[];
  speakingStyle: 'formal' | 'casual' | 'friendly' | 'professional';
  humorLevel: number; // 0-10
  verbosity: number; // 0-10
  emotionalRange: number; // 0-10
}

export interface ElevenLabsVoice {
  voiceId: string;
  name: string;
  preview_url?: string;
  settings: {
    stability: number; // 0-1
    similarityBoost: number; // 0-1
    style?: number; // 0-1
    useSpeakerBoost?: boolean;
  };
}

export enum AgentCapability {
  CONVERSATION = 'conversation',
  MODERATION = 'moderation',
  TRANSLATION = 'translation',
  MUSIC_DJ = 'music_dj',
  GAME_MASTER = 'game_master',
  ASSISTANT = 'assistant',
  STORYTELLER = 'storyteller'
}

// Voice Agent Service
export class VoiceAgentService {
  private agents = new Map<string, VoiceAgentInstance>();
  
  constructor(
    private elevenLabs: ElevenLabsClient,
    private openai: OpenAIClient,
    private voiceService: VoiceChannelService,
    private db: D1Database
  ) {}
  
  async createAgent(config: CreateAgentConfig): Promise<VoiceAgent> {
    // Validate voice exists
    const voice = await this.elevenLabs.getVoice(config.voiceId);
    if (!voice) {
      throw new Error('Invalid voice ID');
    }
    
    const agent: VoiceAgent = {
      id: generateId(),
      waddleId: config.waddleId,
      name: config.name,
      personality: config.personality,
      voice: {
        voiceId: config.voiceId,
        name: voice.name,
        preview_url: voice.preview_url,
        settings: config.voiceSettings || {
          stability: 0.75,
          similarityBoost: 0.75,
          style: 0.5,
          useSpeakerBoost: true
        }
      },
      capabilities: config.capabilities,
      language: config.language || 'en',
      welcomeMessage: config.welcomeMessage,
      systemPrompt: this.generateSystemPrompt(config),
      maxTokensPerResponse: config.maxTokensPerResponse || 150,
      interruptionThreshold: config.interruptionThreshold || 0.7,
      responseDelay: config.responseDelay || 1000,
      status: 'inactive',
      usage: {
        charactersGenerated: 0,
        tokensUsed: 0,
        conversationCount: 0
      },
      createdAt: new Date(),
      updatedAt: new Date()
    };
    
    // Save to database
    await this.saveAgent(agent);
    
    return agent;
  }
  
  async joinChannel(
    agentId: string,
    channelId: string
  ): Promise<void> {
    const agent = await this.getAgent(agentId);
    if (!agent) {
      throw new Error('Agent not found');
    }
    
    // Check if agent already active
    if (this.agents.has(agentId)) {
      throw new Error('Agent is already active');
    }
    
    // Create agent instance
    const instance = new VoiceAgentInstance(
      agent,
      this.elevenLabs,
      this.openai,
      this.voiceService
    );
    
    // Join voice channel
    await instance.joinChannel(channelId);
    
    // Store instance
    this.agents.set(agentId, instance);
    
    // Update status
    await this.updateAgentStatus(agentId, 'active');
    
    // Send welcome message if configured
    if (agent.welcomeMessage) {
      await instance.speak(agent.welcomeMessage);
    }
  }
  
  async leaveChannel(agentId: string): Promise<void> {
    const instance = this.agents.get(agentId);
    if (!instance) {
      throw new Error('Agent not active');
    }
    
    // Leave channel
    await instance.leaveChannel();
    
    // Clean up
    this.agents.delete(agentId);
    
    // Update status
    await this.updateAgentStatus(agentId, 'inactive');
  }
  
  private generateSystemPrompt(config: CreateAgentConfig): string {
    const personality = config.personality;
    
    let prompt = `You are ${config.name}, an AI voice assistant with the following characteristics:\n\n`;
    
    prompt += `Personality: ${personality.description}\n`;
    prompt += `Traits: ${personality.traits.join(', ')}\n`;
    prompt += `Speaking style: ${personality.speakingStyle}\n`;
    prompt += `Humor level: ${personality.humorLevel}/10\n`;
    prompt += `Verbosity: ${personality.verbosity}/10\n`;
    prompt += `Emotional range: ${personality.emotionalRange}/10\n\n`;
    
    prompt += `Your capabilities include: ${config.capabilities.join(', ')}\n\n`;
    
    prompt += `Guidelines:\n`;
    prompt += `- Keep responses concise and natural for voice conversation\n`;
    prompt += `- Use appropriate emotion and tone based on context\n`;
    prompt += `- Avoid long monologues unless specifically asked\n`;
    prompt += `- Be interactive and engaging\n`;
    prompt += `- Respond in ${config.language || 'English'}\n`;
    
    if (config.customInstructions) {
      prompt += `\nAdditional instructions: ${config.customInstructions}\n`;
    }
    
    return prompt;
  }
}
```

### 2. Voice Agent Instance
```typescript
// Voice Agent Instance
export class VoiceAgentInstance {
  private voiceConnection?: VoiceConnection;
  private conversationHistory: ConversationMessage[] = [];
  private audioQueue: AudioQueueItem[] = [];
  private isProcessing = false;
  private isSpeaking = false;
  private currentSpeech?: SpeechSession;
  private speechRecognition?: SpeechRecognitionSession;
  private lastActivityTime = Date.now();
  
  constructor(
    private agent: VoiceAgent,
    private elevenLabs: ElevenLabsClient,
    private openai: OpenAIClient,
    private voiceService: VoiceChannelService
  ) {}
  
  async joinChannel(channelId: string): Promise<void> {
    // Connect to voice channel
    this.voiceConnection = await this.voiceService.connectBot({
      channelId,
      userId: `agent_${this.agent.id}`,
      username: this.agent.name,
      isBot: true
    });
    
    // Setup audio streams
    this.setupAudioPipeline();
    
    // Start listening for speech
    this.startSpeechRecognition();
  }
  
  async leaveChannel(): Promise<void> {
    // Stop all audio
    this.stopSpeaking();
    
    // Disconnect
    if (this.voiceConnection) {
      await this.voiceConnection.disconnect();
      this.voiceConnection = undefined;
    }
    
    // Clean up
    this.speechRecognition?.stop();
    this.conversationHistory = [];
    this.audioQueue = [];
  }
  
  private setupAudioPipeline(): void {
    if (!this.voiceConnection) return;
    
    // Setup incoming audio processing
    this.voiceConnection.on('userSpeaking', async (userId, audio) => {
      // Process incoming speech
      await this.processIncomingSpeech(userId, audio);
    });
    
    // Setup outgoing audio
    this.voiceConnection.on('ready', () => {
      console.log(`Agent ${this.agent.name} connected to voice channel`);
    });
  }
  
  private async processIncomingSpeech(
    userId: string,
    audioData: ArrayBuffer
  ): Promise<void> {
    // Skip if agent is speaking
    if (this.isSpeaking) {
      // Check interruption threshold
      const audioLevel = this.getAudioLevel(audioData);
      if (audioLevel > this.agent.interruptionThreshold) {
        this.handleInterruption();
      }
      return;
    }
    
    // Skip if processing
    if (this.isProcessing) return;
    
    this.isProcessing = true;
    this.lastActivityTime = Date.now();
    
    try {
      // Convert audio to text
      const transcription = await this.transcribeAudio(audioData);
      if (!transcription || transcription.trim().length === 0) {
        return;
      }
      
      // Add to conversation history
      this.conversationHistory.push({
        role: 'user',
        content: transcription,
        userId,
        timestamp: Date.now()
      });
      
      // Wait for response delay
      await new Promise(resolve => 
        setTimeout(resolve, this.agent.responseDelay)
      );
      
      // Generate response
      const response = await this.generateResponse(transcription);
      
      // Add to conversation history
      this.conversationHistory.push({
        role: 'assistant',
        content: response,
        timestamp: Date.now()
      });
      
      // Convert to speech and play
      await this.speak(response);
      
      // Update usage
      await this.updateUsage({
        tokensUsed: this.estimateTokens(transcription + response),
        charactersGenerated: response.length
      });
      
    } catch (error) {
      console.error('Error processing speech:', error);
    } finally {
      this.isProcessing = false;
    }
  }
  
  private async generateResponse(input: string): Promise<string> {
    // Build messages for GPT
    const messages = [
      {
        role: 'system' as const,
        content: this.agent.systemPrompt
      },
      ...this.conversationHistory.slice(-10) // Last 10 messages for context
    ];
    
    // Generate response
    const completion = await this.openai.createChatCompletion({
      model: 'gpt-4-turbo-preview',
      messages,
      max_tokens: this.agent.maxTokensPerResponse,
      temperature: 0.8,
      presence_penalty: 0.6,
      frequency_penalty: 0.3
    });
    
    return completion.choices[0].message.content || '';
  }
  
  async speak(text: string): Promise<void> {
    this.isSpeaking = true;
    
    try {
      // Generate speech with ElevenLabs
      const audioStream = await this.elevenLabs.textToSpeechStream({
        text,
        voice_id: this.agent.voice.voiceId,
        model_id: 'eleven_multilingual_v2',
        voice_settings: this.agent.voice.settings,
        optimize_streaming_latency: 3
      });
      
      // Create speech session
      this.currentSpeech = {
        text,
        startTime: Date.now(),
        audioStream
      };
      
      // Stream audio to voice channel
      await this.streamAudioToChannel(audioStream);
      
    } catch (error) {
      console.error('Error generating speech:', error);
    } finally {
      this.isSpeaking = false;
      this.currentSpeech = undefined;
    }
  }
  
  private async streamAudioToChannel(
    audioStream: ReadableStream
  ): Promise<void> {
    if (!this.voiceConnection) return;
    
    const reader = audioStream.getReader();
    const audioChunks: Uint8Array[] = [];
    
    try {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;
        
        audioChunks.push(value);
        
        // Send audio chunk to voice channel
        if (this.voiceConnection.ready) {
          await this.voiceConnection.sendAudioChunk(value);
        }
      }
    } finally {
      reader.releaseLock();
    }
  }
  
  private handleInterruption(): void {
    console.log('Agent interrupted');
    
    // Stop current speech
    this.stopSpeaking();
    
    // Clear audio queue
    this.audioQueue = [];
    
    // Add interruption marker to history
    this.conversationHistory.push({
      role: 'system',
      content: '[INTERRUPTED]',
      timestamp: Date.now()
    });
  }
  
  private stopSpeaking(): void {
    if (this.currentSpeech) {
      // Cancel audio stream
      this.currentSpeech.audioStream.cancel();
      this.currentSpeech = undefined;
    }
    
    this.isSpeaking = false;
  }
  
  private async transcribeAudio(
    audioData: ArrayBuffer
  ): Promise<string | null> {
    try {
      // Use speech recognition service
      const result = await this.speechRecognition.recognize(audioData, {
        language: this.agent.language
      });
      
      return result.text;
    } catch (error) {
      console.error('Transcription error:', error);
      return null;
    }
  }
  
  private getAudioLevel(audioData: ArrayBuffer): number {
    const samples = new Float32Array(audioData);
    let sum = 0;
    
    for (let i = 0; i < samples.length; i++) {
      sum += Math.abs(samples[i]);
    }
    
    return sum / samples.length;
  }
  
  private estimateTokens(text: string): number {
    // Rough estimation: 1 token â‰ˆ 4 characters
    return Math.ceil(text.length / 4);
  }
}
```

### 3. ElevenLabs Integration
```typescript
// ElevenLabs Client
export class ElevenLabsClient {
  private apiKey: string;
  private baseUrl = 'https://api.elevenlabs.io/v1';
  
  constructor(apiKey: string) {
    this.apiKey = apiKey;
  }
  
  async textToSpeechStream(params: {
    text: string;
    voice_id: string;
    model_id?: string;
    voice_settings?: VoiceSettings;
    optimize_streaming_latency?: number;
  }): Promise<ReadableStream> {
    const response = await fetch(
      `${this.baseUrl}/text-to-speech/${params.voice_id}/stream`,
      {
        method: 'POST',
        headers: {
          'xi-api-key': this.apiKey,
          'Content-Type': 'application/json',
          'Accept': 'audio/mpeg'
        },
        body: JSON.stringify({
          text: params.text,
          model_id: params.model_id || 'eleven_monolingual_v1',
          voice_settings: params.voice_settings || {
            stability: 0.75,
            similarity_boost: 0.75
          },
          optimize_streaming_latency: params.optimize_streaming_latency || 0
        })
      }
    );
    
    if (!response.ok) {
      throw new Error(`ElevenLabs API error: ${response.statusText}`);
    }
    
    if (!response.body) {
      throw new Error('No response body');
    }
    
    // Transform audio stream for real-time processing
    return response.body.pipeThrough(new TransformStream({
      transform: async (chunk, controller) => {
        // Process audio chunk if needed
        controller.enqueue(chunk);
      }
    }));
  }
  
  async getVoices(): Promise<Voice[]> {
    const response = await fetch(`${this.baseUrl}/voices`, {
      headers: {
        'xi-api-key': this.apiKey
      }
    });
    
    const data = await response.json();
    return data.voices;
  }
  
  async getVoice(voiceId: string): Promise<Voice | null> {
    try {
      const response = await fetch(`${this.baseUrl}/voices/${voiceId}`, {
        headers: {
          'xi-api-key': this.apiKey
        }
      });
      
      if (!response.ok) return null;
      
      return await response.json();
    } catch {
      return null;
    }
  }
  
  async createVoice(params: {
    name: string;
    files: File[];
    description?: string;
    labels?: Record<string, string>;
  }): Promise<Voice> {
    const formData = new FormData();
    formData.append('name', params.name);
    
    if (params.description) {
      formData.append('description', params.description);
    }
    
    if (params.labels) {
      formData.append('labels', JSON.stringify(params.labels));
    }
    
    for (const file of params.files) {
      formData.append('files', file);
    }
    
    const response = await fetch(`${this.baseUrl}/voices/add`, {
      method: 'POST',
      headers: {
        'xi-api-key': this.apiKey
      },
      body: formData
    });
    
    if (!response.ok) {
      throw new Error(`Failed to create voice: ${response.statusText}`);
    }
    
    return await response.json();
  }
  
  async getUsage(): Promise<UsageInfo> {
    const response = await fetch(`${this.baseUrl}/user/subscription`, {
      headers: {
        'xi-api-key': this.apiKey
      }
    });
    
    return await response.json();
  }
}

// Voice Settings Optimizer
export class VoiceSettingsOptimizer {
  optimizeForPersonality(
    personality: AgentPersonality
  ): VoiceSettings {
    let stability = 0.75;
    let similarityBoost = 0.75;
    let style = 0.5;
    
    // Adjust based on personality traits
    switch (personality.speakingStyle) {
      case 'casual':
        stability -= 0.1;
        style += 0.2;
        break;
      case 'formal':
        stability += 0.1;
        style -= 0.1;
        break;
      case 'friendly':
        similarityBoost += 0.1;
        style += 0.1;
        break;
    }
    
    // Adjust for emotional range
    stability -= (personality.emotionalRange / 10) * 0.2;
    
    // Ensure values are in valid range
    stability = Math.max(0, Math.min(1, stability));
    similarityBoost = Math.max(0, Math.min(1, similarityBoost));
    style = Math.max(0, Math.min(1, style));
    
    return {
      stability,
      similarity_boost: similarityBoost,
      style,
      use_speaker_boost: true
    };
  }
}
```

### 4. Agent Management UI
```tsx
// Voice Agent Manager Component
export function VoiceAgentManager({ waddleId }: { waddleId: string }) {
  const [agents, setAgents] = useState<VoiceAgent[]>([]);
  const [showCreateModal, setShowCreateModal] = useState(false);
  const [loading, setLoading] = useState(true);
  
  useEffect(() => {
    loadAgents();
  }, [waddleId]);
  
  const loadAgents = async () => {
    try {
      const data = await api.getVoiceAgents(waddleId);
      setAgents(data);
    } finally {
      setLoading(false);
    }
  };
  
  const createAgent = async (config: CreateAgentConfig) => {
    try {
      const agent = await api.createVoiceAgent(config);
      setAgents(prev => [...prev, agent]);
      setShowCreateModal(false);
      toast.success('Voice agent created successfully');
    } catch (error) {
      toast.error('Failed to create voice agent');
    }
  };
  
  const toggleAgent = async (agent: VoiceAgent, channelId?: string) => {
    try {
      if (agent.status === 'active') {
        await api.deactivateVoiceAgent(agent.id);
        setAgents(prev => prev.map(a => 
          a.id === agent.id ? { ...a, status: 'inactive' } : a
        ));
      } else if (channelId) {
        await api.activateVoiceAgent(agent.id, channelId);
        setAgents(prev => prev.map(a => 
          a.id === agent.id ? { ...a, status: 'active' } : a
        ));
      }
    } catch (error) {
      toast.error('Failed to toggle agent status');
    }
  };
  
  return (
    <div className="voice-agent-manager">
      <div className="manager-header">
        <h3>AI Voice Agents</h3>
        <Button
          onClick={() => setShowCreateModal(true)}
          icon={<PlusIcon />}
        >
          Create Agent
        </Button>
      </div>
      
      {loading ? (
        <LoadingSpinner />
      ) : agents.length === 0 ? (
        <EmptyState
          message="No voice agents yet"
          action={
            <Button onClick={() => setShowCreateModal(true)}>
              Create your first agent
            </Button>
          }
        />
      ) : (
        <div className="agents-grid">
          {agents.map(agent => (
            <VoiceAgentCard
              key={agent.id}
              agent={agent}
              onToggle={(channelId) => toggleAgent(agent, channelId)}
              onEdit={() => {/* TODO */}}
              onDelete={() => {/* TODO */}}
            />
          ))}
        </div>
      )}
      
      {showCreateModal && (
        <CreateAgentModal
          waddleId={waddleId}
          onClose={() => setShowCreateModal(false)}
          onCreate={createAgent}
        />
      )}
    </div>
  );
}

// Create Agent Modal
export function CreateAgentModal({ 
  waddleId,
  onClose,
  onCreate
}: CreateAgentModalProps) {
  const [step, setStep] = useState(1);
  const [config, setConfig] = useState<Partial<CreateAgentConfig>>({
    waddleId,
    capabilities: [AgentCapability.CONVERSATION],
    personality: {
      name: '',
      description: '',
      traits: [],
      speakingStyle: 'friendly',
      humorLevel: 5,
      verbosity: 5,
      emotionalRange: 5
    }
  });
  const [voices, setVoices] = useState<Voice[]>([]);
  const [selectedVoice, setSelectedVoice] = useState<Voice>();
  const [previewPlaying, setPreviewPlaying] = useState(false);
  
  useEffect(() => {
    loadVoices();
  }, []);
  
  const loadVoices = async () => {
    const data = await api.getAvailableVoices();
    setVoices(data);
  };
  
  const playVoicePreview = async (voice: Voice) => {
    if (previewPlaying) return;
    
    setPreviewPlaying(true);
    try {
      const audio = new Audio(voice.preview_url);
      audio.addEventListener('ended', () => setPreviewPlaying(false));
      await audio.play();
    } catch (error) {
      setPreviewPlaying(false);
    }
  };
  
  const handleCreate = async () => {
    if (!config.name || !selectedVoice) return;
    
    await onCreate({
      ...config as CreateAgentConfig,
      voiceId: selectedVoice.voice_id
    });
  };
  
  return (
    <Modal onClose={onClose} size="large">
      <div className="create-agent-modal">
        <h2>Create AI Voice Agent</h2>
        
        <ProgressSteps
          steps={[
            'Basic Info',
            'Personality',
            'Voice Selection',
            'Capabilities',
            'Review'
          ]}
          currentStep={step}
        />
        
        {step === 1 && (
          <div className="step-content">
            <FormField label="Agent Name">
              <Input
                value={config.name || ''}
                onChange={(e) => setConfig(prev => ({
                  ...prev,
                  name: e.target.value
                }))}
                placeholder="e.g., Waddle Assistant"
              />
            </FormField>
            
            <FormField label="Language">
              <Select
                value={config.language || 'en'}
                onChange={(value) => setConfig(prev => ({
                  ...prev,
                  language: value
                }))}
              >
                <option value="en">English</option>
                <option value="es">Spanish</option>
                <option value="fr">French</option>
                <option value="de">German</option>
                <option value="ja">Japanese</option>
                <option value="ko">Korean</option>
                <option value="zh">Chinese</option>
              </Select>
            </FormField>
            
            <FormField label="Welcome Message (optional)">
              <TextArea
                value={config.welcomeMessage || ''}
                onChange={(e) => setConfig(prev => ({
                  ...prev,
                  welcomeMessage: e.target.value
                }))}
                placeholder="e.g., Hello! I'm here to help. How can I assist you today?"
              />
            </FormField>
          </div>
        )}
        
        {step === 2 && (
          <PersonalityConfigurator
            personality={config.personality!}
            onChange={(personality) => setConfig(prev => ({
              ...prev,
              personality
            }))}
          />
        )}
        
        {step === 3 && (
          <VoiceSelector
            voices={voices}
            selectedVoice={selectedVoice}
            onSelect={setSelectedVoice}
            onPreview={playVoicePreview}
            previewPlaying={previewPlaying}
          />
        )}
        
        {step === 4 && (
          <CapabilitiesSelector
            selected={config.capabilities!}
            onChange={(capabilities) => setConfig(prev => ({
              ...prev,
              capabilities
            }))}
          />
        )}
        
        {step === 5 && (
          <AgentReview
            config={config as CreateAgentConfig}
            voice={selectedVoice!}
          />
        )}
        
        <div className="modal-actions">
          {step > 1 && (
            <Button
              variant="secondary"
              onClick={() => setStep(step - 1)}
            >
              Previous
            </Button>
          )}
          
          {step < 5 ? (
            <Button
              variant="primary"
              onClick={() => setStep(step + 1)}
              disabled={!isStepValid(step, config, selectedVoice)}
            >
              Next
            </Button>
          ) : (
            <Button
              variant="primary"
              onClick={handleCreate}
            >
              Create Agent
            </Button>
          )}
        </div>
      </div>
    </Modal>
  );
}

// Voice Agent Card
export function VoiceAgentCard({ 
  agent, 
  onToggle, 
  onEdit, 
  onDelete 
}: VoiceAgentCardProps) {
  const [channels, setChannels] = useState<VoiceChannel[]>([]);
  const [selectedChannel, setSelectedChannel] = useState<string>();
  
  useEffect(() => {
    loadChannels();
  }, []);
  
  const loadChannels = async () => {
    const data = await api.getVoiceChannels(agent.waddleId);
    setChannels(data);
  };
  
  return (
    <div className={`agent-card ${agent.status}`}>
      <div className="agent-header">
        <h4>{agent.name}</h4>
        <StatusBadge status={agent.status} />
      </div>
      
      <div className="agent-info">
        <p className="personality">{agent.personality.description}</p>
        <div className="voice-info">
          <VoiceIcon />
          <span>{agent.voice.name}</span>
        </div>
      </div>
      
      <div className="agent-capabilities">
        {agent.capabilities.map(cap => (
          <Badge key={cap}>{cap}</Badge>
        ))}
      </div>
      
      <div className="agent-usage">
        <div className="usage-stat">
          <span className="label">Conversations</span>
          <span className="value">{agent.usage.conversationCount}</span>
        </div>
        <div className="usage-stat">
          <span className="label">Characters</span>
          <span className="value">{formatNumber(agent.usage.charactersGenerated)}</span>
        </div>
      </div>
      
      <div className="agent-actions">
        {agent.status === 'inactive' ? (
          <>
            <Select
              value={selectedChannel || ''}
              onChange={setSelectedChannel}
              placeholder="Select channel"
            >
              {channels.map(channel => (
                <option key={channel.id} value={channel.id}>
                  {channel.name}
                </option>
              ))}
            </Select>
            <Button
              variant="primary"
              size="small"
              onClick={() => onToggle(selectedChannel)}
              disabled={!selectedChannel}
            >
              Activate
            </Button>
          </>
        ) : (
          <Button
            variant="secondary"
            size="small"
            onClick={() => onToggle()}
          >
            Deactivate
          </Button>
        )}
        
        <Button
          variant="text"
          size="small"
          onClick={onEdit}
          icon={<EditIcon />}
        />
        
        <Button
          variant="text"
          size="small"
          onClick={onDelete}
          icon={<DeleteIcon />}
        />
      </div>
    </div>
  );
}
```

### 5. Real-time Voice Processing
```typescript
// Real-time Voice Processor
export class RealtimeVoiceProcessor {
  private processingPipeline: AudioWorkletNode;
  private voiceActivityDetector: VoiceActivityDetector;
  private noiseGate: NoiseGate;
  
  constructor(
    private audioContext: AudioContext
  ) {
    this.setupProcessingPipeline();
  }
  
  private async setupProcessingPipeline(): Promise<void> {
    // Load audio worklet
    await this.audioContext.audioWorklet.addModule('/audio-processor.js');
    
    // Create processing nodes
    this.processingPipeline = new AudioWorkletNode(
      this.audioContext,
      'voice-processor'
    );
    
    // Setup VAD
    this.voiceActivityDetector = new VoiceActivityDetector({
      threshold: 0.05,
      smoothingTimeConstant: 0.95,
      minSilenceDuration: 300
    });
    
    // Setup noise gate
    this.noiseGate = new NoiseGate({
      threshold: -40,
      ratio: 10,
      attack: 0.001,
      release: 0.1
    });
  }
  
  processIncomingAudio(
    audioStream: MediaStream
  ): ProcessedAudioStream {
    const source = this.audioContext.createMediaStreamSource(audioStream);
    
    // Apply noise gate
    source.connect(this.noiseGate.node);
    
    // Apply processing
    this.noiseGate.node.connect(this.processingPipeline);
    
    // Detect voice activity
    this.processingPipeline.port.onmessage = (event) => {
      if (event.data.type === 'audio-level') {
        const isVoiceActive = this.voiceActivityDetector.process(
          event.data.level
        );
        
        if (isVoiceActive) {
          this.onVoiceActivityDetected();
        }
      }
    };
    
    return {
      stream: this.processingPipeline,
      getAudioLevel: () => this.voiceActivityDetector.currentLevel,
      isVoiceActive: () => this.voiceActivityDetector.isActive
    };
  }
  
  private onVoiceActivityDetected(): void {
    // Emit voice activity event
    this.emit('voiceActivity', {
      timestamp: Date.now(),
      level: this.voiceActivityDetector.currentLevel
    });
  }
}

// Audio Worklet Processor (audio-processor.js)
class VoiceProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
    this.smoothingFactor = 0.95;
    this.lastLevel = 0;
  }
  
  process(inputs, outputs, parameters) {
    const input = inputs[0];
    const output = outputs[0];
    
    if (input.length > 0) {
      const samples = input[0];
      
      // Calculate RMS level
      let sum = 0;
      for (let i = 0; i < samples.length; i++) {
        sum += samples[i] * samples[i];
      }
      const rms = Math.sqrt(sum / samples.length);
      
      // Smooth the level
      this.lastLevel = (this.lastLevel * this.smoothingFactor) + 
                      (rms * (1 - this.smoothingFactor));
      
      // Send level to main thread
      if (currentFrame % 128 === 0) {
        this.port.postMessage({
          type: 'audio-level',
          level: this.lastLevel
        });
      }
      
      // Pass through audio
      for (let channel = 0; channel < output.length; channel++) {
        output[channel].set(input[channel]);
      }
    }
    
    return true;
  }
}

registerProcessor('voice-processor', VoiceProcessor);
```

### 6. Usage Analytics
```typescript
// Voice Agent Analytics
export class VoiceAgentAnalytics {
  constructor(
    private db: D1Database,
    private analytics: AnalyticsService
  ) {}
  
  async trackConversation(event: ConversationEvent): Promise<void> {
    await this.analytics.track('agent_conversation', {
      agentId: event.agentId,
      waddleId: event.waddleId,
      channelId: event.channelId,
      duration: event.duration,
      messageCount: event.messageCount,
      tokensUsed: event.tokensUsed,
      charactersGenerated: event.charactersGenerated,
      language: event.language,
      capabilities: event.capabilities,
      userSatisfaction: event.userSatisfaction
    });
    
    // Update agent usage
    await this.updateAgentUsage(event.agentId, {
      conversationCount: 1,
      tokensUsed: event.tokensUsed,
      charactersGenerated: event.charactersGenerated
    });
  }
  
  async getAgentMetrics(
    agentId: string,
    timeRange: TimeRange
  ): Promise<AgentMetrics> {
    const conversations = await this.analytics.query({
      metric: 'agent_conversation',
      filters: { agentId },
      range: timeRange
    });
    
    return {
      totalConversations: conversations.length,
      totalDuration: conversations.reduce((sum, c) => sum + c.duration, 0),
      averageDuration: this.calculateAverage(conversations, 'duration'),
      totalTokensUsed: conversations.reduce((sum, c) => sum + c.tokensUsed, 0),
      totalCharactersGenerated: conversations.reduce((sum, c) => 
        sum + c.charactersGenerated, 0
      ),
      userSatisfaction: this.calculateAverage(conversations, 'userSatisfaction'),
      hourlyDistribution: this.getHourlyDistribution(conversations),
      topCapabilities: this.getTopCapabilities(conversations)
    };
  }
  
  async getUsageCosts(
    waddleId: string,
    period: 'daily' | 'weekly' | 'monthly'
  ): Promise<UsageCosts> {
    const agents = await this.getAgentsByWaddle(waddleId);
    const costs = {
      elevenLabs: 0,
      openai: 0,
      total: 0
    };
    
    for (const agent of agents) {
      // ElevenLabs: $0.30 per 1000 characters
      costs.elevenLabs += (agent.usage.charactersGenerated / 1000) * 0.30;
      
      // OpenAI: $0.03 per 1K tokens (GPT-4)
      costs.openai += (agent.usage.tokensUsed / 1000) * 0.03;
    }
    
    costs.total = costs.elevenLabs + costs.openai;
    
    return costs;
  }
}

// Usage Dashboard Component
export function VoiceAgentUsageDashboard({ waddleId }: { waddleId: string }) {
  const [metrics, setMetrics] = useState<AgentMetrics>();
  const [costs, setCosts] = useState<UsageCosts>();
  const [timeRange, setTimeRange] = useState<'day' | 'week' | 'month'>('week');
  const [loading, setLoading] = useState(true);
  
  useEffect(() => {
    loadData();
  }, [waddleId, timeRange]);
  
  const loadData = async () => {
    setLoading(true);
    try {
      const [metricsData, costsData] = await Promise.all([
        api.getAgentMetrics(waddleId, timeRange),
        api.getAgentUsageCosts(waddleId, timeRange)
      ]);
      
      setMetrics(metricsData);
      setCosts(costsData);
    } finally {
      setLoading(false);
    }
  };
  
  if (loading) return <LoadingSpinner />;
  if (!metrics || !costs) return null;
  
  return (
    <div className="usage-dashboard">
      <div className="dashboard-header">
        <h3>Voice Agent Usage</h3>
        <TimeRangeSelector
          value={timeRange}
          onChange={setTimeRange}
        />
      </div>
      
      <div className="metrics-grid">
        <MetricCard
          title="Total Conversations"
          value={metrics.totalConversations}
          trend={metrics.conversationsTrend}
        />
        
        <MetricCard
          title="Average Duration"
          value={formatDuration(metrics.averageDuration)}
          subtitle="per conversation"
        />
        
        <MetricCard
          title="User Satisfaction"
          value={`${Math.round(metrics.userSatisfaction * 100)}%`}
          trend={metrics.satisfactionTrend}
        />
        
        <MetricCard
          title="Estimated Cost"
          value={`$${costs.total.toFixed(2)}`}
          breakdown={[
            { label: 'Voice', value: `$${costs.elevenLabs.toFixed(2)}` },
            { label: 'AI', value: `$${costs.openai.toFixed(2)}` }
          ]}
        />
      </div>
      
      <div className="charts-grid">
        <HourlyActivityChart data={metrics.hourlyDistribution} />
        <CapabilitiesChart data={metrics.topCapabilities} />
        <ConversationLengthDistribution data={metrics.durationDistribution} />
      </div>
    </div>
  );
}
```

## Dependencies
- ElevenLabs API for text-to-speech
- OpenAI GPT-4 for conversational AI
- Web Audio API for real-time processing
- Speech recognition service
- WebRTC for voice channels
- D1 for agent storage

## Estimated Effort
**6 days**
- 1 day: Agent architecture and core service
- 1 day: ElevenLabs integration and voice synthesis
- 1 day: Real-time voice processing pipeline
- 1 day: Agent personality and conversation system
- 1 day: UI components and management
- 1 day: Analytics and optimization

## Notes
- Consider implementing voice cloning for custom agents
- Add emotion detection and response
- Implement multi-agent conversations
- Consider adding visual avatars for agents
- Plan for conversation memory and context
- Add safety features and content filtering