# Waddle Infrastructure - Environment Configuration
# Copy this file to .env and fill in your values.
# NEVER commit .env to version control!

# =============================================================================
# PROXMOX API CONFIGURATION
# =============================================================================
#
# Proxmox credentials can be provided via multiple methods (in order of precedence):
#
# 1. Terraform variables at apply time:
#    cdktf deploy -- -var="proxmox_endpoint=https://..." -var="proxmox_api_token=..."
#
# 2. terraform.tfvars file in cdktf.out/stacks/waddle-infra/:
#    proxmox_endpoint = "https://proxmox.example.com:8006"
#    proxmox_api_token = "root@pam!terraform=xxx"
#
# 3. TF_VAR_ environment variables (recognized by Terraform):
#    export TF_VAR_proxmox_endpoint="https://proxmox.example.com:8006"
#    export TF_VAR_proxmox_api_token="root@pam!terraform=xxx"
#
# 4. Node.js environment variables below (used as defaults for TF variables):
#
# =============================================================================

# Proxmox API endpoint URL (used as default for TF var proxmox_endpoint)
# Example: https://proxmox.example.com:8006 or https://192.168.1.100:8006
PROXMOX_VE_ENDPOINT=https://proxmox.example.com:8006

# Proxmox API token for authentication (used as default for TF var proxmox_api_token)
# Format: user@realm!tokenid=secret
#
# To create an API token:
# 1. Log into Proxmox web UI
# 2. Go to: Datacenter → Permissions → API Tokens → Add
# 3. Select a user (e.g., root@pam) and create a token
# 4. Copy the token ID and secret (shown only once!)
# 5. Combine them as: user@realm!tokenid=secret
#
# Example: root@pam!terraform=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
PROXMOX_VE_API_TOKEN=root@pam!terraform=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx

# =============================================================================
# OPTIONAL: Proxmox SSH Configuration
# =============================================================================

# SSH credentials are required for:
# - Uploading cloud-init snippets to Proxmox storage
# - Managing files on the Proxmox host
# - Using file-based provisioning features

# SSH username (usually 'root' for Proxmox)
PROXMOX_VE_SSH_USERNAME=root

# SSH password (consider using SSH keys or agent instead)
PROXMOX_VE_SSH_PASSWORD=

# Use SSH agent for authentication (recommended over password)
# Set to 'true' to use SSH agent
PROXMOX_VE_SSH_AGENT=false

# =============================================================================
# OPTIONAL: TLS Configuration
# =============================================================================

# Skip TLS certificate verification
# WARNING: Only use for testing with self-signed certificates
# Set to 'true' to skip verification (NOT recommended for production)
PROXMOX_VE_INSECURE=false

# =============================================================================
# DEPLOYMENT CONFIGURATION
# =============================================================================

# Deployment environment (used for tagging resources)
# Values: development, staging, production
ENVIRONMENT=development

# =============================================================================
# SECURITY BEST PRACTICES
# =============================================================================
#
# 1. NEVER commit this file to version control
#    - Add .env to .gitignore (already done)
#    - Use CI/CD secrets for automation
#
# 2. Use API tokens instead of passwords
#    - API tokens can be revoked independently
#    - Tokens can have limited permissions (Privilege Separation)
#
# 3. For production:
#    - Use HashiCorp Vault or similar secrets management
#    - Configure Proxmox API token with minimal required permissions
#    - Enable TLS verification (PROXMOX_VE_INSECURE=false)
#    - Use SSH keys instead of passwords
#
# 4. Token permissions for Talos VM provisioning:
#    - VM.Allocate, VM.Clone, VM.Config.*, VM.Audit
#    - Datastore.AllocateSpace, Datastore.Audit
#    - SDN.Use (if using SDN)
#    - Sys.Modify (for uploading ISO/snippets)
#
# 5. Kubeconfig and Talosconfig (Phase 4):
#    - Never commit kubeconfig or talosconfig to version control
#    - Store in ~/.kube/config or use KUBECONFIG environment variable
#    - Rotate credentials regularly
#    - Use RBAC to limit access

# =============================================================================
# TALOS VM PROVISIONING CONFIGURATION
# =============================================================================
#
# These variables configure Talos VM provisioning on Proxmox.
# VM provisioning is triggered when PROXMOX_NODE_NAME is set.
# If not set, only provider configuration will be deployed (no VMs).
#
# =============================================================================

# Target Proxmox node name where VMs will be created
# Find this in Proxmox UI: Datacenter → [node name]
# Common values: 'pve', 'proxmox01', 'node1'
PROXMOX_NODE_NAME=pve

# Storage ID for VM disks
# Use 'pvesm status' or Proxmox UI to list available storage
# This is where VM disks are stored (may be Ceph pool, local-lvm, local-zfs, etc.)
# Common values: 'local-lvm', 'local-zfs', 'ceph-pool'
PROXMOX_STORAGE_ID=local-lvm

# Storage ID for ISO/image content (must support 'iso' content type)
# Default: 'local' - typically the local storage on Proxmox
# NOTE: This is separate from PROXMOX_STORAGE_ID because Ceph pools and LVM
# storage typically don't support ISO content, while 'local' always does.
# Use 'pvesm status' to verify which storage supports 'iso' content.
PROXMOX_IMAGE_STORAGE_ID=local

# Network bridge for VM network interfaces
# Default Proxmox installation uses 'vmbr0'
# Check 'ip link' or Network section in Proxmox UI
PROXMOX_NETWORK_BRIDGE=vmbr0

# =============================================================================
# TALOS CLUSTER CONFIGURATION
# =============================================================================

# Talos Linux version to deploy
# Check releases at: https://github.com/siderolabs/talos/releases
# Use format: v1.x.x (include the 'v' prefix)
TALOS_VERSION=v1.11.5

# Kubernetes cluster name (used for resource naming and identification)
# Lowercase alphanumeric with hyphens, no underscores
TALOS_CLUSTER_NAME=waddle-cluster

# Kubernetes API endpoint (VIP or load balancer)
# This is the address clients will use to connect to the Kubernetes API
# For single-node: use the first control plane IP
# For HA: use a VIP managed by keepalived or external load balancer
# Format: https://<ip-or-hostname>:6443
TALOS_CLUSTER_ENDPOINT=https://192.168.1.100:6443

# =============================================================================
# NODE NETWORK CONFIGURATION
# =============================================================================
#
# Two network modes are supported:
#
# MODE 1: STATIC IP (recommended for production)
# ----------------------------------------------
# Set TALOS_NODE_IP_PREFIX and TALOS_NODE_GATEWAY to enable static IPs.
# Nodes receive sequential IPs starting from TALOS_NODE_IP_START.
#
# Example with ipPrefix=192.168.1, ipStart=101, and 3 control planes + 2 workers:
#   - Control plane 0: 192.168.1.101
#   - Control plane 1: 192.168.1.102
#   - Control plane 2: 192.168.1.103
#   - Worker 0:        192.168.1.104
#   - Worker 1:        192.168.1.105
#
# IMPORTANT: Only /24 networks are currently supported. The IP prefix must be
# exactly three octets (e.g., '192.168.1'), and addresses are assigned by
# incrementing the last octet.
#
# MODE 2: DHCP
# ----------------------------------------------
# Leave TALOS_NODE_IP_PREFIX and TALOS_NODE_GATEWAY empty (or unset) to use DHCP.
# VMs will obtain IP addresses from your DHCP server.
#
# With DHCP mode, you should:
# 1. Configure DHCP reservations based on VM MAC addresses (visible in Proxmox)
# 2. Ensure TALOS_CLUSTER_ENDPOINT points to a stable IP (VIP or first node IP)
#
# =============================================================================

# IP address prefix for nodes (first 3 octets for /24 networks)
# Format: 'X.X.X' - exactly three octets (e.g., '192.168.1')
# Leave empty for DHCP mode
TALOS_NODE_IP_PREFIX=192.168.1

# Starting IP for node allocation (last octet)
# Example: '101' results in first node at 192.168.1.101
# Worker nodes (if any) continue sequentially after control planes
TALOS_NODE_IP_START=101

# Network gateway IP address
# Usually your router IP, e.g., 192.168.1.1
# Leave empty for DHCP mode (must match TALOS_NODE_IP_PREFIX setting)
TALOS_NODE_GATEWAY=192.168.1.1

# Network CIDR suffix (number of network bits)
# Only used in static IP mode
# Common values: '24' for /24 (255.255.255.0)
TALOS_NODE_NETMASK=24

# =============================================================================
# CONTROL PLANE CONFIGURATION
# =============================================================================
#
# Control plane nodes run Kubernetes control plane components (API server,
# etcd, controller-manager, scheduler). For HA, use 3 or 5 nodes.
#
# Resource recommendations:
#   - Minimum: 2 cores, 4GB RAM, 50GB disk
#   - Recommended: 4 cores, 8GB RAM, 50GB disk
#   - Production: 4+ cores, 16GB RAM, 100GB disk
#
# =============================================================================

# Number of control plane nodes (use 1, 3, or 5 for etcd quorum)
# 3 nodes provides HA with single-node failure tolerance
TALOS_CONTROL_PLANE_COUNT=3

# CPU cores per control plane node
TALOS_CONTROL_PLANE_CORES=4

# Memory in MB per control plane node (8192 = 8GB)
TALOS_CONTROL_PLANE_MEMORY=8192

# Disk size in GB per control plane node
TALOS_CONTROL_PLANE_DISK_SIZE=50

# =============================================================================
# WORKER NODE CONFIGURATION (OPTIONAL)
# =============================================================================
#
# Worker nodes run application workloads. They are optional - control plane
# nodes can also run workloads if no workers are configured.
#
# Supported topologies:
# - 3 control planes, 0 workers (default) - HA cluster, workloads on CP nodes
# - 1 control plane, 2+ workers - Small cluster with dedicated workers
# - 3 control planes, N workers - Production HA with dedicated workers
#
# Resource recommendations for workers:
#   - Minimum: 2 cores, 4GB RAM, 50GB disk
#   - Recommended: 4 cores, 8GB RAM, 100GB disk
#   - Production: Based on workload requirements
#
# =============================================================================

# Number of worker nodes (0 = no dedicated workers)
TALOS_WORKER_COUNT=0

# CPU cores per worker node
TALOS_WORKER_CORES=2

# Memory in MB per worker node (4096 = 4GB)
TALOS_WORKER_MEMORY=4096

# Disk size in GB per worker node
TALOS_WORKER_DISK_SIZE=50

# =============================================================================
# KUBERNETES TOPOLOGY LABELS
# =============================================================================
#
# Topology labels enable zone-aware scheduling and storage placement.
# Labels are stored in VM description and applied to nodes in Phase 4.
#
# For single Proxmox host: use a single zone
# For multi-host cluster: assign different zones per physical host
#
# =============================================================================

# Topology region label (topology.kubernetes.io/region)
# Logical grouping of zones (e.g., datacenter, cloud region)
TALOS_TOPOLOGY_REGION=proxmox

# Topology zone label (topology.kubernetes.io/zone)
# Individual failure domain (e.g., host, rack, availability zone)
TALOS_TOPOLOGY_ZONE=zone-1

# =============================================================================
# TALOS CLUSTER BOOTSTRAP CONFIGURATION (PHASE 4)
# =============================================================================
#
# These variables configure Kubernetes cluster bootstrap on Talos nodes.
# Most have sensible defaults and are optional.
#
# IMPORTANT: Phase 4 requires static IP configuration. DHCP mode is currently
# supported only for VM provisioning (Phase 3). To enable cluster bootstrap,
# you MUST configure static IPs by setting TALOS_NODE_IP_PREFIX and
# TALOS_NODE_GATEWAY above. If DHCP mode is detected, Phase 4 will be skipped
# with a warning, and VMs will be provisioned without cluster bootstrap.
# IP discovery for DHCP mode will be implemented in a future release.
#
# =============================================================================

# Kubernetes version to install (optional)
# If not specified, uses the default version for the Talos release
# Format: v1.x.x (include the 'v' prefix)
# Check compatibility: https://www.talos.dev/v1.11/introduction/support-matrix/
# TALOS_KUBERNETES_VERSION=v1.31.0

# Cluster domain for Kubernetes services (default: cluster.local)
TALOS_CLUSTER_DOMAIN=cluster.local

# Pod network CIDR (default: 10.244.0.0/16)
# This is the IP range for pod networking (managed by CNI)
TALOS_CLUSTER_NETWORK=10.244.0.0/16

# Service network CIDR (default: 10.96.0.0/12)
# This is the IP range for Kubernetes services
TALOS_SERVICE_NETWORK=10.96.0.0/12

# CNI (Container Network Interface) configuration
# Set to 'none' to install CNI separately (Cilium in Phase 6)
# Other options: 'flannel' (built-in, not recommended for production)
TALOS_CNI=none

# Disk path for Talos OS installation (default: /dev/sda)
# Common values: /dev/sda, /dev/vda (virtio), /dev/nvme0n1 (NVMe)
# Use 'lsblk' in Talos maintenance mode to identify disks
TALOS_INSTALL_DISK=/dev/sda

# Allow scheduling workloads on control plane nodes (default: auto)
# Auto behavior:
#   - true if no worker nodes (control-plane-only cluster)
#   - false if worker nodes exist (dedicated workers)
# Set explicitly to override auto behavior
# TALOS_ALLOW_SCHEDULING_ON_CONTROL_PLANES=true

# =============================================================================
# TELEPORT SECURE ACCESS CONFIGURATION (PHASE 5)
# =============================================================================
#
# Teleport provides zero-trust access to Proxmox and Kubernetes infrastructure.
# When enabled, a dedicated Teleport VM is provisioned on Proxmox running:
# - Teleport Auth Service (authentication and authorization)
# - Teleport Proxy Service (public-facing gateway)
#
# Teleport protects:
# - Proxmox web UI (port 8006) via Application Access
# - Proxmox SSH (port 22) via SSH Access
# - Kubernetes API (port 6443) via Kubernetes Service (Phase 6+)
# - Talos API (port 50000) via Application Access
#
# Architecture: Single public IP on Teleport Proxy, all other services accessed
# via reverse tunnels (no need to expose Proxmox/K8s ports publicly).
# =============================================================================

# Enable Teleport VM provisioning (set to 'true' to enable)
TELEPORT_ENABLED=false

# Public domain for Teleport (must be a valid FQDN pointing to your public IP)
# This domain is used for:
# - Teleport web UI: https://{domain}
# - Teleport API: {domain}:443
# - SSH proxy: {domain}:3024
# Example: teleport.example.com
TELEPORT_DOMAIN=teleport.example.com

# Email for Let's Encrypt ACME certificate registration
# Used for automatic TLS certificate provisioning
TELEPORT_LETSENCRYPT_EMAIL=admin@example.com

# Static IP address for Teleport VM (required)
# This should be a static IP on your network, ideally the public IP or
# an IP that is port-forwarded from your public IP
TELEPORT_IP_ADDRESS=192.168.1.100

# Network gateway for Teleport VM
TELEPORT_GATEWAY=192.168.1.1

# Network CIDR suffix (default: 24)
TELEPORT_NETMASK=24

# SSH public keys for initial Teleport VM access (comma-separated)
# These keys are added to the VM's authorized_keys for emergency access
# Example: "ssh-ed25519 AAAA..., ssh-rsa AAAA..."
TELEPORT_SSH_KEYS=

# =============================================================================
# OPTIONAL: Teleport VM Configuration
# =============================================================================

# Proxmox node for Teleport VM (defaults to PROXMOX_NODE_NAME)
TELEPORT_NODE_NAME=pve

# Storage for Teleport VM disk (defaults to PROXMOX_STORAGE_ID)
TELEPORT_STORAGE_ID=local-lvm

# Storage for cloud images (defaults to PROXMOX_IMAGE_STORAGE_ID)
TELEPORT_IMAGE_STORAGE_ID=local

# Network bridge for Teleport VM (defaults to PROXMOX_NETWORK_BRIDGE)
TELEPORT_NETWORK_BRIDGE=vmbr0

# Teleport VM name in Proxmox (default: 'teleport')
TELEPORT_VM_NAME=teleport

# CPU cores for Teleport VM (default: 2, minimum: 2)
TELEPORT_CORES=2

# Memory in MB for Teleport VM (default: 4096, minimum: 2048)
TELEPORT_MEMORY=4096

# Disk size in GB for Teleport VM (default: 50, minimum: 20)
TELEPORT_DISK_SIZE=50

# Teleport version (default: 'latest' - uses latest stable release)
# Specify version like 'v17.0.0' to pin a specific version
TELEPORT_VERSION=latest

# =============================================================================
# TELEPORT SETUP NOTES
# =============================================================================
#
# 1. DNS Configuration:
#    - Create an A record pointing TELEPORT_DOMAIN to your public IP
#    - Ensure ports 443 and 3024 are forwarded to TELEPORT_IP_ADDRESS
#
# 2. Firewall Rules (on your router/firewall):
#    - Forward port 443 (TCP) to TELEPORT_IP_ADDRESS:443 (Web UI/Proxy)
#    - Forward port 3024 (TCP) to TELEPORT_IP_ADDRESS:3024 (SSH Proxy)
#    - Port 3025 (reverse tunnel) is outbound-only, no forwarding needed
#
# 3. Initial Setup (after VM deployment):
#    - SSH to Teleport VM: ssh admin@{TELEPORT_IP_ADDRESS}
#    - Follow the manual installation steps in docs/teleport-setup.md
#    - Create first admin user: sudo tctl users add admin --roles=editor,access --logins=root,admin
#    - Access web UI: https://{TELEPORT_DOMAIN}
#
# 4. Proxmox Integration (configured after Teleport setup):
#    - Application Access for Proxmox web UI
#    - SSH Access for Proxmox host
#
# 5. Kubernetes Integration (configured after Phase 6 - Cilium):
#    - Deploy Teleport Kube Agent via Helm
#    - Agent connects to Proxy via reverse tunnel
#
# 6. Security Best Practices:
#    - Use hardware security keys (YubiKey) for MFA
#    - Configure session recording for audit compliance
#    - Set up RBAC roles with least-privilege access
#    - Enable per-session MFA for production access
#    - Regularly review audit logs
#
# =============================================================================

# =============================================================================
# PROXMOX CSI DRIVER CONFIGURATION (PHASE 7)
# =============================================================================
#
# Proxmox CSI driver provides persistent storage for Kubernetes workloads.
# It provisions volumes dynamically from Proxmox storage backends.
#
# Prerequisites:
# - Proxmox user 'kubernetes-csi@pve' with CSI role and API token
# - Proxmox storage backend configured (LVM, ZFS, Ceph, etc.)
#
# Setup instructions: docs/proxmox-csi-setup.md
# =============================================================================

# Proxmox storage ID for CSI volumes (must match a storage backend in Proxmox)
# This is typically the same as PROXMOX_STORAGE_ID used for VM disks
# Use 'pvesm status' on Proxmox to list available storage
# Common values: 'local-lvm', 'local-zfs', 'ceph-pool'
PROXMOX_CSI_STORAGE_ID=local-lvm

# Proxmox API endpoint for CSI driver (usually same as PROXMOX_VE_ENDPOINT)
# Format: https://<proxmox-host>:8006
PROXMOX_CSI_ENDPOINT=https://proxmox.example.com:8006

# Proxmox API token for CSI driver (format: user@realm!tokenid)
# This should be the token created for kubernetes-csi@pve user
# Example: kubernetes-csi@pve!csi
# Note: The token secret is stored separately in Kubernetes Secret
PROXMOX_CSI_TOKEN_ID=kubernetes-csi@pve!csi

# Skip TLS verification for CSI driver (default: false)
# WARNING: Only use for testing with self-signed certificates
PROXMOX_CSI_INSECURE=false

# Topology region for CSI driver (should match TALOS_TOPOLOGY_REGION)
# Used for zone-aware volume provisioning
PROXMOX_CSI_REGION=proxmox

# =============================================================================
# PROXMOX CSI SETUP NOTES
# =============================================================================
#
# 1. Create Proxmox CSI user and role:
#    pveum role add CSI -privs "VM.Audit VM.Config.Disk Datastore.Allocate Datastore.AllocateSpace Datastore.Audit"
#    pveum user add kubernetes-csi@pve
#    pveum aclmod / -user kubernetes-csi@pve -role CSI
#    pveum user token add kubernetes-csi@pve csi -privsep 0
#
# 2. Create Kubernetes Secret with credentials:
#    kubectl create namespace csi-proxmox
#    kubectl create secret generic proxmox-csi-credentials \
#      --from-file=config.yaml=proxmox-csi-config.yaml \
#      -n csi-proxmox
#
# 3. Deploy CSI driver:
#    helm repo add proxmox-csi https://sergelogvinov.github.io/proxmox-csi-plugin
#    helm install proxmox-csi proxmox-csi/proxmox-csi-plugin \
#      --version 0.13.0 \
#      --namespace csi-proxmox \
#      --values infrastructure-k8s/storage/helm-values.yaml
#
# 4. Verify installation:
#    kubectl get pods -n csi-proxmox
#    kubectl get storageclass proxmox-csi
#
# 5. Test volume provisioning:
#    kubectl apply -f infrastructure-k8s/storage/verification/test-pvc.yaml
#
# See docs/proxmox-csi-setup.md for detailed instructions.
#
# =============================================================================

# =============================================================================
# CERT-MANAGER AND TLS CERTIFICATE CONFIGURATION (PHASE 9)
# =============================================================================
#
# cert-manager provides automated TLS certificate management using Let's Encrypt
# ACME protocol with Cloudflare DNS01 challenge solver.
#
# Prerequisites:
# - Cloudflare account with domain(s) to manage
# - Cloudflare API token with Zone:DNS:Edit permissions
# - Domain DNS managed by Cloudflare
#
# Setup instructions: docs/cert-manager-setup.md
# =============================================================================

# Email for Let's Encrypt ACME registration and renewal notifications
# This can reuse TELEPORT_LETSENCRYPT_EMAIL or be set separately
# Let's Encrypt will send expiration warnings to this email
CERT_MANAGER_EMAIL=admin@example.com

# Cloudflare API token for DNS01 challenge solver
# Create at: https://dash.cloudflare.com/profile/api-tokens
# Required permissions: Zone:DNS:Edit for all zones (or specific zone)
# Note: The token is stored in Kubernetes Secret, not here
# This variable is for documentation reference only
# CLOUDFLARE_API_TOKEN=<create-via-kubectl-secret>

# =============================================================================
# CERT-MANAGER SETUP NOTES
# =============================================================================
#
# 1. Create Cloudflare API token:
#    - Go to https://dash.cloudflare.com/profile/api-tokens
#    - Click "Create Token"
#    - Use "Edit zone DNS" template or create custom token
#    - Permissions: Zone:DNS:Edit
#    - Zone Resources: Include > All zones (or specific zone)
#    - Copy the token (shown only once!)
#
# 2. Create Kubernetes namespace and Secret:
#    kubectl create namespace cert-manager
#    kubectl create secret generic cloudflare-api-token \
#      --from-literal=api-token=<your-token> \
#      -n cert-manager
#
# 3. Deploy cert-manager:
#    helm repo add jetstack https://charts.jetstack.io
#    helm install cert-manager jetstack/cert-manager \
#      --version v1.16.2 \
#      --namespace cert-manager \
#      --values infrastructure-k8s/cert-manager/helm-values.yaml
#
# 4. Create ClusterIssuers (update email first!):
#    kubectl apply -f infrastructure-k8s/cert-manager/clusterissuer-letsencrypt-staging.yaml
#    kubectl apply -f infrastructure-k8s/cert-manager/clusterissuer-letsencrypt-production.yaml
#
# 5. Test certificate issuance:
#    # Edit test-certificate.yaml with your domain first
#    kubectl apply -f infrastructure-k8s/cert-manager/verification/test-certificate.yaml
#    kubectl get certificate -n cert-manager
#
# See docs/cert-manager-setup.md for detailed instructions.
#
# =============================================================================

# =============================================================================
# GATEWAY API CONFIGURATION (PHASE 10)
# =============================================================================
#
# Gateway API provides HTTP/HTTPS ingress with TLS termination.
# Configuration is in infrastructure-k8s/gateway/ manifests.
#
# These variables are for DOCUMENTATION ONLY - actual configuration is in:
# - infrastructure-k8s/gateway/gateway.yaml (Gateway resource)
# - infrastructure-k8s/gateway/certificate.yaml (TLS certificate)
#
# =============================================================================

# Primary domain for Gateway (update in gateway.yaml and certificate.yaml)
# Example: example.com
# GATEWAY_DOMAIN=example.com

# Enable wildcard certificate for subdomains
# Example: true for *.example.com
# GATEWAY_WILDCARD_ENABLED=true

# =============================================================================
# GATEWAY API SETUP NOTES
# =============================================================================
#
# 1. Update Configuration:
#    Edit infrastructure-k8s/gateway/gateway.yaml:
#    - Replace '*.example.com' with your domain
#    Edit infrastructure-k8s/gateway/certificate.yaml:
#    - Update dnsNames with your domain(s)
#
# 2. Deploy Gateway:
#    kubectl apply -f infrastructure-k8s/gateway/namespace.yaml
#    kubectl apply -f infrastructure-k8s/gateway/gateway.yaml
#    kubectl apply -f infrastructure-k8s/gateway/certificate.yaml
#
# 3. Get LoadBalancer IP:
#    kubectl get gateway gateway -n gateway-ingress -o jsonpath='{.status.addresses[0].value}'
#
# 4. DNS Configuration (Cloudflare):
#    - Log into Cloudflare Dashboard
#    - Go to your domain's DNS settings
#    - Create A record: example.com → <LoadBalancer-IP>
#    - Create A record: *.example.com → <LoadBalancer-IP> (for wildcard)
#    - Set proxy status to "DNS only" (gray cloud)
#
# 5. Verify DNS Propagation:
#    dig example.com +short
#    # Should show LoadBalancer IP
#
# 6. Verify Certificate:
#    kubectl get certificate -n gateway-ingress
#    # Wait for READY=True
#
# 7. Test Access:
#    curl -v https://example.com
#
# 8. Port Forwarding (if using single public IP):
#    If your LoadBalancer IP is internal, configure port forwarding:
#    - Forward port 80 (TCP) to <LoadBalancer-IP>:80
#    - Forward port 443 (TCP) to <LoadBalancer-IP>:443
#
# 9. Troubleshooting:
#    - Gateway Programmed: kubectl describe gateway gateway -n gateway-ingress
#    - Certificate Ready: kubectl describe certificate gateway-tls -n gateway-ingress
#    - DNS Propagation: dig example.com +short (should show IP)
#    - Challenges: kubectl get challenges -A
#
# See docs/gateway-api-setup.md for detailed instructions.
#
# =============================================================================

# =============================================================================
# CLOUDNATIVEPG CONFIGURATION (PHASE 11)
# =============================================================================
#
# CloudNativePG operator provides PostgreSQL database management on Kubernetes.
# It handles cluster provisioning, high availability, backup, and monitoring.
#
# Features:
# - Automated PostgreSQL cluster deployment (primary + replicas)
# - High availability with automatic failover
# - Continuous backup and point-in-time recovery (PITR)
# - Prometheus metrics integration
# - Connection pooling support (PgBouncer)
#
# Prerequisites:
# - Cilium CNI operational (Phase 6)
# - Proxmox CSI driver installed (Phase 7)
# - StorageClass 'proxmox-csi' available
#
# Setup instructions: docs/cloudnativepg-setup.md
# =============================================================================

# CloudNativePG operator is deployed to 'cnpg-system' namespace
# PostgreSQL clusters can be created in any namespace

# No environment variables required for operator deployment
# Configuration is in infrastructure-k8s/cnpg/helm-values.yaml

# =============================================================================
# CLOUDNATIVEPG SETUP NOTES
# =============================================================================
#
# 1. Verify operator installation:
#    kubectl get pods -n cnpg-system
#    kubectl get crds | grep cnpg
#
# 2. Create a PostgreSQL cluster:
#    kubectl apply -f infrastructure-k8s/cnpg/verification/namespace.yaml
#    kubectl apply -f infrastructure-k8s/cnpg/verification/sample-cluster.yaml
#
# 3. Check cluster status:
#    kubectl get cluster -n cnpg-test
#    kubectl get pods -n cnpg-test
#
# 4. Get connection credentials:
#    kubectl get secret <cluster-name>-superuser -n <namespace> -o jsonpath='{.data.password}' | base64 -d
#
# 5. Connect to PostgreSQL:
#    kubectl port-forward -n <namespace> <cluster-name>-1 5432:5432
#    psql -h localhost -U postgres -d <database>
#
# 6. Backup Configuration (Optional):
#    CloudNativePG supports backup to S3-compatible object stores (MinIO, AWS S3, etc.)
#    Configure in Cluster resource spec.backup.barmanObjectStore:
#    - destinationPath: s3://bucket-name/path
#    - s3Credentials: reference to Secret with ACCESS_KEY_ID and ACCESS_SECRET_KEY
#    - retentionPolicy: "30d" (keep backups for 30 days)
#
#    Example Secret creation:
#    kubectl create secret generic s3-creds \
#      --from-literal=ACCESS_KEY_ID=<key> \
#      --from-literal=ACCESS_SECRET_KEY=<secret> \
#      -n <namespace>
#
# 7. Monitoring (Phase 13):
#    Enable PodMonitor in Cluster resource: spec.monitoring.enablePodMonitor: true
#    Metrics available on port 9187
#    Default metrics in ConfigMap 'default-monitoring'
#
# 8. Security Best Practices:
#    - Never commit PostgreSQL credentials to Git
#    - Use sealed-secrets or external-secrets for production
#    - Rotate credentials regularly
#    - Enable TLS for PostgreSQL connections
#    - Configure network policies to restrict access
#
# 9. High Availability:
#    - Use 3 instances (1 primary + 2 replicas) for HA
#    - Automatic failover on primary failure
#    - Synchronous replication for data consistency
#
# 10. Common Issues:
#     - PVC provisioning failures: Check Proxmox CSI driver logs
#     - Cluster not ready: Check operator logs in cnpg-system namespace
#     - Connection failures: Verify Service and Secret exist
#     - Backup failures: Check S3 credentials and connectivity
#
# See infrastructure-k8s/cnpg/README.md for detailed documentation.
#
# =============================================================================

# =============================================================================
# OBSERVABILITY STACK CONFIGURATION (PHASE 13)
# =============================================================================
#
# LGTM observability stack (Loki, Grafana, Tempo, Mimir) with OpenTelemetry
# Collector provides unified logs, metrics, and traces for the entire cluster.
#
# Components:
# - Grafana: Visualization and dashboards (port 3000)
# - Loki: Log aggregation (port 3100)
# - Tempo: Distributed tracing (port 3100, 4317, 4318)
# - Mimir: Metrics storage (port 8080)
# - OpenTelemetry Collector: Unified telemetry collection
# - Prometheus Operator: ServiceMonitor/PodMonitor CRDs
#
# Prerequisites:
# - Cilium CNI operational (Phase 6)
# - Proxmox CSI driver installed (Phase 7)
# - Flux GitOps setup (Phase 8)
#
# Setup instructions: docs/observability-setup.md
# =============================================================================

# Grafana admin password (NEVER commit actual password!)
# Create Secret manually:
#   kubectl create secret generic grafana-admin \
#     --from-literal=admin-user=admin \
#     --from-literal=admin-password=<secure-password> \
#     -n observability
# GRAFANA_ADMIN_PASSWORD=<create-via-kubectl-secret>

# Storage sizing for observability components (using Proxmox CSI)
# Adjust based on cluster size and retention requirements
OBSERVABILITY_LOKI_STORAGE_SIZE=50Gi      # Log storage (30-day retention)
OBSERVABILITY_TEMPO_STORAGE_SIZE=30Gi     # Trace storage (30-day retention)
OBSERVABILITY_MIMIR_STORAGE_SIZE=100Gi    # Metric storage (30-day retention)
OBSERVABILITY_GRAFANA_STORAGE_SIZE=10Gi   # Dashboard/config storage

# Retention policies (duration format: 30d, 720h, etc.)
OBSERVABILITY_LOG_RETENTION=30d
OBSERVABILITY_TRACE_RETENTION=30d
OBSERVABILITY_METRIC_RETENTION=30d

# =============================================================================
# OBSERVABILITY SETUP NOTES
# =============================================================================
#
# 1. Deploy observability stack (Phase 13):
#    Flux will automatically deploy all components after infrastructure is ready.
#    Check status:
#      flux get helmreleases -n observability
#      kubectl get pods -n observability
#
# 2. Create Grafana admin password Secret:
#    kubectl create namespace observability
#    kubectl create secret generic grafana-admin \
#      --from-literal=admin-user=admin \
#      --from-literal=admin-password=$(openssl rand -base64 32) \
#      -n observability
#
# 3. Access Grafana:
#    kubectl port-forward -n observability svc/grafana 3000:80
#    Open http://localhost:3000
#    Login: admin / <password-from-secret>
#
# 4. Verify data sources:
#    Grafana → Configuration → Data Sources
#    Should see: Loki, Tempo, Mimir (all configured automatically)
#
# 5. Explore dashboards:
#    Grafana → Dashboards
#    Pre-installed: Kubernetes Cluster, Cilium, Talos, PostgreSQL
#
# 6. Query logs (Loki):
#    Grafana → Explore → Loki
#    Example: {namespace="observability"}
#
# 7. Query metrics (Mimir):
#    Grafana → Explore → Mimir
#    Example: sum(rate(container_cpu_usage_seconds_total[5m])) by (pod)
#
# 8. Query traces (Tempo):
#    Grafana → Explore → Tempo
#    Search by service name, duration, etc.
#
# 9. Add monitoring to new components:
#    Create ServiceMonitor or PodMonitor in component namespace
#    OpenTelemetry Collector will automatically scrape metrics
#
# 10. Troubleshooting:
#     - Component not starting: Check PVC provisioning (Proxmox CSI)
#     - No metrics: Check ServiceMonitor labels and endpoints
#     - No logs: Check OTel Collector logs and Loki ingestion
#     - No traces: Check OTel Collector receivers and Tempo ingestion
#     - Grafana login fails: Verify Secret exists and password is correct
#
# See docs/observability-setup.md for detailed instructions.
#
# =============================================================================

# =============================================================================
# ALERTING CONFIGURATION (PHASE 15)
# =============================================================================
#
# Alertmanager for alert routing and notification.
#
# Prerequisites:
# - Prometheus Operator deployed (Phase 13)
# - Notification channel credentials (Slack webhook, email SMTP)
#
# Configuration:
# - Create alertmanager-config Secret manually (see docs/observability-setup.md)
# - Configure notification receivers (Slack, email, PagerDuty)
# - Define alert routing rules by severity/namespace
#
# Example Slack webhook:
# ALERTMANAGER_SLACK_WEBHOOK_URL=https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXX
#
# Example email SMTP:
# ALERTMANAGER_SMTP_HOST=smtp.gmail.com
# ALERTMANAGER_SMTP_PORT=587
# ALERTMANAGER_SMTP_FROM=alerts@example.com
# ALERTMANAGER_SMTP_USERNAME=alerts@example.com
# ALERTMANAGER_SMTP_PASSWORD=<app-password>
#
# Setup Notes:
# 1. Create Slack incoming webhook: https://api.slack.com/messaging/webhooks
# 2. Create alertmanager-config Secret:
#    kubectl create secret generic alertmanager-config \
#      --from-file=alertmanager.yaml \
#      -n observability
# 3. Deploy Alertmanager via Flux (clusters/production/infrastructure/)
# 4. Verify alerts: kubectl get prometheusrules -n observability
# 5. Test notifications: kubectl port-forward svc/alertmanager 9093:9093
#
# Security:
# - Never commit alertmanager-config.yaml to Git
# - Use sealed-secrets or external-secrets for production
# - Rotate webhook URLs/SMTP passwords regularly
#
# =============================================================================

# =============================================================================
# CONFIGURATION EXAMPLES
# =============================================================================
#
# Example 1: 3 control planes with static IPs (HA home lab)
# ---------------------------------------------------------
# PROXMOX_NODE_NAME=pve
# PROXMOX_STORAGE_ID=local-lvm
# PROXMOX_IMAGE_STORAGE_ID=local
# PROXMOX_NETWORK_BRIDGE=vmbr0
# TALOS_VERSION=v1.11.5
# TALOS_CLUSTER_NAME=homelab
# TALOS_CLUSTER_ENDPOINT=https://192.168.1.101:6443
# TALOS_NODE_IP_PREFIX=192.168.1
# TALOS_NODE_IP_START=101
# TALOS_NODE_GATEWAY=192.168.1.1
# TALOS_NODE_NETMASK=24
# TALOS_CONTROL_PLANE_COUNT=3
# TALOS_WORKER_COUNT=0
# TALOS_TOPOLOGY_REGION=home
# TALOS_TOPOLOGY_ZONE=proxmox-1
#
# Example 2: 1 control plane + 2 workers with static IPs (small cluster)
# ----------------------------------------------------------------------
# PROXMOX_NODE_NAME=pve
# PROXMOX_STORAGE_ID=local-lvm
# TALOS_CLUSTER_ENDPOINT=https://192.168.1.101:6443
# TALOS_NODE_IP_PREFIX=192.168.1
# TALOS_NODE_IP_START=101
# TALOS_NODE_GATEWAY=192.168.1.1
# TALOS_CONTROL_PLANE_COUNT=1
# TALOS_WORKER_COUNT=2
# (Results in CP at .101, workers at .102, .103)
#
# Example 3: DHCP mode (simpler network setup)
# --------------------------------------------
# PROXMOX_NODE_NAME=pve
# PROXMOX_STORAGE_ID=local-lvm
# TALOS_CLUSTER_ENDPOINT=https://192.168.1.100:6443  # VIP or reserved IP
# # TALOS_NODE_IP_PREFIX=     (leave empty or unset for DHCP)
# # TALOS_NODE_GATEWAY=       (leave empty or unset for DHCP)
# TALOS_CONTROL_PLANE_COUNT=3
# (Configure DHCP reservations in your router after seeing MAC addresses)
#
# Example 4: Ceph storage cluster (image on local, disks on Ceph)
# ---------------------------------------------------------------
# PROXMOX_NODE_NAME=pve1
# PROXMOX_STORAGE_ID=ceph-pool          # Ceph pool for VM disks
# PROXMOX_IMAGE_STORAGE_ID=local        # ISO content on local storage
# TALOS_CLUSTER_ENDPOINT=https://10.0.0.100:6443
# TALOS_NODE_IP_PREFIX=10.0.0
# TALOS_NODE_IP_START=101
# TALOS_NODE_GATEWAY=10.0.0.1
#
# =============================================================================
