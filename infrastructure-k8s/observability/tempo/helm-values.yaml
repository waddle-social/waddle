# =============================================================================
# TEMPO HELM VALUES
# =============================================================================
#
# Chart: grafana/tempo
# Version: 1.x.x
#
# Purpose: Deploy Tempo for distributed tracing with TraceQL query support.
#
# Usage:
#   helm repo add grafana https://grafana.github.io/helm-charts
#   helm install tempo grafana/tempo -n observability -f helm-values.yaml
#
# Prerequisites:
# - Namespace 'observability' created
# - Proxmox CSI driver installed (Phase 7)
#
# Deployment Mode: Monolithic
#   Simple single-process deployment suitable for small to medium trace volumes.
#   For high availability, consider distributed mode.
#
# Storage: Filesystem via PVC (Proxmox CSI)
#   30Gi provides ~30 days of traces for a small cluster.
#   Adjust size based on trace volume and retention requirements.
#
# Receivers:
# - OTLP gRPC (4317) - Primary, used by OTel Collector
# - OTLP HTTP (4318) - Alternative for HTTP clients
# - Jaeger gRPC (14250) - For Jaeger compatibility
# - Jaeger Thrift HTTP (14268) - For legacy Jaeger clients
#
# Verification:
#   kubectl get pods -n observability -l app.kubernetes.io/name=tempo
#   kubectl get pvc -n observability | grep tempo
#   # Test:
#   kubectl port-forward -n observability svc/tempo 3100
#   curl http://localhost:3100/ready
#
# Query Examples (TraceQL):
#   {}  # All traces
#   { status = error }  # Error traces
#   { duration > 1s }  # Slow traces
#   { resource.service.name = "my-service" }  # By service
#
# Troubleshooting:
# - PVC pending: Check Proxmox CSI driver status
# - No traces: Check OTel Collector exporter config, receiver ports
# - Query timeout: Add time constraints, reduce result limit
# - OOM: Increase memory limits, configure sampling
#
# =============================================================================

# -----------------------------------------------------------------------------
# Tempo Configuration
# -----------------------------------------------------------------------------
tempo:
  # Retention: 30 days
  retention: 720h

  # Reporting disabled
  reportingEnabled: false

  # Structured configuration
  structuredConfig:
    # Storage backend
    storage:
      trace:
        backend: local
        local:
          path: /var/tempo/traces
        wal:
          path: /var/tempo/wal
        block:
          bloom_filter_false_positive: 0.05
          v2_index_downsample_bytes: 1000
          v2_encoding: zstd

    # Distributor configuration
    distributor:
      receivers:
        otlp:
          protocols:
            grpc:
              endpoint: "0.0.0.0:4317"
            http:
              endpoint: "0.0.0.0:4318"
        jaeger:
          protocols:
            grpc:
              endpoint: "0.0.0.0:14250"
            thrift_http:
              endpoint: "0.0.0.0:14268"

    # Ingester configuration
    ingester:
      max_block_duration: 5m
      trace_idle_period: 10s
      max_block_bytes: 1073741824  # 1GB

    # Compactor configuration
    compactor:
      compaction:
        block_retention: 720h  # 30 days
        compacted_block_retention: 1h

    # Querier configuration
    querier:
      max_concurrent_queries: 20
      search:
        prefer_self: 10

    # Query frontend
    query_frontend:
      search:
        concurrent_jobs: 1000
        target_bytes_per_job: 104857600

    # Server configuration
    server:
      http_listen_port: 3100
      grpc_listen_port: 9095
      log_level: info

    # Metrics generator (optional - generates metrics from traces)
    metrics_generator:
      registry:
        external_labels:
          source: tempo
      storage:
        path: /var/tempo/generator/wal
        remote_write: []  # Configure if metrics generation needed

# -----------------------------------------------------------------------------
# Persistence
# -----------------------------------------------------------------------------
persistence:
  enabled: true
  storageClassName: proxmox-csi
  size: 30Gi
  accessModes:
    - ReadWriteOnce

# -----------------------------------------------------------------------------
# Resources
# -----------------------------------------------------------------------------
resources:
  requests:
    cpu: 200m
    memory: 512Mi
  limits:
    cpu: "1"
    memory: 2Gi

# -----------------------------------------------------------------------------
# Node Selector (control-plane nodes)
# -----------------------------------------------------------------------------
nodeSelector:
  node-role.kubernetes.io/control-plane: ""

# -----------------------------------------------------------------------------
# Tolerations (control-plane taints)
# -----------------------------------------------------------------------------
tolerations:
  - key: node-role.kubernetes.io/control-plane
    operator: Exists
    effect: NoSchedule

# -----------------------------------------------------------------------------
# Security Context
# -----------------------------------------------------------------------------
securityContext:
  runAsNonRoot: true
  runAsUser: 10001
  runAsGroup: 10001
  fsGroup: 10001

containerSecurityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  capabilities:
    drop:
      - ALL

# -----------------------------------------------------------------------------
# Service
# -----------------------------------------------------------------------------
service:
  type: ClusterIP

# -----------------------------------------------------------------------------
# Service Monitor
# -----------------------------------------------------------------------------
serviceMonitor:
  enabled: true
  labels:
    app.kubernetes.io/part-of: observability

# -----------------------------------------------------------------------------
# Extra Volumes
# -----------------------------------------------------------------------------
extraVolumes:
  - name: tmp
    emptyDir: {}

extraVolumeMounts:
  - name: tmp
    mountPath: /tmp
