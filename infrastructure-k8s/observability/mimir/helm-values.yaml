# =============================================================================
# MIMIR HELM VALUES
# =============================================================================
#
# Chart: grafana/mimir-distributed
# Version: 5.x.x
#
# Purpose: Deploy Mimir for metrics storage with PromQL query support.
# Mimir is a Prometheus-compatible long-term storage backend.
#
# Usage:
#   helm repo add grafana https://grafana.github.io/helm-charts
#   helm install mimir grafana/mimir-distributed -n observability -f helm-values.yaml
#
# Prerequisites:
# - Namespace 'observability' created
# - Proxmox CSI driver installed (Phase 7)
# - Prometheus Operator CRDs installed (for ServiceMonitors)
#
# Deployment Mode: Monolithic
#   Single-process deployment suitable for small to medium metric volumes.
#   For high availability, enable multiple replicas and components.
#
# Storage: Filesystem via PVC (Proxmox CSI)
#   100Gi provides ~30 days of metrics for a small cluster.
#   Adjust size based on metric cardinality and retention requirements.
#
# Remote Write:
#   OTel Collector sends metrics via Prometheus remote_write protocol.
#   Endpoint: http://mimir-nginx:80/api/v1/push
#
# Verification:
#   kubectl get pods -n observability -l app.kubernetes.io/name=mimir
#   kubectl get pvc -n observability | grep mimir
#   # Test:
#   kubectl port-forward -n observability svc/mimir-nginx 8080:80
#   curl http://localhost:8080/ready
#
# Query Examples (PromQL):
#   up  # All up metrics
#   sum(rate(container_cpu_usage_seconds_total[5m])) by (pod)
#   kubelet_running_pods
#
# Troubleshooting:
# - PVC pending: Check Proxmox CSI driver status
# - No metrics: Check OTel Collector remote_write config
# - Query timeout: Reduce time range, add label filters
# - High cardinality: Review metric labels, add relabeling rules
# - OOM: Increase memory limits, reduce active series
#
# =============================================================================

# -----------------------------------------------------------------------------
# Global Configuration
# -----------------------------------------------------------------------------
global:
  # Pod security context
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 10001
    runAsGroup: 10001
    fsGroup: 10001

# -----------------------------------------------------------------------------
# Mimir Structured Configuration
# -----------------------------------------------------------------------------
mimir:
  structuredConfig:
    # Common storage configuration
    common:
      storage:
        backend: filesystem
        filesystem:
          dir: /data

    # Limits configuration
    limits:
      # Retention: 30 days
      compactor_blocks_retention_period: 720h
      # Ingestion limits
      ingestion_rate: 50000
      ingestion_burst_size: 100000
      max_global_series_per_user: 1000000
      max_global_series_per_metric: 100000
      # Query limits
      max_fetched_series_per_query: 100000
      max_fetched_chunks_per_query: 2000000

    # Block storage configuration
    blocks_storage:
      backend: filesystem
      filesystem:
        dir: /data/blocks
      tsdb:
        dir: /data/tsdb
        block_ranges_period:
          - 2h
        retention_period: 720h

    # Alertmanager storage (disabled but path required)
    alertmanager_storage:
      backend: filesystem
      filesystem:
        dir: /data/alertmanager

    # Ruler storage
    ruler_storage:
      backend: filesystem
      filesystem:
        dir: /data/rules

    # Ingester configuration
    ingester:
      ring:
        replication_factor: 1
        heartbeat_timeout: 10m

    # Store gateway (for querying blocks)
    store_gateway:
      sharding_ring:
        replication_factor: 1

    # Compactor configuration
    compactor:
      data_dir: /data/compactor
      sharding_ring:
        replication_factor: 1

    # Query scheduler
    query_scheduler:
      max_outstanding_requests_per_tenant: 100

    # Server configuration
    server:
      http_listen_port: 8080
      grpc_listen_port: 9095
      log_level: info

    # Activity tracker
    activity_tracker:
      filepath: /active-query-tracker/activity.log

    # Memberlist (for single node, use localhost)
    memberlist:
      join_members: []

# -----------------------------------------------------------------------------
# Deployment Mode: Monolithic
# -----------------------------------------------------------------------------
deploymentMode: Monolithic

# -----------------------------------------------------------------------------
# Monolithic Mode Configuration
# -----------------------------------------------------------------------------
monolithic:
  replicas: 1

  # Persistence
  persistentVolume:
    enabled: true
    storageClass: proxmox-csi
    size: 100Gi
    accessModes:
      - ReadWriteOnce

  # Resources
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: "2"
      memory: 4Gi

  # Node selector (control-plane nodes)
  nodeSelector:
    node-role.kubernetes.io/control-plane: ""

  # Tolerations (control-plane taints)
  tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule

  # Security context
  containerSecurityContext:
    allowPrivilegeEscalation: false
    readOnlyRootFilesystem: true
    capabilities:
      drop:
        - ALL

  # Extra volumes
  extraVolumes:
    - name: active-query-tracker
      emptyDir: {}
    - name: tmp
      emptyDir: {}

  extraVolumeMounts:
    - name: active-query-tracker
      mountPath: /active-query-tracker
    - name: tmp
      mountPath: /tmp

# -----------------------------------------------------------------------------
# MinIO - DISABLED (using filesystem storage)
# -----------------------------------------------------------------------------
minio:
  enabled: false

# -----------------------------------------------------------------------------
# NGINX Gateway
# -----------------------------------------------------------------------------
nginx:
  enabled: true
  replicas: 1

  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 200m
      memory: 128Mi

  nodeSelector:
    node-role.kubernetes.io/control-plane: ""

  tolerations:
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule

  service:
    type: ClusterIP

# -----------------------------------------------------------------------------
# Meta Monitoring - ServiceMonitor
# -----------------------------------------------------------------------------
metaMonitoring:
  serviceMonitor:
    enabled: true
    labels:
      app.kubernetes.io/part-of: observability

  grafanaAgent:
    enabled: false

# -----------------------------------------------------------------------------
# Distributed Components - DISABLED (using monolithic mode)
# -----------------------------------------------------------------------------
ingester:
  replicas: 0
distributor:
  replicas: 0
querier:
  replicas: 0
query_frontend:
  replicas: 0
query_scheduler:
  replicas: 0
store_gateway:
  replicas: 0
compactor:
  replicas: 0
ruler:
  replicas: 0
alertmanager:
  replicas: 0
overrides_exporter:
  replicas: 0

# -----------------------------------------------------------------------------
# Rollout Operator - DISABLED
# -----------------------------------------------------------------------------
rollout_operator:
  enabled: false

# -----------------------------------------------------------------------------
# Gateway - Use NGINX instead
# -----------------------------------------------------------------------------
gateway:
  enabledNonEnterprise: false
